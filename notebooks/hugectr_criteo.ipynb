{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfec37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "# Each user is responsible for checking the content of datasets and the\n",
    "# applicable licenses and determining if suitable for the intended use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe9a708",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/notebooks/dlsw-notebooks/merlin_hugectr_hugectr-criteo/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# Introduction to the HugeCTR Python Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f72e368",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "HugeCTR version 3.1 introduces an enhanced Python interface\n",
    "The interface supports continuous training and inference with high-level APIs.\n",
    "There are four main improvements.\n",
    "\n",
    "* First, the model graph can be constructed and dumped to a JSON file with Python code and it saves users from writing JSON configuration files.\n",
    "* Second, the API supports the feature of embedding training cache with high-level APIs and extends it further for online training cases.\n",
    "(For learn about continuous training, you can view the [example notebook](./continuous_training.ipynb)).\n",
    "* Third, the freezing method is provided for both sparse embedding and dense network.\n",
    "This method enables transfer learning and fine-tuning for CTR tasks.\n",
    "* Finally, the pre-trained embeddings in other formats can be converted to HugeCTR sparse models and then loaded to facilitate the training process. This is shown in the Load Pre-trained Embeddings section of this notebook.\n",
    "\n",
    "This notebook explains how to access and use the enhanced HugeCTR Python interface.\n",
    "Although the low-level training APIs are still maintained for users who want to have precise control of each training iteration, migrating to the high-level training APIs is strongly recommended.\n",
    "For more details of the usage of the Python API, refer to the [HugeCTR Python Interface](https://nvidia-merlin.github.io/HugeCTR/master/api/python_interface.html) documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651880c9",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "To setup the environment, refer to [HugeCTR Example Notebooks](../notebooks) and follow the instructions there before running the following."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9003e512-17b0-464c-90a7-973e080f8759",
   "metadata": {},
   "source": [
    "## DCN Model\n",
    "\n",
    "Note: If you already have the data downloaded, then skip to the preprocessing step (2). If preprocessing is also done, skip to creating the softlink between the processed data to the `notebooks/` directory (3).\n",
    "\n",
    "\n",
    "### Data Preparation\n",
    "\n",
    "To download and prepare the dataset we will be doing the following steps. At the end of this cell, we provide the shell commands you can run on the terminal to get the data ready for this notebook.\n",
    "\n",
    "**Note**: If you already have the data downloaded, then skip to the preprocessing step (2). If preprocessing is also done, skip to creating the softlink between the processed data to the `notebooks/` directory (3).\n",
    "\n",
    "\n",
    "1. Download the Criteo dataset\n",
    "\n",
    "To preprocess the downloaded Kaggle Criteo dataset, we'll make the following operations: \n",
    "\n",
    "   * Reduce the amounts of data to speed up the preprocessing\n",
    "   * Fill missing values\n",
    "   * Remove the feature values whose occurrences are very rare, etc.\n",
    "\n",
    "\n",
    "2. Preprocessing by Pandas:\n",
    "   \n",
    "   Meanings of the command line arguments:\n",
    "\n",
    "   * The 1st argument represents the dataset postfix. It is `1` here since `day_1` is used.\n",
    "   * The 2nd argument `wdl_data` is where the preprocessed data is stored.\n",
    "   * The 3rd argument `pandas` is the processing script going to use, here we choose `pandas`.\n",
    "   * The 4th argument `1` embodies that the normalization is applied to dense features.\n",
    "   * The 5th argument `1` means that the feature crossing is applied.\n",
    "   * The 6th argument `100` means the number of data files in each file list.\n",
    "\n",
    "   For more details about the data preprocessing, please refer to the \"Preprocess the Criteo Dataset\" section of the README in the [samples/criteo](https://github.com/NVIDIA-Merlin/HugeCTR/tree/master/samples/criteo) directory of the repository on GitHub.\n",
    "   \n",
    "3. Create a soft link of the dataset folder to the path of this notebook\n",
    "\n",
    "#### Run the following commands on the terminal to prepare the data for this notebook\n",
    "\n",
    "\n",
    "```shell\n",
    "export project_root=/home/hugectr # set this to the directory where hugectr is downloaded\n",
    "cd ${project_root}/tools\n",
    "# Step 1\n",
    "wget https://storage.googleapis.com/criteo-cail-datasets/day_0.gz\n",
    "#Step 2\n",
    "bash preprocess.sh 0 dcn_data pandas 1 0\n",
    "#Step 3\n",
    "ln -s ${project_root}/tools/dcn_data ${project_root}/notebooks/dcn_data\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a887074-75a3-4005-bbb9-268ac195c1c3",
   "metadata": {},
   "source": [
    "**Note**: It will take a while (dozens of minutes) to preprocess the dataset. Please make sure that it is finished successfully before moving forward to the next section."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "072829a0",
   "metadata": {},
   "source": [
    "### Train from Scratch\n",
    "\n",
    "We can train from scratch, dump the model graph to a JSON file, and save the model weights and optimizer states by performing the following with Python APIs:\n",
    "\n",
    "1. Create the solver, reader and optimizer, then initialize the model.\n",
    "2. Construct the model graph by adding input, sparse embedding and dense layers in order.\n",
    "3. Compile the model and have an overview of the model graph.\n",
    "4. Dump the model graph to the JSON file.\n",
    "5. Fit the model, save the model weights and optimizer states implicitly.\n",
    "\n",
    "Please note that the training mode is determined by `repeat_dataset` within [hugectr.CreateSolver](https://nvidia-merlin.github.io/HugeCTR/main/api/python_interface.html#createsolver-method).\n",
    "If it is `True`, the non-epoch mode training is adopted and the maximum iterations should be specified by `max_iter` within [hugectr.Model.fit](https://nvidia-merlin.github.io/HugeCTR/main/api/python_interface.html#fit-method).\n",
    "If it is `False`, the epoch-mode training is adopted and the number of epochs should be specified by `num_epochs` within [hugectr.Model.fit](https://nvidia-merlin.github.io/HugeCTR/main/api/python_interface.html#fit-method).\n",
    "\n",
    "The optimizer that is used to initialize the model applies to the weights of dense layers, while the optimizer for each sparse embedding layer can be specified independently within [hugectr.SparseEmbedding](https://nvidia-merlin.github.io/HugeCTR/main/api/hugectr_layer_book.html#embedding-types-detail)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe15b878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing dcn_train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile dcn_train.py\n",
    "import hugectr\n",
    "from mpi4py import MPI\n",
    "solver = hugectr.CreateSolver(max_eval_batches = 1500,\n",
    "                              batchsize_eval = 4096,\n",
    "                              batchsize = 4096,\n",
    "                              lr = 0.001,\n",
    "                              vvgpu = [[0]],\n",
    "                              i64_input_key = False,\n",
    "                              use_mixed_precision = False,\n",
    "                              repeat_dataset = True,\n",
    "                              use_cuda_graph = True)\n",
    "reader = hugectr.DataReaderParams(data_reader_type = hugectr.DataReaderType_t.Norm,\n",
    "                                  source = [\"./dcn_data/file_list.txt\"],\n",
    "                                  eval_source = \"./dcn_data/file_list_test.txt\",\n",
    "                                  check_type = hugectr.Check_t.Sum)\n",
    "optimizer = hugectr.CreateOptimizer(optimizer_type = hugectr.Optimizer_t.Adam)\n",
    "model = hugectr.Model(solver, reader, optimizer)\n",
    "model.add(hugectr.Input(label_dim = 1, label_name = \"label\",\n",
    "                        dense_dim = 13, dense_name = \"dense\",\n",
    "                        data_reader_sparse_param_array = \n",
    "                        [hugectr.DataReaderSparseParam(\"data1\", 2, False, 26)]))\n",
    "model.add(hugectr.SparseEmbedding(embedding_type = hugectr.Embedding_t.DistributedSlotSparseEmbeddingHash, \n",
    "                            workspace_size_per_gpu_in_mb = 264,\n",
    "                            embedding_vec_size = 16,\n",
    "                            combiner = \"sum\",\n",
    "                            sparse_embedding_name = \"sparse_embedding1\",\n",
    "                            bottom_name = \"data1\",\n",
    "                            optimizer = optimizer))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Reshape,\n",
    "                            bottom_names = [\"sparse_embedding1\"],\n",
    "                            top_names = [\"reshape1\"],\n",
    "                            leading_dim=416))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Concat,\n",
    "                            bottom_names = [\"reshape1\", \"dense\"], top_names = [\"concat1\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.MultiCross,\n",
    "                            bottom_names = [\"concat1\"],\n",
    "                            top_names = [\"multicross1\"],\n",
    "                            num_layers=6))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"concat1\"],\n",
    "                            top_names = [\"fc1\"],\n",
    "                            num_output=1024))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc1\"],\n",
    "                            top_names = [\"relu1\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Dropout,\n",
    "                            bottom_names = [\"relu1\"],\n",
    "                            top_names = [\"dropout1\"],\n",
    "                            dropout_rate=0.5))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"dropout1\"],\n",
    "                            top_names = [\"fc2\"],\n",
    "                            num_output=1024))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc2\"],\n",
    "                            top_names = [\"relu2\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Dropout,\n",
    "                            bottom_names = [\"relu2\"],\n",
    "                            top_names = [\"dropout2\"],\n",
    "                            dropout_rate=0.5))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Concat,\n",
    "                            bottom_names = [\"dropout2\", \"multicross1\"],\n",
    "                            top_names = [\"concat2\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"concat2\"],\n",
    "                            top_names = [\"fc3\"],\n",
    "                            num_output=1))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.BinaryCrossEntropyLoss,\n",
    "                            bottom_names = [\"fc3\", \"label\"],\n",
    "                            top_names = [\"loss\"]))\n",
    "model.compile()\n",
    "model.summary()\n",
    "model.graph_to_json(graph_config_file = \"dcn.json\")\n",
    "model.fit(max_iter = 1200, display = 500, eval_interval = 100, snapshot = 1000, snapshot_prefix = \"dcn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56c99500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HugeCTR Version: 4.1\n",
      "====================================================Model Init=====================================================\n",
      "[HCTR][08:37:13.891][WARNING][RK0][main]: The model name is not specified when creating the solver.\n",
      "[HCTR][08:37:13.892][INFO][RK0][main]: Global seed is 3840413353\n",
      "[HCTR][08:37:13.894][INFO][RK0][main]: Device to NUMA mapping:\n",
      "  GPU 0 ->  node 0\n",
      "[HCTR][08:37:15.710][WARNING][RK0][main]: Peer-to-peer access cannot be fully enabled.\n",
      "[HCTR][08:37:15.710][INFO][RK0][main]: Start all2all warmup\n",
      "[HCTR][08:37:15.710][INFO][RK0][main]: End all2all warmup\n",
      "[HCTR][08:37:15.710][INFO][RK0][main]: Using All-reduce algorithm: NCCL\n",
      "[HCTR][08:37:15.711][INFO][RK0][main]: Device 0: Tesla V100-SXM2-32GB\n",
      "[HCTR][08:37:15.712][INFO][RK0][main]: num of DataReader workers for train: 12\n",
      "[HCTR][08:37:15.712][INFO][RK0][main]: num of DataReader workers for eval: 12\n",
      "[HCTR][08:37:15.748][INFO][RK0][main]: max_vocabulary_size_per_gpu_=1441792\n",
      "[HCTR][08:37:15.750][INFO][RK0][main]: Graph analysis to resolve tensor dependency\n",
      "[HCTR][08:37:15.750][INFO][RK0][main]: Add Slice layer for tensor: concat1, creating 2 copies\n",
      "===================================================Model Compile===================================================\n",
      "[HCTR][08:37:29.637][INFO][RK0][main]: gpu0 start to init embedding\n",
      "[HCTR][08:37:29.638][INFO][RK0][main]: gpu0 init embedding done\n",
      "[HCTR][08:37:29.640][INFO][RK0][main]: Starting AUC NCCL warm-up\n",
      "[HCTR][08:37:29.643][INFO][RK0][main]: Warm-up done\n",
      "===================================================Model Summary===================================================\n",
      "[HCTR][08:37:29.643][INFO][RK0][main]: Model structure on each GPU\n",
      "Label                                   Dense                         Sparse                        \n",
      "label                                   dense                          data1                         \n",
      "(4096,1)                                (4096,13)                               \n",
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "Layer Type                              Input Name                    Output Name                   Output Shape                  \n",
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "DistributedSlotSparseEmbeddingHash      data1                         sparse_embedding1             (4096,26,16)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Reshape                                 sparse_embedding1             reshape1                      (4096,416)                    \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Concat                                  reshape1                      concat1                       (4096,429)                    \n",
      "                                        dense                                                                                     \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Slice                                   concat1                       concat1_slice0                (4096,429)                    \n",
      "                                                                      concat1_slice1                (4096,429)                    \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "MultiCross                              concat1_slice0                multicross1                   (4096,429)                    \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            concat1_slice1                fc1                           (4096,1024)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "ReLU                                    fc1                           relu1                         (4096,1024)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Dropout                                 relu1                         dropout1                      (4096,1024)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            dropout1                      fc2                           (4096,1024)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "ReLU                                    fc2                           relu2                         (4096,1024)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Dropout                                 relu2                         dropout2                      (4096,1024)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Concat                                  dropout2                      concat2                       (4096,1453)                   \n",
      "                                        multicross1                                                                               \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            concat2                       fc3                           (4096,1)                      \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "BinaryCrossEntropyLoss                  fc3                           loss                                                        \n",
      "                                        label                                                                                     \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "[HCTR][08:37:29.645][INFO][RK0][main]: Save the model graph to dcn.json successfully\n",
      "=====================================================Model Fit=====================================================\n",
      "[HCTR][08:37:29.645][INFO][RK0][main]: Use non-epoch mode with number of iterations: 1200\n",
      "[HCTR][08:37:29.645][INFO][RK0][main]: Training batchsize: 4096, evaluation batchsize: 4096\n",
      "[HCTR][08:37:29.645][INFO][RK0][main]: Evaluation interval: 100, snapshot interval: 1000\n",
      "[HCTR][08:37:29.645][INFO][RK0][main]: Dense network trainable: True\n",
      "[HCTR][08:37:29.645][INFO][RK0][main]: Sparse embedding sparse_embedding1 trainable: True\n",
      "[HCTR][08:37:29.645][INFO][RK0][main]: Use mixed precision: False, scaler: 1.000000, use cuda graph: True\n",
      "[HCTR][08:37:29.645][INFO][RK0][main]: lr: 0.001000, warmup_steps: 1, end_lr: 0.000000\n",
      "[HCTR][08:37:29.645][INFO][RK0][main]: decay_start: 0, decay_steps: 1, decay_power: 2.000000\n",
      "[HCTR][08:37:29.645][INFO][RK0][main]: Training source file: ./dcn_data/file_list.txt\n",
      "[HCTR][08:37:29.645][INFO][RK0][main]: Evaluation source file: ./dcn_data/file_list_test.txt\n",
      "[HCTR][08:37:33.797][INFO][RK0][main]: Evaluation, AUC: 0.722862\n",
      "[HCTR][08:37:33.797][INFO][RK0][main]: Eval Time for 1500 iters: 3.29509s\n",
      "[HCTR][08:37:37.855][INFO][RK0][main]: Evaluation, AUC: 0.738291\n",
      "[HCTR][08:37:37.855][INFO][RK0][main]: Eval Time for 1500 iters: 3.29477s\n",
      "[HCTR][08:37:41.915][INFO][RK0][main]: Evaluation, AUC: 0.748639\n",
      "[HCTR][08:37:41.915][INFO][RK0][main]: Eval Time for 1500 iters: 3.2957s\n",
      "[HCTR][08:37:45.987][INFO][RK0][main]: Evaluation, AUC: 0.753537\n",
      "[HCTR][08:37:45.987][INFO][RK0][main]: Eval Time for 1500 iters: 3.30702s\n",
      "[HCTR][08:37:46.752][INFO][RK0][main]: Iter: 500 Time(500 iters): 17.1058s Loss: 0.126047 lr:0.001\n",
      "[HCTR][08:37:50.048][INFO][RK0][main]: Evaluation, AUC: 0.755874\n",
      "[HCTR][08:37:50.048][INFO][RK0][main]: Eval Time for 1500 iters: 3.29613s\n",
      "[HCTR][08:37:54.108][INFO][RK0][main]: Evaluation, AUC: 0.758\n",
      "[HCTR][08:37:54.108][INFO][RK0][main]: Eval Time for 1500 iters: 3.29422s\n",
      "[HCTR][08:37:58.166][INFO][RK0][main]: Evaluation, AUC: 0.760666\n",
      "[HCTR][08:37:58.166][INFO][RK0][main]: Eval Time for 1500 iters: 3.29585s\n",
      "[HCTR][08:38:02.225][INFO][RK0][main]: Evaluation, AUC: 0.763448\n",
      "[HCTR][08:38:02.225][INFO][RK0][main]: Eval Time for 1500 iters: 3.29549s\n",
      "[HCTR][08:38:06.286][INFO][RK0][main]: Evaluation, AUC: 0.764422\n",
      "[HCTR][08:38:06.286][INFO][RK0][main]: Eval Time for 1500 iters: 3.29466s\n",
      "[HCTR][08:38:07.051][INFO][RK0][main]: Iter: 1000 Time(500 iters): 20.2983s Loss: 0.113481 lr:0.001\n",
      "[HCTR][08:38:10.347][INFO][RK0][main]: Evaluation, AUC: 0.755063\n",
      "[HCTR][08:38:10.347][INFO][RK0][main]: Eval Time for 1500 iters: 3.2954s\n",
      "[HCTR][08:38:10.347][INFO][RK0][main]: Using Local file system backend.\n",
      "[HCTR][08:38:10.369][INFO][RK0][main]: Rank0: Write hash table to file\n",
      "[HCTR][08:38:10.429][INFO][RK0][main]: Dumping sparse weights to files, successful\n",
      "[HCTR][08:38:10.485][INFO][RK0][main]: Rank0: Write optimzer state to file\n",
      "[HCTR][08:38:10.485][INFO][RK0][main]: Using Local file system backend.\n",
      "[HCTR][08:38:10.729][INFO][RK0][main]: Done\n",
      "[HCTR][08:38:10.790][INFO][RK0][main]: Rank0: Write optimzer state to file\n",
      "[HCTR][08:38:10.790][INFO][RK0][main]: Using Local file system backend.\n",
      "[HCTR][08:38:11.037][INFO][RK0][main]: Done\n",
      "[HCTR][08:38:11.042][INFO][RK0][main]: Dumping sparse optimzer states to files, successful\n",
      "[HCTR][08:38:11.045][INFO][RK0][main]: Using Local file system backend.\n",
      "[HCTR][08:38:11.062][INFO][RK0][main]: Dumping dense weights to file, successful\n",
      "[HCTR][08:38:11.067][INFO][RK0][main]: Using Local file system backend.\n",
      "[HCTR][08:38:11.098][INFO][RK0][main]: Dumping dense optimizer states to file, successful\n",
      "[HCTR][08:38:15.161][INFO][RK0][main]: Evaluation, AUC: 0.741793\n",
      "[HCTR][08:38:15.162][INFO][RK0][main]: Eval Time for 1500 iters: 3.29657s\n",
      "[HCTR][08:38:15.922][INFO][RK0][main]: Finish 1200 iterations with batchsize: 4096 in 46.28s.\n"
     ]
    }
   ],
   "source": [
    "!python3 dcn_train.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "650ff4d9",
   "metadata": {},
   "source": [
    "### Continue Training\n",
    "\n",
    "We can continue our training based on the saved model graph, model weights, and optimizer states by performing the following with Python APIs:\n",
    "\n",
    "1. Create the solver, reader and optimizer, then initialize the model.\n",
    "2. Construct the model graph from the saved JSON file, see Python API details [here](https://nvidia-merlin.github.io/HugeCTR/main/api/python_interface.html#construct-from-json-method).\n",
    "3. Compile the model and have an overview of the model graph.\n",
    "4. Load the model weights and optimizer states, see Python API details [here](https://nvidia-merlin.github.io/HugeCTR/main/api/python_interface.html#load-dense-weights-method).\n",
    "5. Fit the model, save the model weights and optimizer states implicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "339fb808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing dcn_continue.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile dcn_continue.py\n",
    "import hugectr\n",
    "from mpi4py import MPI\n",
    "solver = hugectr.CreateSolver(max_eval_batches = 1500,\n",
    "                              batchsize_eval = 4096,\n",
    "                              batchsize = 4096,\n",
    "                              vvgpu = [[0]],\n",
    "                              i64_input_key = False,\n",
    "                              use_mixed_precision = False,\n",
    "                              repeat_dataset = True,\n",
    "                              use_cuda_graph = True)\n",
    "reader = hugectr.DataReaderParams(data_reader_type = hugectr.DataReaderType_t.Norm,\n",
    "                                  source = [\"./dcn_data/file_list.txt\"],\n",
    "                                  eval_source = \"./dcn_data/file_list_test.txt\",\n",
    "                                  check_type = hugectr.Check_t.Sum)\n",
    "optimizer = hugectr.CreateOptimizer(optimizer_type = hugectr.Optimizer_t.Adam)\n",
    "model = hugectr.Model(solver, reader, optimizer)\n",
    "model.construct_from_json(graph_config_file = \"dcn.json\", include_dense_network = True)\n",
    "model.compile()\n",
    "model.load_dense_weights(\"dcn_dense_1000.model\")\n",
    "model.load_sparse_weights([\"dcn0_sparse_1000.model\"])\n",
    "model.load_dense_optimizer_states(\"dcn_opt_dense_1000.model\")\n",
    "model.load_sparse_optimizer_states([\"dcn0_opt_sparse_1000.model\"])\n",
    "model.summary()\n",
    "model.fit(max_iter = 500, display = 50, eval_interval = 100, snapshot = 10000, snapshot_prefix = \"dcn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b48ed0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HugeCTR Version: 4.1\n",
      "====================================================Model Init=====================================================\n",
      "[HCTR][08:38:41.347][WARNING][RK0][main]: The model name is not specified when creating the solver.\n",
      "[HCTR][08:38:41.347][INFO][RK0][main]: Global seed is 2833570033\n",
      "[HCTR][08:38:41.349][INFO][RK0][main]: Device to NUMA mapping:\n",
      "  GPU 0 ->  node 0\n",
      "[HCTR][08:38:43.163][WARNING][RK0][main]: Peer-to-peer access cannot be fully enabled.\n",
      "[HCTR][08:38:43.163][INFO][RK0][main]: Start all2all warmup\n",
      "[HCTR][08:38:43.164][INFO][RK0][main]: End all2all warmup\n",
      "[HCTR][08:38:43.164][INFO][RK0][main]: Using All-reduce algorithm: NCCL\n",
      "[HCTR][08:38:43.165][INFO][RK0][main]: Device 0: Tesla V100-SXM2-32GB\n",
      "[HCTR][08:38:43.166][INFO][RK0][main]: num of DataReader workers for train: 12\n",
      "[HCTR][08:38:43.166][INFO][RK0][main]: num of DataReader workers for eval: 12\n",
      "[HCTR][08:38:43.203][WARNING][RK0][main]: Embedding vector size(16) is not a multiple of 32, which may affect the GPU resource utilization.\n",
      "[HCTR][08:38:43.203][INFO][RK0][main]: max_num_frequent_categories is not specified using default: 1\n",
      "[HCTR][08:38:43.203][INFO][RK0][main]: max_num_infrequent_samples is not specified using default: -1\n",
      "[HCTR][08:38:43.203][INFO][RK0][main]: p_dup_max is not specified using default: 0.01\n",
      "[HCTR][08:38:43.203][INFO][RK0][main]: max_all_reduce_bandwidth is not specified using default: 1.3e+11\n",
      "[HCTR][08:38:43.203][INFO][RK0][main]: max_all_to_all_bandwidth is not specified using default: 1.9e+11\n",
      "[HCTR][08:38:43.203][INFO][RK0][main]: efficiency_bandwidth_ratio is not specified using default: 1\n",
      "[HCTR][08:38:43.203][INFO][RK0][main]: communication_type is not specified using default: IB_NVLink\n",
      "[HCTR][08:38:43.203][INFO][RK0][main]: hybrid_embedding_type is not specified using default: Distributed\n",
      "[HCTR][08:38:43.203][INFO][RK0][main]: max_vocabulary_size_per_gpu_=1441792\n",
      "[HCTR][08:38:43.205][INFO][RK0][main]: Load the model graph from dcn.json successfully\n",
      "[HCTR][08:38:43.205][INFO][RK0][main]: Graph analysis to resolve tensor dependency\n",
      "===================================================Model Compile===================================================\n",
      "[HCTR][08:38:57.092][INFO][RK0][main]: gpu0 start to init embedding\n",
      "[HCTR][08:38:57.093][INFO][RK0][main]: gpu0 init embedding done\n",
      "[HCTR][08:38:57.095][INFO][RK0][main]: Starting AUC NCCL warm-up\n",
      "[HCTR][08:38:57.099][INFO][RK0][main]: Warm-up done\n",
      "[HCTR][08:38:57.101][INFO][RK0][main]: Using Local file system backend.\n",
      "[HCTR][08:38:57.103][INFO][RK0][main]: Loading sparse model: dcn0_sparse_1000.model\n",
      "[HCTR][08:38:57.103][INFO][RK0][main]: Using Local file system backend.\n",
      "[HCTR][08:38:57.183][INFO][RK0][main]: Using Local file system backend.\n",
      "[HCTR][08:38:57.198][INFO][RK0][main]: Loading dense opt states: dcn_opt_dense_1000.model\n",
      "[HCTR][08:38:57.199][INFO][RK0][main]: Loading sparse optimizer states: dcn0_opt_sparse_1000.model\n",
      "[HCTR][08:38:57.200][INFO][RK0][main]: Rank0: Read optimzer state from file\n",
      "[HCTR][08:38:57.200][INFO][RK0][main]: Using Local file system backend.\n",
      "[HCTR][08:38:57.291][INFO][RK0][main]: Done\n",
      "[HCTR][08:38:57.291][INFO][RK0][main]: Rank0: Read optimzer state from file\n",
      "[HCTR][08:38:57.291][INFO][RK0][main]: Using Local file system backend.\n",
      "[HCTR][08:38:57.376][INFO][RK0][main]: Done\n",
      "===================================================Model Summary===================================================\n",
      "[HCTR][08:38:57.376][INFO][RK0][main]: Model structure on each GPU\n",
      "Label                                   Dense                         Sparse                        \n",
      "label                                   dense                          data1                         \n",
      "(4096,1)                                (4096,13)                               \n",
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "Layer Type                              Input Name                    Output Name                   Output Shape                  \n",
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "DistributedSlotSparseEmbeddingHash      data1                         sparse_embedding1             (4096,26,16)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Reshape                                 sparse_embedding1             reshape1                      (4096,416)                    \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Concat                                  reshape1                      concat1                       (4096,429)                    \n",
      "                                        dense                                                                                     \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Slice                                   concat1                       concat1_slice0                (4096,429)                    \n",
      "                                                                      concat1_slice1                (4096,429)                    \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "MultiCross                              concat1_slice0                multicross1                   (4096,429)                    \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            concat1_slice1                fc1                           (4096,1024)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "ReLU                                    fc1                           relu1                         (4096,1024)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Dropout                                 relu1                         dropout1                      (4096,1024)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            dropout1                      fc2                           (4096,1024)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "ReLU                                    fc2                           relu2                         (4096,1024)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Dropout                                 relu2                         dropout2                      (4096,1024)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Concat                                  dropout2                      concat2                       (4096,1453)                   \n",
      "                                        multicross1                                                                               \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            concat2                       fc3                           (4096,1)                      \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "BinaryCrossEntropyLoss                  fc3                           loss                                                        \n",
      "                                        label                                                                                     \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "=====================================================Model Fit=====================================================\n",
      "[HCTR][08:38:57.376][INFO][RK0][main]: Use non-epoch mode with number of iterations: 500\n",
      "[HCTR][08:38:57.376][INFO][RK0][main]: Training batchsize: 4096, evaluation batchsize: 4096\n",
      "[HCTR][08:38:57.376][INFO][RK0][main]: Evaluation interval: 100, snapshot interval: 10000\n",
      "[HCTR][08:38:57.376][INFO][RK0][main]: Dense network trainable: True\n",
      "[HCTR][08:38:57.376][INFO][RK0][main]: Sparse embedding sparse_embedding1 trainable: True\n",
      "[HCTR][08:38:57.376][INFO][RK0][main]: Use mixed precision: False, scaler: 1.000000, use cuda graph: True\n",
      "[HCTR][08:38:57.376][INFO][RK0][main]: lr: 0.001000, warmup_steps: 1, end_lr: 0.000000\n",
      "[HCTR][08:38:57.376][INFO][RK0][main]: decay_start: 0, decay_steps: 1, decay_power: 2.000000\n",
      "[HCTR][08:38:57.376][INFO][RK0][main]: Training source file: ./dcn_data/file_list.txt\n",
      "[HCTR][08:38:57.376][INFO][RK0][main]: Evaluation source file: ./dcn_data/file_list_test.txt\n",
      "[HCTR][08:38:57.853][INFO][RK0][main]: Iter: 50 Time(50 iters): 0.475771s Loss: 0.113989 lr:0.001\n",
      "[HCTR][08:38:58.237][INFO][RK0][main]: Iter: 100 Time(50 iters): 0.382273s Loss: 0.105428 lr:0.001\n",
      "[HCTR][08:39:01.531][INFO][RK0][main]: Evaluation, AUC: 0.746004\n",
      "[HCTR][08:39:01.531][INFO][RK0][main]: Eval Time for 1500 iters: 3.29415s\n",
      "[HCTR][08:39:01.915][INFO][RK0][main]: Iter: 150 Time(50 iters): 3.67713s Loss: 0.112908 lr:0.001\n",
      "[HCTR][08:39:02.299][INFO][RK0][main]: Iter: 200 Time(50 iters): 0.382364s Loss: 0.110116 lr:0.001\n",
      "[HCTR][08:39:05.592][INFO][RK0][main]: Evaluation, AUC: 0.743096\n",
      "[HCTR][08:39:05.592][INFO][RK0][main]: Eval Time for 1500 iters: 3.29324s\n",
      "[HCTR][08:39:05.976][INFO][RK0][main]: Iter: 250 Time(50 iters): 3.67566s Loss: 0.113728 lr:0.001\n",
      "[HCTR][08:39:06.359][INFO][RK0][main]: Iter: 300 Time(50 iters): 0.381715s Loss: 0.114037 lr:0.001\n",
      "[HCTR][08:39:09.651][INFO][RK0][main]: Evaluation, AUC: 0.744914\n",
      "[HCTR][08:39:09.651][INFO][RK0][main]: Eval Time for 1500 iters: 3.29269s\n",
      "[HCTR][08:39:10.036][INFO][RK0][main]: Iter: 350 Time(50 iters): 3.67574s Loss: 0.100788 lr:0.001\n",
      "[HCTR][08:39:10.418][INFO][RK0][main]: Iter: 400 Time(50 iters): 0.381021s Loss: 0.119661 lr:0.001\n",
      "[HCTR][08:39:13.713][INFO][RK0][main]: Evaluation, AUC: 0.739262\n",
      "[HCTR][08:39:13.713][INFO][RK0][main]: Eval Time for 1500 iters: 3.29468s\n",
      "[HCTR][08:39:14.094][INFO][RK0][main]: Iter: 450 Time(50 iters): 3.67472s Loss: 0.122573 lr:0.001\n",
      "[HCTR][08:39:14.466][INFO][RK0][main]: Finish 500 iterations with batchsize: 4096 in 17.09s.\n"
     ]
    }
   ],
   "source": [
    "!python3 dcn_continue.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "95eb57e8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Inference\n",
    "\n",
    "The HugeCTR inference is enabled by [hugectr.inference.InferenceSession.predict](https://nvidia-merlin.github.io/HugeCTR/main/api/python_interface.html#predict-method) method of InferenceSession.\n",
    "This method requires dense features, embedding columns, and row pointers of slots as the input and gives the prediction result as the output.\n",
    "We need to convert the Criteo data to inference format first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ef1896e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 ../tools/criteo_predict/criteo2predict.py --src_csv_path=dcn_data/val/test.txt --src_config=../tools/criteo_predict/dcn_data.json --dst_path=./dcn_csr.txt --batch_size=1024"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b53ce38b",
   "metadata": {},
   "source": [
    "We can then make inferences based on the saved model graph and model weights by performing the following with Python APIs:\n",
    "\n",
    "1. Configure the inference related parameters.\n",
    "2. Create the inference session.\n",
    "3. Make inference with the [hugectr.inference.InferenceSession.predict](https://nvidia-merlin.github.io/HugeCTR/main/api/python_interface.html#predict-method) method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f54d51b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing dcn_inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile dcn_inference.py\n",
    "from hugectr.inference import InferenceParams, CreateInferenceSession\n",
    "from mpi4py import MPI\n",
    "\n",
    "def calculate_accuracy(labels, output):\n",
    "    num_samples = len(labels)\n",
    "    flags = [1 if ((labels[i] == 0 and output[i] <= 0.5) or (labels[i] == 1 and output[i] > 0.5)) else 0 for i in range(num_samples)]\n",
    "    correct_samples = sum(flags)\n",
    "    return float(correct_samples)/(float(num_samples)+1e-16)\n",
    "\n",
    "data_file = open(\"dcn_csr.txt\")\n",
    "config_file = \"dcn.json\"\n",
    "labels = [int(item) for item in data_file.readline().split(' ')]\n",
    "dense_features = [float(item) for item in data_file.readline().split(' ') if item!=\"\\n\"]\n",
    "embedding_columns = [int(item) for item in data_file.readline().split(' ')]\n",
    "row_ptrs = [int(item) for item in data_file.readline().split(' ')]\n",
    "\n",
    "# create parameter server, embedding cache and inference session\n",
    "inference_params = InferenceParams(model_name = \"dcn\",\n",
    "                                max_batchsize = 1024,\n",
    "                                hit_rate_threshold = 0.6,\n",
    "                                dense_model_file = \"./dcn_dense_1000.model\",\n",
    "                                sparse_model_files = [\"./dcn0_sparse_1000.model\"],\n",
    "                                device_id = 0,\n",
    "                                use_gpu_embedding_cache = True,\n",
    "                                cache_size_percentage = 0.9,\n",
    "                                i64_input_key = False,\n",
    "                                use_mixed_precision = False)\n",
    "inference_session = CreateInferenceSession(config_file, inference_params)\n",
    "output = inference_session.predict(dense_features, embedding_columns, row_ptrs)\n",
    "accuracy = calculate_accuracy(labels, output)\n",
    "print(\"[HUGECTR][INFO] number samples: {}, accuracy: {}\".format(len(labels), accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71e89b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HCTR][08:41:25.641][INFO][RK0][main]: default_emb_vec_value is not specified using default: 0\n",
      "====================================================HPS Create====================================================\n",
      "[HCTR][08:41:25.641][INFO][RK0][main]: Creating HashMap CPU database backend...\n",
      "[HCTR][08:41:25.642][DEBUG][RK0][main]: Created blank database backend in local memory!\n",
      "[HCTR][08:41:25.642][INFO][RK0][main]: Volatile DB: initial cache rate = 1\n",
      "[HCTR][08:41:25.642][INFO][RK0][main]: Volatile DB: cache missed embeddings = 0\n",
      "[HCTR][08:41:25.642][DEBUG][RK0][main]: Created raw model loader in local memory!\n",
      "[HCTR][08:41:25.642][INFO][RK0][main]: Using Local file system backend.\n",
      "[HCTR][08:41:25.891][INFO][RK0][main]: Table: hps_et.dcn.sparse_embedding1; cached 282873 / 282873 embeddings in volatile database (HashMapBackend); load: 282873 / 18446744073709551615 (0.00%).\n",
      "[HCTR][08:41:25.892][DEBUG][RK0][main]: Real-time subscribers created!\n",
      "[HCTR][08:41:25.892][INFO][RK0][main]: Creating embedding cache in device 0.\n",
      "[HCTR][08:41:25.898][INFO][RK0][main]: Model name: dcn\n",
      "[HCTR][08:41:25.898][INFO][RK0][main]: Number of embedding tables: 1\n",
      "[HCTR][08:41:25.898][INFO][RK0][main]: Use GPU embedding cache: True, cache size percentage: 0.900000\n",
      "[HCTR][08:41:25.898][INFO][RK0][main]: Use I64 input key: False\n",
      "[HCTR][08:41:25.898][INFO][RK0][main]: Configured cache hit rate threshold: 0.600000\n",
      "[HCTR][08:41:25.898][INFO][RK0][main]: The size of thread pool: 80\n",
      "[HCTR][08:41:25.898][INFO][RK0][main]: The size of worker memory pool: 2\n",
      "[HCTR][08:41:25.898][INFO][RK0][main]: The size of refresh memory pool: 1\n",
      "[HCTR][08:41:25.898][INFO][RK0][main]: The refresh percentage : 0.000000\n",
      "[HCTR][08:41:26.851][INFO][RK0][main]: Global seed is 1715681389\n",
      "[HCTR][08:41:26.854][INFO][RK0][main]: Device to NUMA mapping:\n",
      "  GPU 0 ->  node 0\n",
      "[HCTR][08:41:27.788][WARNING][RK0][main]: Peer-to-peer access cannot be fully enabled.\n",
      "[HCTR][08:41:27.788][INFO][RK0][main]: Start all2all warmup\n",
      "[HCTR][08:41:27.788][INFO][RK0][main]: End all2all warmup\n",
      "[HCTR][08:41:27.788][INFO][RK0][main]: Model name: dcn\n",
      "[HCTR][08:41:27.788][INFO][RK0][main]: Use mixed precision: False\n",
      "[HCTR][08:41:27.788][INFO][RK0][main]: Use cuda graph: True\n",
      "[HCTR][08:41:27.788][INFO][RK0][main]: Max batchsize: 1024\n",
      "[HCTR][08:41:27.788][INFO][RK0][main]: Use I64 input key: False\n",
      "[HCTR][08:41:27.788][INFO][RK0][main]: start create embedding for inference\n",
      "[HCTR][08:41:27.788][INFO][RK0][main]: sparse_input name data1\n",
      "[HCTR][08:41:27.788][INFO][RK0][main]: create embedding for inference success\n",
      "[HCTR][08:41:27.789][INFO][RK0][main]: Inference stage skip BinaryCrossEntropyLoss layer, replaced by Sigmoid layer\n",
      "[HUGECTR][INFO] number samples: 1024, accuracy: 0.970703125\n"
     ]
    }
   ],
   "source": [
    "!python3 dcn_inference.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4cb459",
   "metadata": {},
   "source": [
    "## Wide and Deep Model\n",
    "\n",
    "### Download and Preprocess Data\n",
    "\n",
    "1. Download the Criteo dataset using the following command:\n",
    "\n",
    "   In preprocessing, we will further reduce the amounts of data to speedup the preprocessing, fill missing values, remove the feature values whose occurrences are very rare, etc. Here we choose pandas preprocessing method to make the dataset ready for HugeCTR training.\n",
    "\n",
    "2. Preprocessing by Pandas using the following command:\n",
    "   \n",
    "   The first argument represents the dataset postfix. It is 1 here since day_1 is used. The second argument wdl_data is where the preprocessed data is stored. The fourth argument (one after pandas) 1 embodies that the normalization is applied to dense features. The fifth argument 1 means that the feature crossing is applied. The last argument 100 means the number of data files in each file list.\n",
    "   \n",
    "3. Create a soft link to the dataset folder using the following command:\n",
    "\n",
    "   \n",
    "#### Run the following commands on the terminal to prepare the data for this notebook\n",
    "```shell\n",
    "export project_root=/home/hugectr # set this to the directory where hugectr is downloaded\n",
    "cd ${project_root}/tools\n",
    "# Step 1\n",
    "wget https://storage.googleapis.com/criteo-cail-datasets/day_0.gz\n",
    "#Step 2\n",
    "bash preprocess.sh 1 wdl_data pandas 1 1 100\n",
    "#Step 3\n",
    "ln -s ${project_root}/tools/wdl_data ${project_root}/notebooks/wdl_data\n",
    "```\n",
    "**Note**: It will take a while (dozens of minutes) to preprocess the dataset. Please make sure that it is finished successfully before moving forward to the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1aebf8d",
   "metadata": {},
   "source": [
    "### Train from Scratch\n",
    "\n",
    "We can train from scratch, dump the model graph to a JSON file, and save the model weights and optimizer states by performing the same steps that we followed with the DCN Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c12473",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile wdl_train.py\n",
    "import hugectr\n",
    "from mpi4py import MPI\n",
    "solver = hugectr.CreateSolver(max_eval_batches = 5000,\n",
    "                              batchsize_eval = 1024,\n",
    "                              batchsize = 1024,\n",
    "                              lr = 0.001,\n",
    "                              vvgpu = [[0]],\n",
    "                              i64_input_key = False,\n",
    "                              use_mixed_precision = False,\n",
    "                              repeat_dataset = False,\n",
    "                              use_cuda_graph = True)\n",
    "reader = hugectr.DataReaderParams(data_reader_type = hugectr.DataReaderType_t.Norm,\n",
    "                          source = [\"wdl_data/file_list.0.txt\"],\n",
    "                          eval_source = \"wdl_data/file_list.1.txt\",\n",
    "                          check_type = hugectr.Check_t.Sum)\n",
    "optimizer = hugectr.CreateOptimizer(optimizer_type = hugectr.Optimizer_t.Adam)\n",
    "model = hugectr.Model(solver, reader, optimizer)\n",
    "model.add(hugectr.Input(label_dim = 1, label_name = \"label\",\n",
    "                        dense_dim = 13, dense_name = \"dense\",\n",
    "                        data_reader_sparse_param_array = \n",
    "                        [hugectr.DataReaderSparseParam(\"wide_data\", 30, True, 1),\n",
    "                        hugectr.DataReaderSparseParam(\"deep_data\", 2, False, 26)]))\n",
    "model.add(hugectr.SparseEmbedding(embedding_type = hugectr.Embedding_t.DistributedSlotSparseEmbeddingHash, \n",
    "                            workspace_size_per_gpu_in_mb = 69,\n",
    "                            embedding_vec_size = 1,\n",
    "                            combiner = \"sum\",\n",
    "                            sparse_embedding_name = \"sparse_embedding2\",\n",
    "                            bottom_name = \"wide_data\",\n",
    "                            optimizer = optimizer))\n",
    "model.add(hugectr.SparseEmbedding(embedding_type = hugectr.Embedding_t.DistributedSlotSparseEmbeddingHash, \n",
    "                            workspace_size_per_gpu_in_mb = 1074,\n",
    "                            embedding_vec_size = 16,\n",
    "                            combiner = \"sum\",\n",
    "                            sparse_embedding_name = \"sparse_embedding1\",\n",
    "                            bottom_name = \"deep_data\",\n",
    "                            optimizer = optimizer))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Reshape,\n",
    "                            bottom_names = [\"sparse_embedding1\"],\n",
    "                            top_names = [\"reshape1\"],\n",
    "                            leading_dim=416))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Reshape,\n",
    "                            bottom_names = [\"sparse_embedding2\"],\n",
    "                            top_names = [\"reshape2\"],\n",
    "                            leading_dim=1))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Concat,\n",
    "                            bottom_names = [\"reshape1\", \"dense\"], top_names = [\"concat1\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"concat1\"],\n",
    "                            top_names = [\"fc1\"],\n",
    "                            num_output=1024))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc1\"],\n",
    "                            top_names = [\"relu1\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Dropout,\n",
    "                            bottom_names = [\"relu1\"],\n",
    "                            top_names = [\"dropout1\"],\n",
    "                            dropout_rate=0.5))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"dropout1\"],\n",
    "                            top_names = [\"fc2\"],\n",
    "                            num_output=1024))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc2\"],\n",
    "                            top_names = [\"relu2\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Dropout,\n",
    "                            bottom_names = [\"relu2\"],\n",
    "                            top_names = [\"dropout2\"],\n",
    "                            dropout_rate=0.5))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"dropout2\"],\n",
    "                            top_names = [\"fc3\"],\n",
    "                            num_output=1))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Add,\n",
    "                            bottom_names = [\"fc3\", \"reshape2\"],\n",
    "                            top_names = [\"add1\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.BinaryCrossEntropyLoss,\n",
    "                            bottom_names = [\"add1\", \"label\"],\n",
    "                            top_names = [\"loss\"]))\n",
    "model.compile()\n",
    "model.summary()\n",
    "model.graph_to_json(graph_config_file = \"wdl.json\")\n",
    "model.fit(num_epochs = 1, display = 500, eval_interval = 500, snapshot = 4000, snapshot_prefix = \"wdl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a54ff32",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 wdl_train.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "79f88be7",
   "metadata": {},
   "source": [
    "### Fine-tuning\n",
    "\n",
    "We can only load the sparse embedding layers, their corresponding weights, and then construct a new dense network. The dense weights will be trained first and the sparse weights will be fine-tuned later. We can achieve this by performing the following with Python APIs:\n",
    "\n",
    "1. Create the solver, reader and optimizer, then initialize the model.\n",
    "2. Load the sparse embedding layers from the saved JSON file.\n",
    "3. Add the dense layers on top of the loaded model graph.\n",
    "4. Compile the model and have an overview of the model graph.\n",
    "5. Load the sparse weights and [freeze](https://nvidia-merlin.github.io/HugeCTR/main/api/python_interface.html#freeze-embedding-method) the sparse embedding layers.\n",
    "6. Train the dense weights.\n",
    "7. [Unfreeze](https://nvidia-merlin.github.io/HugeCTR/main/api/python_interface.html#unfreeze-embedding-method) the sparse embedding layers and [freeze](https://nvidia-merlin.github.io/HugeCTR/main/api/python_interface.html#freeze-dense-method) the dense layers, reset the learning rate scheduler with a small rate.\n",
    "8. Fine-tune the sparse weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60908e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile wdl_fine_tune.py\n",
    "import hugectr\n",
    "from mpi4py import MPI\n",
    "solver = hugectr.CreateSolver(max_eval_batches = 5000,\n",
    "                              batchsize_eval = 1024,\n",
    "                              batchsize = 1024,\n",
    "                              vvgpu = [[0]],\n",
    "                              i64_input_key = False,\n",
    "                              use_mixed_precision = False,\n",
    "                              repeat_dataset = False,\n",
    "                              use_cuda_graph = True)\n",
    "reader = hugectr.DataReaderParams(data_reader_type = hugectr.DataReaderType_t.Norm,\n",
    "                          source = [\"wdl_data/file_list.2.txt\"],\n",
    "                          eval_source = \"wdl_data/file_list.3.txt\",\n",
    "                          check_type = hugectr.Check_t.Sum)\n",
    "optimizer = hugectr.CreateOptimizer(optimizer_type = hugectr.Optimizer_t.Adam)\n",
    "model = hugectr.Model(solver, reader, optimizer)\n",
    "model.construct_from_json(graph_config_file = \"wdl.json\", include_dense_network = False)\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Reshape,\n",
    "                            bottom_names = [\"sparse_embedding1\"],\n",
    "                            top_names = [\"reshape1\"],\n",
    "                            leading_dim=416))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Reshape,\n",
    "                            bottom_names = [\"sparse_embedding2\"],\n",
    "                            top_names = [\"reshape2\"],\n",
    "                            leading_dim=1))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Concat,\n",
    "                            bottom_names = [\"reshape1\", \"reshape2\", \"dense\"], top_names = [\"concat1\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"concat1\"],\n",
    "                            top_names = [\"fc1\"],\n",
    "                            num_output=1024))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc1\"],\n",
    "                            top_names = [\"relu1\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Dropout,\n",
    "                            bottom_names = [\"relu1\"],\n",
    "                            top_names = [\"dropout1\"],\n",
    "                            dropout_rate=0.5))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"dropout1\"],\n",
    "                            top_names = [\"fc2\"],\n",
    "                            num_output=1))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.BinaryCrossEntropyLoss,\n",
    "                            bottom_names = [\"fc2\", \"label\"],\n",
    "                            top_names = [\"loss\"]))\n",
    "model.compile()\n",
    "model.summary()\n",
    "model.load_sparse_weights([\"wdl0_sparse_4000.model\", \"wdl1_sparse_4000.model\"])\n",
    "model.freeze_embedding()\n",
    "model.fit(num_epochs = 1, display = 500, eval_interval = 1000, snapshot = 100000, snapshot_prefix = \"wdl\")\n",
    "model.unfreeze_embedding()\n",
    "model.freeze_dense()\n",
    "model.reset_learning_rate_scheduler(base_lr = 0.0001)\n",
    "model.fit(num_epochs = 2, display = 500, eval_interval = 1000, snapshot = 100000, snapshot_prefix = \"wdl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6af6081",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 wdl_fine_tune.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8847365b",
   "metadata": {},
   "source": [
    "### Load Pre-trained Embeddings\n",
    "\n",
    "If you have the pre-trained embeddings in other formats, you can convert them to the HugeCTR sparse models and then [load](https://nvidia-merlin.github.io/HugeCTR/main/api/python_interface.html#load-sparse-weights-method) them to facilitate the training process. For the sake of simplicity and generality, we represent the pretrained embeddings with the dictionary of randomly initialized numpy arrays, of which the keys indicate the embedding keys and the array values embody the embedding values. It is worth mentioning that there are two embedding tables for the Wide&Deep model, and here we only load the pre-trained embeddings for one table and freeze the corresponding embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6b1785",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile wdl_load_pretrained.py\n",
    "import hugectr\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "import os\n",
    "import struct\n",
    "solver = hugectr.CreateSolver(max_eval_batches = 5000,\n",
    "                              batchsize_eval = 1024,\n",
    "                              batchsize = 1024,\n",
    "                              vvgpu = [[0]],\n",
    "                              i64_input_key = False,\n",
    "                              use_mixed_precision = False,\n",
    "                              repeat_dataset = False,\n",
    "                              use_cuda_graph = True)\n",
    "reader = hugectr.DataReaderParams(data_reader_type = hugectr.DataReaderType_t.Norm,\n",
    "                          source = [\"wdl_data/file_list.0.txt\"],\n",
    "                          eval_source = \"wdl_data/file_list.1.txt\",\n",
    "                          check_type = hugectr.Check_t.Sum)\n",
    "optimizer = hugectr.CreateOptimizer(optimizer_type = hugectr.Optimizer_t.Adam)\n",
    "model = hugectr.Model(solver, reader, optimizer)\n",
    "model.construct_from_json(graph_config_file = \"wdl.json\", include_dense_network = True)\n",
    "model.compile()\n",
    "model.summary()\n",
    "\n",
    "def convert_pretrained_embeddings_to_sparse_model(pre_trained_sparse_embeddings, hugectr_sparse_model, embedding_vec_size):\n",
    "    os.system(\"mkdir -p {}\".format(hugectr_sparse_model))\n",
    "    with open(\"{}/key\".format(hugectr_sparse_model), 'wb') as key_file, \\\n",
    "        open(\"{}/emb_vector\".format(hugectr_sparse_model), 'wb') as vec_file:\n",
    "      for key in pre_trained_sparse_embeddings:\n",
    "        vec = pre_trained_sparse_embeddings[key]\n",
    "        key_struct = struct.pack('q', key)\n",
    "        vec_struct = struct.pack(str(embedding_vec_size) + \"f\", *vec)\n",
    "        key_file.write(key_struct)\n",
    "        vec_file.write(vec_struct)\n",
    "\n",
    "# Convert the pretrained embeddings\n",
    "pretrained_embeddings = dict()\n",
    "hugectr_sparse_model = \"wdl1_pretrained.model\"\n",
    "embedding_vec_size = 16\n",
    "key_range = (0, 100000)\n",
    "for key in range(key_range[0], key_range[1]):\n",
    "    pretrained_embeddings[key] = np.random.randn(embedding_vec_size).astype(np.float32)\n",
    "convert_pretrained_embeddings_to_sparse_model(pretrained_embeddings, hugectr_sparse_model, embedding_vec_size)\n",
    "print(\"Successfully convert pretrained embeddings to {}\".format(hugectr_sparse_model))\n",
    "\n",
    "# Load the pretrained sparse models\n",
    "model.load_sparse_weights({\"sparse_embedding1\": hugectr_sparse_model})\n",
    "model.freeze_embedding(\"sparse_embedding1\")\n",
    "model.fit(num_epochs = 1, display = 500, eval_interval = 1000, snapshot = 100000, snapshot_prefix = \"wdl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1efe377",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 wdl_load_pretrained.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1378815",
   "metadata": {},
   "source": [
    "### Low-level Training\n",
    "\n",
    "The low-level training APIs are maintained in the enhanced HugeCTR Python interface. If you want to have precise control of each training iteration and each evaluation step, you may find it helpful to use these APIs. Since the data reader behavior is different in epoch mode and non-epoch mode, we should pay attention to how to tweak the data reader when using low-level training.\n",
    "We will demonstrate how to write the low-level training scripts for non-epoch mode, epoch mode, and embedding training cache mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc97b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile wdl_non_epoch.py\n",
    "import hugectr\n",
    "from mpi4py import MPI\n",
    "solver = hugectr.CreateSolver(max_eval_batches = 5000,\n",
    "                              batchsize_eval = 1024,\n",
    "                              batchsize = 1024,\n",
    "                              vvgpu = [[0]],\n",
    "                              i64_input_key = False,\n",
    "                              use_mixed_precision = False,\n",
    "                              repeat_dataset = True,\n",
    "                              use_cuda_graph = True)\n",
    "reader = hugectr.DataReaderParams(data_reader_type = hugectr.DataReaderType_t.Norm,\n",
    "                          source = [\"wdl_data/file_list.0.txt\"],\n",
    "                          eval_source = \"wdl_data/file_list.1.txt\",\n",
    "                          check_type = hugectr.Check_t.Sum)\n",
    "optimizer = hugectr.CreateOptimizer(optimizer_type = hugectr.Optimizer_t.Adam)\n",
    "model = hugectr.Model(solver, reader, optimizer)\n",
    "model.construct_from_json(graph_config_file = \"wdl.json\", include_dense_network = True)\n",
    "model.compile()\n",
    "model.start_data_reading()\n",
    "lr_sch = model.get_learning_rate_scheduler()\n",
    "max_iter = 2000\n",
    "for i in range(max_iter):\n",
    "    lr = lr_sch.get_next()\n",
    "    model.set_learning_rate(lr)\n",
    "    model.train()\n",
    "    if (i%100 == 0):\n",
    "        loss = model.get_current_loss()\n",
    "        print(\"[HUGECTR][INFO] iter: {}; loss: {}\".format(i, loss))\n",
    "    if (i%1000 == 0 and i != 0):\n",
    "        for _ in range(solver.max_eval_batches):\n",
    "            model.eval()\n",
    "        metrics = model.get_eval_metrics()\n",
    "        print(\"[HUGECTR][INFO] iter: {}, {}\".format(i, metrics))\n",
    "model.save_params_to_files(\"./\", max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca7d711",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 wdl_non_epoch.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c218a76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile wdl_epoch.py\n",
    "import hugectr\n",
    "from mpi4py import MPI\n",
    "solver = hugectr.CreateSolver(max_eval_batches = 5000,\n",
    "                              batchsize_eval = 1024,\n",
    "                              batchsize = 1024,\n",
    "                              vvgpu = [[0]],\n",
    "                              i64_input_key = False,\n",
    "                              use_mixed_precision = False,\n",
    "                              repeat_dataset = False,\n",
    "                              use_cuda_graph = True)\n",
    "reader = hugectr.DataReaderParams(data_reader_type = hugectr.DataReaderType_t.Norm,\n",
    "                          source = [\"wdl_data/file_list.0.txt\"],\n",
    "                          eval_source = \"wdl_data/file_list.1.txt\",\n",
    "                          check_type = hugectr.Check_t.Sum)\n",
    "optimizer = hugectr.CreateOptimizer(optimizer_type = hugectr.Optimizer_t.Adam)\n",
    "model = hugectr.Model(solver, reader, optimizer)\n",
    "model.construct_from_json(graph_config_file = \"wdl.json\", include_dense_network = True)\n",
    "model.compile()\n",
    "lr_sch = model.get_learning_rate_scheduler()\n",
    "data_reader_train = model.get_data_reader_train()\n",
    "data_reader_eval = model.get_data_reader_eval()\n",
    "data_reader_eval.set_source()\n",
    "data_reader_eval_flag = True\n",
    "iteration = 0\n",
    "for epoch in range(2):\n",
    "  print(\"[HUGECTR][INFO] epoch: \", epoch)\n",
    "  data_reader_train.set_source()\n",
    "  data_reader_train_flag = True\n",
    "  while True:\n",
    "    lr = lr_sch.get_next()\n",
    "    model.set_learning_rate(lr)\n",
    "    data_reader_train_flag = model.train()\n",
    "    if not data_reader_train_flag:\n",
    "      break\n",
    "    if iteration % 1000 == 0:\n",
    "      batches = 0\n",
    "      while data_reader_eval_flag:\n",
    "        if batches >= solver.max_eval_batches:\n",
    "          break\n",
    "        data_reader_eval_flag = model.eval()\n",
    "        batches += 1\n",
    "      if not data_reader_eval_flag:\n",
    "        data_reader_eval.set_source()\n",
    "        data_reader_eval_flag = True\n",
    "      metrics = model.get_eval_metrics()\n",
    "      print(\"[HUGECTR][INFO] iter: {}, metrics: {}\".format(iteration, metrics))\n",
    "    iteration += 1\n",
    "model.save_params_to_files(\"./\", iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746b9fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 wdl_epoch.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
