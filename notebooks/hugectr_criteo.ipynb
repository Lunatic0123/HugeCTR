{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "amber-netscape",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# HugeCTR Python Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "professional-burning",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In HugeCTR version 3.1, we provide an enhanced Python inferface, which supports continuous training and inference with high-level APIs. There are three main improvements. Firstly, the model graph can be constructed and dumped to the JSON file with Python code and it saves users from writing JSON configuration files. Secondly, we support the feature of model oversubscription with high-level APIs and extend it further for online training cases. Thirdly, the freezing method is provided for both sparse embedding and dense network, which enables transfer learning and fine-tune for CTR tasks.\n",
    "\n",
    "This notebook explains how to access and use the enhanced HugeCTR Python interface. Please NOTE that the low-level training APIs are still maintained for users who want to have precise control of each training iteration, while migrating to the high-level training APIs is strongly recommended. For more details of the usage of Python API, please refer to [HugeCTR Python Interface](../docs/python_interface.md).\n",
    "\n",
    "## Table of Contents\n",
    "-  [Access the HugeCTR Python Interface](#1)\n",
    "-  [DCN Demo](#2)\n",
    "   * [Download and Preprocess Data](#21)\n",
    "   * [Train from Scratch](#22)\n",
    "   * [Continue Training](#23)\n",
    "   * [Inference](#24)\n",
    "-  [Wide&Deep Demo](#3)\n",
    "   * [Download and Preprocess Data](#31)\n",
    "   * [Model Oversubscription](#32)\n",
    "   * [Fine-tune](#33)\n",
    "   * [Low-level Training](#34)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twenty-corpus",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "## 1. Access the HugeCTR Python Interface\n",
    "\n",
    "1. Please make sure that you have started the notebook inside the running NGC docker container: `nvcr.io/nvidia/merlin/merlin-training:0.6`.\n",
    "\n",
    "   A dynamic link to the `hugectr.so` library is installed to the system path `/usr/local/hugectr/lib/`. Besides, this system path is added to the environment variable `PYTHONPATH`, which means that you can use the Python interface within the docker container environment. Check the dynamic link with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparable-hudson",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /usr/local/hugectr/lib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "underlying-filter",
   "metadata": {},
   "source": [
    "2. Import HugeCTR, in order to train your model with Python as shown here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "medieval-daniel",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hugectr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intelligent-peeing",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "## 2. DCN Demo\n",
    "\n",
    "<a id=\"21\"></a>\n",
    "### 2.1 Download and Preprocess Data\n",
    "1. Download the Kaggle Criteo dataset using the following command:\n",
    "   ```shell\n",
    "   $ cd ${project-root}/tools\n",
    "   $ wget http://azuremlsampleexperiments.blob.core.windows.net/criteo/day_1.gz\n",
    "   ```\n",
    "   \n",
    "   In preprocessing, we will further reduce the amounts of data to speedup the preprocessing, fill missing values, remove the feature values whose occurrences are very rare, etc. Here we choose pandas preprocessing method to make the dataset ready for HugeCTR training.\n",
    "\n",
    "2. Preprocessing by Pandas using the following command:\n",
    "   ```shell\n",
    "   $ bash preprocess.sh 1 dcn_data pandas 1 0\n",
    "   ```\n",
    "   \n",
    "   The first argument represents the dataset postfix. It is 1 here since day_1 is used. The second argument dcn_data is where the preprocessed data is stored. The fourth arguement (one after pandas) 1 embodies that the normalization is applied to dense features. The last argument 0 means that the feature crossing is not applied.\n",
    "\n",
    "3. Create a soft link to the dataset folder using the following command:\n",
    "   ```shell\n",
    "   $ ln ${project-root}/tools/dcn_data ${project_root}/notebooks/dcn_data\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broadband-dynamics",
   "metadata": {},
   "source": [
    "<a id=\"22\"></a>\n",
    "### 2.2 Train from Scratch\n",
    "\n",
    "We can train fom scratch, dump the model graph to a JSON file, and save the model weights and optimizer states by doing the following with Python APIs:\n",
    "\n",
    "1. Create the solver, reader and optimizer, then initialize the model.\n",
    "2. Construct the model graph by adding input, sparse embedding and dense layers in order.\n",
    "3. Compile the model and have an overview of the model graph.\n",
    "4. Dump the model graph to the JSON file.\n",
    "5. Fit the model, save the model weights and optimizer states implicitly.\n",
    "\n",
    "Please note that the training mode is determined by `repeat_dataset` within `hugectr.CreateSolver`. If it is True, the non-epoch mode training will be adopted and the maximum iterations should be specified by `max_iter` within `hugectr.Model.fit`. If it is False, then the epoch-mode training will be adopted and the number of epochs should be specified by `num_epochs` within `hugectr.Model.fit`.\n",
    "\n",
    "The optimizer that is used to initialize the model applies to the weights of dense layers, while the optimizer for each sparse embedding layer can be specified independently within `hugectr.SparseEmbedding`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "suited-input",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dcn_train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile dcn_train.py\n",
    "import hugectr\n",
    "from mpi4py import MPI\n",
    "solver = hugectr.CreateSolver(max_eval_batches = 1500,\n",
    "                              batchsize_eval = 4096,\n",
    "                              batchsize = 4096,\n",
    "                              lr = 0.001,\n",
    "                              vvgpu = [[3]],\n",
    "                              i64_input_key = False,\n",
    "                              use_mixed_precision = False,\n",
    "                              repeat_dataset = True,\n",
    "                              use_cuda_graph = True)\n",
    "reader = hugectr.DataReaderParams(data_reader_type = hugectr.DataReaderType_t.Norm,\n",
    "                                  source = [\"./dcn_test_norm/file_list.txt\"], #./dcn_data/file_list.txt\n",
    "                                  eval_source = \"./dcn_test_norm/file_list_test.txt\", #./dcn_data/file_list_test.txt\n",
    "                                  check_type = hugectr.Check_t.Sum)\n",
    "optimizer = hugectr.CreateOptimizer(optimizer_type = hugectr.Optimizer_t.Adam)\n",
    "model = hugectr.Model(solver, reader, optimizer)\n",
    "model.add(hugectr.Input(label_dim = 1, label_name = \"label\",\n",
    "                        dense_dim = 13, dense_name = \"dense\",\n",
    "                        data_reader_sparse_param_array = \n",
    "                        [hugectr.DataReaderSparseParam(\"data1\", 2, False, 26)]))\n",
    "model.add(hugectr.SparseEmbedding(embedding_type = hugectr.Embedding_t.DistributedSlotSparseEmbeddingHash, \n",
    "                            workspace_size_per_gpu_in_mb = 88,\n",
    "                            embedding_vec_size = 16,\n",
    "                            combiner = \"sum\",\n",
    "                            sparse_embedding_name = \"sparse_embedding1\",\n",
    "                            bottom_name = \"data1\",\n",
    "                            optimizer = optimizer))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Reshape,\n",
    "                            bottom_names = [\"sparse_embedding1\"],\n",
    "                            top_names = [\"reshape1\"],\n",
    "                            leading_dim=416))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Concat,\n",
    "                            bottom_names = [\"reshape1\", \"dense\"], top_names = [\"concat1\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Slice,\n",
    "                            bottom_names = [\"concat1\"],\n",
    "                            top_names = [\"slice11\", \"slice12\"],\n",
    "                            ranges=[(0,429),(0,429)]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.MultiCross,\n",
    "                            bottom_names = [\"slice11\"],\n",
    "                            top_names = [\"multicross1\"],\n",
    "                            num_layers=6))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"slice12\"],\n",
    "                            top_names = [\"fc1\"],\n",
    "                            num_output=1024))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc1\"],\n",
    "                            top_names = [\"relu1\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Dropout,\n",
    "                            bottom_names = [\"relu1\"],\n",
    "                            top_names = [\"dropout1\"],\n",
    "                            dropout_rate=0.5))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"dropout1\"],\n",
    "                            top_names = [\"fc2\"],\n",
    "                            num_output=1024))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc2\"],\n",
    "                            top_names = [\"relu2\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Dropout,\n",
    "                            bottom_names = [\"relu2\"],\n",
    "                            top_names = [\"dropout2\"],\n",
    "                            dropout_rate=0.5))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Concat,\n",
    "                            bottom_names = [\"dropout2\", \"multicross1\"],\n",
    "                            top_names = [\"concat2\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"concat2\"],\n",
    "                            top_names = [\"fc3\"],\n",
    "                            num_output=1))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.BinaryCrossEntropyLoss,\n",
    "                            bottom_names = [\"fc3\", \"label\"],\n",
    "                            top_names = [\"loss\"]))\n",
    "model.compile()\n",
    "model.summary()\n",
    "model.graph_to_json(graph_config_file = \"dcn.json\")\n",
    "model.fit(max_iter = 1200, display = 500, eval_interval = 100, snapshot = 1000, snapshot_prefix = \"dcn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "million-flower",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================Model Init=====================================================\n",
      "[12d04h35m26s][HUGECTR][INFO]: Global seed is 2222470846\n",
      "[12d04h35m29s][HUGECTR][INFO]: Peer-to-peer access cannot be fully enabled.\n",
      "Device 3: Tesla T4\n",
      "[12d04h35m29s][HUGECTR][INFO]: num of DataReader workers: 12\n",
      "[12d04h35m29s][HUGECTR][INFO]: max_vocabulary_size_per_gpu_=1441792\n",
      "===================================================Model Compile===================================================\n",
      "[12d04h36m02s][HUGECTR][INFO]: gpu0 start to init embedding\n",
      "[12d04h36m02s][HUGECTR][INFO]: gpu0 init embedding done\n",
      "===================================================Model Summary===================================================\n",
      "Label                                   Dense                         Sparse                        \n",
      "label                                   dense                          data1                         \n",
      "(None, 1)                               (None, 13)                              \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Layer Type                              Input Name                    Output Name                   Output Shape                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "DistributedSlotSparseEmbeddingHash      data1                         sparse_embedding1             (None, 26, 16)                \n",
      "Reshape                                 sparse_embedding1             reshape1                      (None, 416)                   \n",
      "Concat                                  reshape1,dense                concat1                       (None, 429)                   \n",
      "Slice                                   concat1                       slice11,slice12                                             \n",
      "MultiCross                              slice11                       multicross1                   (None, 429)                   \n",
      "InnerProduct                            slice12                       fc1                           (None, 1024)                  \n",
      "ReLU                                    fc1                           relu1                         (None, 1024)                  \n",
      "Dropout                                 relu1                         dropout1                      (None, 1024)                  \n",
      "InnerProduct                            dropout1                      fc2                           (None, 1024)                  \n",
      "ReLU                                    fc2                           relu2                         (None, 1024)                  \n",
      "Dropout                                 relu2                         dropout2                      (None, 1024)                  \n",
      "Concat                                  dropout2,multicross1          concat2                       (None, 1453)                  \n",
      "InnerProduct                            concat2                       fc3                           (None, 1)                     \n",
      "BinaryCrossEntropyLoss                  fc3,label                     loss                                                        \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "[12d40h36m20s][HUGECTR][INFO]: Save the model graph to dcn.json, successful\n",
      "=====================================================Model Fit=====================================================\n",
      "[12d40h36m20s][HUGECTR][INFO]: Use non-epoch mode with number of iterations: 1200\n",
      "[12d40h36m20s][HUGECTR][INFO]: Training batchsize: 4096, evaluation batchsize: 4096\n",
      "[12d40h36m20s][HUGECTR][INFO]: Evaluation interval: 100, snapshot interval: 1000\n",
      "[12d40h36m20s][HUGECTR][INFO]: Sparse embedding trainable: 1, dense network trainable: 1\n",
      "[12d40h36m20s][HUGECTR][INFO]: Use mixed precision: 0, scaler: 1.000000, use cuda graph: 1\n",
      "[12d40h36m20s][HUGECTR][INFO]: lr: 0.001000, warmup_steps: 1, decay_start: 0, decay_steps: 1, decay_power: 2.000000, end_lr: 0.000000\n",
      "[12d40h36m20s][HUGECTR][INFO]: Training source file: ./dcn_test_norm/file_list.txt\n",
      "[12d40h36m20s][HUGECTR][INFO]: Evaluation source file: ./dcn_test_norm/file_list_test.txt\n",
      "[12d40h36m13s][HUGECTR][INFO]: Evaluation, AUC: 0.726876\n",
      "[12d40h36m13s][HUGECTR][INFO]: Eval Time for 1500 iters: 9.146584s\n",
      "[12d40h36m24s][HUGECTR][INFO]: Evaluation, AUC: 0.743489\n",
      "[12d40h36m24s][HUGECTR][INFO]: Eval Time for 1500 iters: 9.256361s\n",
      "[12d40h36m36s][HUGECTR][INFO]: Evaluation, AUC: 0.750275\n",
      "[12d40h36m36s][HUGECTR][INFO]: Eval Time for 1500 iters: 9.276145s\n",
      "[12d40h36m47s][HUGECTR][INFO]: Evaluation, AUC: 0.753153\n",
      "[12d40h36m47s][HUGECTR][INFO]: Eval Time for 1500 iters: 9.292409s\n",
      "[12d40h36m49s][HUGECTR][INFO]: Iter: 500 Time(500 iters): 47.042388s Loss: 0.126962 lr:0.001000\n",
      "[12d40h36m58s][HUGECTR][INFO]: Evaluation, AUC: 0.757515\n",
      "[12d40h36m58s][HUGECTR][INFO]: Eval Time for 1500 iters: 9.298578s\n",
      "[12d40h37m10s][HUGECTR][INFO]: Evaluation, AUC: 0.759702\n",
      "[12d40h37m10s][HUGECTR][INFO]: Eval Time for 1500 iters: 9.293401s\n",
      "[12d40h37m21s][HUGECTR][INFO]: Evaluation, AUC: 0.762518\n",
      "[12d40h37m21s][HUGECTR][INFO]: Eval Time for 1500 iters: 9.285566s\n",
      "[12d40h37m32s][HUGECTR][INFO]: Evaluation, AUC: 0.763844\n",
      "[12d40h37m32s][HUGECTR][INFO]: Eval Time for 1500 iters: 9.266832s\n",
      "[12d40h37m44s][HUGECTR][INFO]: Evaluation, AUC: 0.760928\n",
      "[12d40h37m44s][HUGECTR][INFO]: Eval Time for 1500 iters: 9.245463s\n",
      "[12d40h37m46s][HUGECTR][INFO]: Iter: 1000 Time(500 iters): 56.443762s Loss: 0.109272 lr:0.001000\n",
      "[12d40h37m55s][HUGECTR][INFO]: Evaluation, AUC: 0.746082\n",
      "[12d40h37m55s][HUGECTR][INFO]: Eval Time for 1500 iters: 9.236829s\n",
      "[12d40h37m55s][HUGECTR][INFO]: Rank0: Write hash table to file\n",
      "[12d40h37m55s][HUGECTR][INFO]: Dumping sparse weights to files, successful\n",
      "[12d40h37m55s][HUGECTR][INFO]: Rank0: Write optimzer state to file\n",
      "[12d40h37m55s][HUGECTR][INFO]: Done\n",
      "[12d40h37m55s][HUGECTR][INFO]: Rank0: Write optimzer state to file\n",
      "[12d40h37m55s][HUGECTR][INFO]: Done\n",
      "[12d40h37m55s][HUGECTR][INFO]: Dumping sparse optimzer states to files, successful\n",
      "[12d40h37m55s][HUGECTR][INFO]: Dumping dense weights to file, successful\n",
      "[12d40h37m55s][HUGECTR][INFO]: Dumping dense optimizer states to file, successful\n",
      "[12d40h37m55s][HUGECTR][INFO]: Dumping untrainable weights to file, successful\n",
      "[12d40h38m60s][HUGECTR][INFO]: Evaluation, AUC: 0.744923\n",
      "[12d40h38m60s][HUGECTR][INFO]: Eval Time for 1500 iters: 9.289410s\n"
     ]
    }
   ],
   "source": [
    "!python3 dcn_train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imposed-czech",
   "metadata": {},
   "source": [
    "<a id=\"23\"></a>\n",
    "### 2.3 Continue Training\n",
    "\n",
    "We can continue our training based on the saved model graph, model weights and optimizer states by doing the following with Python APIs:\n",
    "\n",
    "1. Create the solver, reader and optimizer, then initialize the model.\n",
    "2. Construct the model graph from the saved JSON file.\n",
    "3. Compile the model and have an overview of the model graph.\n",
    "4. Load the model weights and optimizer states.\n",
    "5. Fit the model, save the model weights and optimizer states implicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "supposed-intersection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dcn0_opt_sparse_1000.model  dcn_dense_1000.model  dcn_opt_dense_1000.model\n",
      "\n",
      "dcn0_sparse_1000.model:\n",
      "emb_vector  key\n"
     ]
    }
   ],
   "source": [
    "!ls *.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "verbal-representation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dcn_continue.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile dcn_continue.py\n",
    "import hugectr\n",
    "from mpi4py import MPI\n",
    "solver = hugectr.CreateSolver(max_eval_batches = 1500,\n",
    "                              batchsize_eval = 4096,\n",
    "                              batchsize = 4096,\n",
    "                              vvgpu = [[3]],\n",
    "                              i64_input_key = False,\n",
    "                              use_mixed_precision = False,\n",
    "                              repeat_dataset = True,\n",
    "                              use_cuda_graph = True)\n",
    "reader = hugectr.DataReaderParams(data_reader_type = hugectr.DataReaderType_t.Norm,\n",
    "                                  source = [\"./dcn_test_norm/file_list.txt\"], #\"./dcn_data/file_list.txt\"\n",
    "                                  eval_source = \"./dcn_test_norm/file_list_test.txt\", #\"./dcn_data/file_list_test.txt\"\n",
    "                                  check_type = hugectr.Check_t.Sum)\n",
    "optimizer = hugectr.CreateOptimizer(optimizer_type = hugectr.Optimizer_t.Adam)\n",
    "model = hugectr.Model(solver, reader, optimizer)\n",
    "model.construct_from_json(graph_config_file = \"dcn.json\", include_dense_network = True)\n",
    "model.compile()\n",
    "model.load_dense_weights(\"dcn_dense_1000.model\")\n",
    "model.load_sparse_weights([\"dcn0_sparse_1000.model\"])\n",
    "model.load_dense_optimizer_states(\"dcn_opt_dense_1000.model\")\n",
    "model.load_sparse_optimizer_states([\"dcn0_opt_sparse_1000.model\"])\n",
    "model.summary()\n",
    "model.fit(max_iter = 500, display = 50, eval_interval = 100, snapshot = 10000, snapshot_prefix = \"dcn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "floral-finland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================Model Init=====================================================\n",
      "[12d04h50m55s][HUGECTR][INFO]: Global seed is 2635854291\n",
      "[12d04h50m58s][HUGECTR][INFO]: Peer-to-peer access cannot be fully enabled.\n",
      "Device 3: Tesla T4\n",
      "[12d04h50m58s][HUGECTR][INFO]: num of DataReader workers: 12\n",
      "[12d04h50m58s][HUGECTR][INFO]: max_vocabulary_size_per_gpu_=1441792\n",
      "[12d04h50m58s][HUGECTR][INFO]: Load the model graph from dcn.json, successful\n",
      "===================================================Model Compile===================================================\n",
      "[12d04h51m30s][HUGECTR][INFO]: gpu0 start to init embedding\n",
      "[12d04h51m30s][HUGECTR][INFO]: gpu0 init embedding done\n",
      "[12d04h51m30s][HUGECTR][INFO]: Loading dense model: dcn_dense_1000.model\n",
      "[12d04h51m30s][HUGECTR][INFO]: Loading sparse model: dcn0_sparse_1000.model\n",
      "[12d04h51m30s][HUGECTR][INFO]: Loading dense opt states: dcn_opt_dense_1000.model\n",
      "[12d04h51m30s][HUGECTR][INFO]: Loading sparse optimizer states: dcn0_opt_sparse_1000.model\n",
      "[12d04h51m30s][HUGECTR][INFO]: Rank0: Read optimzer state from file\n",
      "[12d04h51m30s][HUGECTR][INFO]: Done\n",
      "[12d04h51m30s][HUGECTR][INFO]: Rank0: Read optimzer state from file\n",
      "[12d04h51m30s][HUGECTR][INFO]: Done\n",
      "===================================================Model Summary===================================================\n",
      "Label                                   Dense                         Sparse                        \n",
      "label                                   dense                          data1                         \n",
      "(None, 1)                               (None, 13)                              \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Layer Type                              Input Name                    Output Name                   Output Shape                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "DistributedSlotSparseEmbeddingHash      data1                         sparse_embedding1             (None, 26, 16)                \n",
      "Reshape                                 sparse_embedding1             reshape1                      (None, 416)                   \n",
      "Concat                                  reshape1,dense                concat1                       (None, 429)                   \n",
      "Slice                                   concat1                       slice11,slice12                                             \n",
      "MultiCross                              slice11                       multicross1                   (None, 429)                   \n",
      "InnerProduct                            slice12                       fc1                           (None, 1024)                  \n",
      "ReLU                                    fc1                           relu1                         (None, 1024)                  \n",
      "Dropout                                 relu1                         dropout1                      (None, 1024)                  \n",
      "InnerProduct                            dropout1                      fc2                           (None, 1024)                  \n",
      "ReLU                                    fc2                           relu2                         (None, 1024)                  \n",
      "Dropout                                 relu2                         dropout2                      (None, 1024)                  \n",
      "Concat                                  dropout2,multicross1          concat2                       (None, 1453)                  \n",
      "InnerProduct                            concat2                       fc3                           (None, 1)                     \n",
      "BinaryCrossEntropyLoss                  fc3,label                     loss                                                        \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "=====================================================Model Fit=====================================================\n",
      "[12d40h51m30s][HUGECTR][INFO]: Use non-epoch mode with number of iterations: 500\n",
      "[12d40h51m30s][HUGECTR][INFO]: Training batchsize: 4096, evaluation batchsize: 4096\n",
      "[12d40h51m30s][HUGECTR][INFO]: Evaluation interval: 100, snapshot interval: 10000\n",
      "[12d40h51m30s][HUGECTR][INFO]: Sparse embedding trainable: 1, dense network trainable: 1\n",
      "[12d40h51m30s][HUGECTR][INFO]: Use mixed precision: 0, scaler: 1.000000, use cuda graph: 1\n",
      "[12d40h51m30s][HUGECTR][INFO]: lr: 0.001000, warmup_steps: 1, decay_start: 0, decay_steps: 1, decay_power: 2.000000, end_lr: 0.000000\n",
      "[12d40h51m30s][HUGECTR][INFO]: Training source file: ./dcn_test_norm/file_list.txt\n",
      "[12d40h51m30s][HUGECTR][INFO]: Evaluation source file: ./dcn_test_norm/file_list_test.txt\n",
      "[12d40h51m31s][HUGECTR][INFO]: Iter: 50 Time(50 iters): 1.000325s Loss: 0.115547 lr:0.001000\n",
      "[12d40h51m32s][HUGECTR][INFO]: Iter: 100 Time(50 iters): 0.943880s Loss: 0.101662 lr:0.001000\n",
      "[12d40h51m41s][HUGECTR][INFO]: Evaluation, AUC: 0.726328\n",
      "[12d40h51m41s][HUGECTR][INFO]: Eval Time for 1500 iters: 8.899718s\n",
      "[12d40h51m42s][HUGECTR][INFO]: Iter: 150 Time(50 iters): 9.861160s Loss: 0.110540 lr:0.001000\n",
      "[12d40h51m43s][HUGECTR][INFO]: Iter: 200 Time(50 iters): 0.950498s Loss: 0.113643 lr:0.001000\n",
      "[12d40h51m52s][HUGECTR][INFO]: Evaluation, AUC: 0.741423\n",
      "[12d40h51m52s][HUGECTR][INFO]: Eval Time for 1500 iters: 8.918206s\n",
      "[12d40h51m53s][HUGECTR][INFO]: Iter: 250 Time(50 iters): 9.881610s Loss: 0.112450 lr:0.001000\n",
      "[12d40h51m54s][HUGECTR][INFO]: Iter: 300 Time(50 iters): 0.955767s Loss: 0.093038 lr:0.001000\n",
      "[12d40h52m30s][HUGECTR][INFO]: Evaluation, AUC: 0.743224\n",
      "[12d40h52m30s][HUGECTR][INFO]: Eval Time for 1500 iters: 9.015887s\n",
      "[12d40h52m40s][HUGECTR][INFO]: Iter: 350 Time(50 iters): 9.988512s Loss: 0.120821 lr:0.001000\n",
      "[12d40h52m50s][HUGECTR][INFO]: Iter: 400 Time(50 iters): 0.965789s Loss: 0.109372 lr:0.001000\n",
      "[12d40h52m14s][HUGECTR][INFO]: Evaluation, AUC: 0.736142\n",
      "[12d40h52m14s][HUGECTR][INFO]: Eval Time for 1500 iters: 9.081340s\n",
      "[12d40h52m15s][HUGECTR][INFO]: Iter: 450 Time(50 iters): 10.062430s Loss: 0.123176 lr:0.001000\n"
     ]
    }
   ],
   "source": [
    "!python3 dcn_continue.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fourth-stewart",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id=\"24\"></a>\n",
    "### 2.4 Inference\n",
    "\n",
    "The HugeCTR inference is enabled by `hugectr.inference.InferenceSession.predict` method of InferenceSession, which requires dense features, embedding columns and row pointers of slots as the input and gives the prediction result as the output. We need to convert the criteo data to inference format first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "unsigned-galaxy",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 ../tools/criteo_predict/criteo2predict.py --src_csv_path=dcn_data/val/test.txt --src_config=../tools/criteo_predict/dcn_data.json --dst_path=./dcn_csr.txt --batch_size=1024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italic-domain",
   "metadata": {},
   "source": [
    "We can then make inference based on the saved model graph and model weights by doing the following with Python APIs:\n",
    "\n",
    "1. Configure the inference related parameters.\n",
    "2. Create the inference session.\n",
    "3. Make inference with the `InferenceSession.predict` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "blond-joseph",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dcn_inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile dcn_inference.py\n",
    "from hugectr.inference import InferenceParams, CreateInferenceSession\n",
    "from mpi4py import MPI\n",
    "\n",
    "def calculate_accuracy(labels, output):\n",
    "    num_samples = len(labels)\n",
    "    flags = [1 if ((labels[i] == 0 and output[i] <= 0.5) or (labels[i] == 1 and output[i] > 0.5)) else 0 for i in range(num_samples)]\n",
    "    correct_samples = sum(flags)\n",
    "    return float(correct_samples)/(float(num_samples)+1e-16)\n",
    "\n",
    "data_file = open(\"dcn_csr.txt\")\n",
    "config_file = \"dcn.json\"\n",
    "labels = [int(item) for item in data_file.readline().split(' ')]\n",
    "dense_features = [float(item) for item in data_file.readline().split(' ') if item!=\"\\n\"]\n",
    "embedding_columns = [int(item) for item in data_file.readline().split(' ')]\n",
    "row_ptrs = [int(item) for item in data_file.readline().split(' ')]\n",
    "\n",
    "# create parameter server, embedding cache and inference session\n",
    "inference_params = InferenceParams(model_name = \"dcn\",\n",
    "                                max_batchsize = 1024,\n",
    "                                hit_rate_threshold = 0.6,\n",
    "                                dense_model_file = \"./dcn_dense_1000.model\",\n",
    "                                sparse_model_files = [\"./dcn0_sparse_1000.model\"],\n",
    "                                device_id = 3,\n",
    "                                use_gpu_embedding_cache = True,\n",
    "                                cache_size_percentage = 0.9,\n",
    "                                i64_input_key = False,\n",
    "                                use_mixed_precision = False)\n",
    "inference_session = CreateInferenceSession(config_file, inference_params)\n",
    "output = inference_session.predict(dense_features, embedding_columns, row_ptrs)\n",
    "accuracy = calculate_accuracy(labels, output)\n",
    "print(\"[HUGECTR][INFO] number samples: {}, accuracy: {}\".format(len(labels), accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "authorized-shooting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12d04h52m44s][HUGECTR][INFO]: default_emb_vec_value is not specified using default: 0.000000\n",
      "[12d04h52m46s][HUGECTR][INFO]: Global seed is 1116959941\n",
      "[12d04h52m47s][HUGECTR][INFO]: Peer-to-peer access cannot be fully enabled.\n",
      "[12d04h52m47s][HUGECTR][INFO]: Use mixed precision: 0\n",
      "[12d04h52m47s][HUGECTR][INFO]: start create embedding for inference\n",
      "[12d04h52m47s][HUGECTR][INFO]: sparse_input name data1\n",
      "[12d04h52m47s][HUGECTR][INFO]: create embedding for inference success\n",
      "[12d04h52m47s][HUGECTR][INFO]: Inference stage skip BinaryCrossEntropyLoss layer, replaced by Sigmoid layer\n",
      "[HUGECTR][INFO] number samples: 1024, accuracy: 0.9716796875\n"
     ]
    }
   ],
   "source": [
    "!python3 dcn_inference.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "first-frederick",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "## 3. Wide&Deep Demo\n",
    "\n",
    "<a id=\"31\"></a>\n",
    "### 3.1 Download and Preprocess Data\n",
    "1. Download the Kaggle Criteo dataset using the following command:\n",
    "   ```shell\n",
    "   $ cd ${project_root}/tools\n",
    "   $ wget http://azuremlsampleexperiments.blob.core.windows.net/criteo/day_1.gz\n",
    "   ```\n",
    "   \n",
    "   In preprocessing, we will further reduce the amounts of data to speedup the preprocessing, fill missing values, remove the feature values whose occurrences are very rare, etc. Here we choose pandas preprocessing method to make the dataset ready for HugeCTR training.\n",
    "\n",
    "2. Preprocessing by Pandas using the following command:\n",
    "   ```shell\n",
    "   $ bash preprocess.sh 1 wdl_data pandas 1 1 100\n",
    "   ```\n",
    "   \n",
    "   The first argument represents the dataset postfix. It is 1 here since day_1 is used. The second argument wdl_data is where the preprocessed data is stored. The fourth arguement (one after pandas) 1 embodies that the normalization is applied to dense features. The fifth argument 1 means that the feature crossing is applied. The last argument 100 means the number of data files in each file list.\n",
    "   \n",
    "3. Create a soft link to the dataset folder using the following command:\n",
    "   ```shell\n",
    "   $ ln ${project_root}/tools/wdl_data ${project_root}/notebooks/wdl_data\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smart-bottom",
   "metadata": {},
   "source": [
    "<a id=\"32\"></a>\n",
    "### 3.2 Model Oversubscription\n",
    "\n",
    "We can train fom scratch using model oversubscriber, dump the model graph to a JSON file, and save the trained dense weights and sparse embedding weights by doing the following with Python APIs:\n",
    "\n",
    "1. Create the solver, reader, optimizer and MOS, then initialize the model.\n",
    "2. Construct the model graph by adding input, sparse embedding and dense layers in order.\n",
    "3. Compile the model and have an overview of the model graph.\n",
    "4. Dump the model graph to the JSON file.\n",
    "5. Fit the model.\n",
    "6. Save the model weights and optimizer states explicitly.\n",
    "\n",
    "To use model oversubscription, we should specify `repeat_dataset` as False. The data in each file list will be trained for specified number of epochs under this mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "permanent-google",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wdl_train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wdl_train.py\n",
    "import hugectr\n",
    "from mpi4py import MPI\n",
    "solver = hugectr.CreateSolver(max_eval_batches = 5000,\n",
    "                              batchsize_eval = 1024,\n",
    "                              batchsize = 1024,\n",
    "                              lr = 0.001,\n",
    "                              vvgpu = [[3]],\n",
    "                              i64_input_key = False,\n",
    "                              use_mixed_precision = False,\n",
    "                              repeat_dataset = False,\n",
    "                              use_cuda_graph = True)\n",
    "reader = hugectr.DataReaderParams(data_reader_type = hugectr.DataReaderType_t.Norm,\n",
    "                          source = [\"wdl_data/file_list.\"+str(i)+\".txt\" for i in range(2)],\n",
    "                          keyset = [\"wdl_data/file_list.\"+str(i)+\".keyset\" for i in range(2)],\n",
    "                          eval_source = \"wdl_data/file_list.2.txt\",\n",
    "                          check_type = hugectr.Check_t.Sum)\n",
    "optimizer = hugectr.CreateOptimizer(optimizer_type = hugectr.Optimizer_t.Adam)\n",
    "mos = hugectr.CreateMOS(train_from_scratch = True, use_host_memory_ps = True, dest_sparse_models = [\"./wdl_0_sparse_model\", \"./wdl_1_sparse_model\"])\n",
    "model = hugectr.Model(solver, reader, optimizer, mos)\n",
    "model.add(hugectr.Input(label_dim = 1, label_name = \"label\",\n",
    "                        dense_dim = 13, dense_name = \"dense\",\n",
    "                        data_reader_sparse_param_array = \n",
    "                        [hugectr.DataReaderSparseParam(\"wide_data\", 30, True, 1),\n",
    "                        hugectr.DataReaderSparseParam(\"deep_data\", 2, False, 26)]))\n",
    "model.add(hugectr.SparseEmbedding(embedding_type = hugectr.Embedding_t.DistributedSlotSparseEmbeddingHash, \n",
    "                            workspace_size_per_gpu_in_mb = 23,\n",
    "                            embedding_vec_size = 1,\n",
    "                            combiner = \"sum\",\n",
    "                            sparse_embedding_name = \"sparse_embedding2\",\n",
    "                            bottom_name = \"wide_data\",\n",
    "                            optimizer = optimizer))\n",
    "model.add(hugectr.SparseEmbedding(embedding_type = hugectr.Embedding_t.DistributedSlotSparseEmbeddingHash, \n",
    "                            workspace_size_per_gpu_in_mb = 358,\n",
    "                            embedding_vec_size = 16,\n",
    "                            combiner = \"sum\",\n",
    "                            sparse_embedding_name = \"sparse_embedding1\",\n",
    "                            bottom_name = \"deep_data\",\n",
    "                            optimizer = optimizer))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Reshape,\n",
    "                            bottom_names = [\"sparse_embedding1\"],\n",
    "                            top_names = [\"reshape1\"],\n",
    "                            leading_dim=416))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Reshape,\n",
    "                            bottom_names = [\"sparse_embedding2\"],\n",
    "                            top_names = [\"reshape2\"],\n",
    "                            leading_dim=1))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Concat,\n",
    "                            bottom_names = [\"reshape1\", \"dense\"], top_names = [\"concat1\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"concat1\"],\n",
    "                            top_names = [\"fc1\"],\n",
    "                            num_output=1024))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc1\"],\n",
    "                            top_names = [\"relu1\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Dropout,\n",
    "                            bottom_names = [\"relu1\"],\n",
    "                            top_names = [\"dropout1\"],\n",
    "                            dropout_rate=0.5))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"dropout1\"],\n",
    "                            top_names = [\"fc2\"],\n",
    "                            num_output=1024))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc2\"],\n",
    "                            top_names = [\"relu2\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Dropout,\n",
    "                            bottom_names = [\"relu2\"],\n",
    "                            top_names = [\"dropout2\"],\n",
    "                            dropout_rate=0.5))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"dropout2\"],\n",
    "                            top_names = [\"fc3\"],\n",
    "                            num_output=1))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Add,\n",
    "                            bottom_names = [\"fc3\", \"reshape2\"],\n",
    "                            top_names = [\"add1\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.BinaryCrossEntropyLoss,\n",
    "                            bottom_names = [\"add1\", \"label\"],\n",
    "                            top_names = [\"loss\"]))\n",
    "model.compile()\n",
    "model.summary()\n",
    "model.graph_to_json(graph_config_file = \"wdl.json\")\n",
    "model.fit(num_epochs = 1, display = 500, eval_interval = 1000)\n",
    "# Get the updated embedding features in model.fit()\n",
    "# updated_model = model.get_incremental_model()\n",
    "# User defined operations to the updated_model\n",
    "# ...\n",
    "\n",
    "model.set_source(source = [\"wdl_data/file_list.3.txt\", \"wdl_data/file_list.4.txt\"], keyset = [\"wdl_data/file_list.3.keyset\", \"wdl_data/file_list.4.keyset\"], eval_source = \"wdl_data/file_list.5.txt\")\n",
    "model.fit(num_epochs = 1, display = 500, eval_interval = 1000)\n",
    "# Get the updated embedding features in model.fit()\n",
    "# updated_model = model.get_incremental_model()\n",
    "# User defined operations to the updated_model\n",
    "# ...\n",
    "model.save_params_to_files(\"wdl_mos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "divided-damages",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================Model Init=====================================================\n",
      "[12d06h51m09s][HUGECTR][INFO]: Global seed is 3905962573\n",
      "[12d06h51m12s][HUGECTR][INFO]: Peer-to-peer access cannot be fully enabled.\n",
      "Device 3: Tesla T4\n",
      "[12d06h51m12s][HUGECTR][INFO]: num of DataReader workers: 12\n",
      "[12d06h51m12s][HUGECTR][INFO]: max_vocabulary_size_per_gpu_=6029312\n",
      "[12d06h51m12s][HUGECTR][INFO]: max_vocabulary_size_per_gpu_=5865472\n",
      "===================================================Model Compile===================================================\n",
      "[12d06h51m20s][HUGECTR][INFO]: gpu0 start to init embedding\n",
      "[12d06h51m20s][HUGECTR][INFO]: gpu0 init embedding done\n",
      "[12d06h51m20s][HUGECTR][INFO]: gpu0 start to init embedding\n",
      "[12d06h51m20s][HUGECTR][INFO]: gpu0 init embedding done\n",
      "[12d06h51m20s][HUGECTR][INFO]: Host MEM-based Parameter Server is enabled\n",
      "[12d06h51m20s][HUGECTR][INFO]: construct sparse models for model oversubscriber: ./wdl_0_sparse_model\n",
      "[12d06h51m20s][HUGECTR][INFO]: ./wdl_0_sparse_model not exist, create and train from scratch\n",
      "[12d06h51m21s][HUGECTR][INFO]: construct sparse models for model oversubscriber: ./wdl_1_sparse_model\n",
      "[12d06h51m21s][HUGECTR][INFO]: ./wdl_1_sparse_model not exist, create and train from scratch\n",
      "===================================================Model Summary===================================================\n",
      "Label                                   Dense                         Sparse                        \n",
      "label                                   dense                          wide_data,deep_data           \n",
      "(None, 1)                               (None, 13)                              \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Layer Type                              Input Name                    Output Name                   Output Shape                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "DistributedSlotSparseEmbeddingHash      wide_data                     sparse_embedding2             (None, 1, 1)                  \n",
      "DistributedSlotSparseEmbeddingHash      deep_data                     sparse_embedding1             (None, 26, 16)                \n",
      "Reshape                                 sparse_embedding1             reshape1                      (None, 416)                   \n",
      "Reshape                                 sparse_embedding2             reshape2                      (None, 1)                     \n",
      "Concat                                  reshape1,dense                concat1                       (None, 429)                   \n",
      "InnerProduct                            concat1                       fc1                           (None, 1024)                  \n",
      "ReLU                                    fc1                           relu1                         (None, 1024)                  \n",
      "Dropout                                 relu1                         dropout1                      (None, 1024)                  \n",
      "InnerProduct                            dropout1                      fc2                           (None, 1024)                  \n",
      "ReLU                                    fc2                           relu2                         (None, 1024)                  \n",
      "Dropout                                 relu2                         dropout2                      (None, 1024)                  \n",
      "InnerProduct                            dropout2                      fc3                           (None, 1)                     \n",
      "Add                                     fc3,reshape2                  add1                          (None, 1)                     \n",
      "BinaryCrossEntropyLoss                  add1,label                    loss                                                        \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "[12d60h51m21s][HUGECTR][INFO]: Save the model graph to wdl.json, successful\n",
      "=====================================================Model Fit=====================================================\n",
      "[12d60h51m21s][HUGECTR][INFO]: Use model oversubscriber mode with number of training sources: 2, number of epochs: 1\n",
      "[12d60h51m21s][HUGECTR][INFO]: Training batchsize: 1024, evaluation batchsize: 1024\n",
      "[12d60h51m21s][HUGECTR][INFO]: Evaluation interval: 1000, snapshot interval: 10000\n",
      "[12d60h51m21s][HUGECTR][INFO]: Sparse embedding trainable: 1, dense network trainable: 1\n",
      "[12d60h51m21s][HUGECTR][INFO]: Use mixed precision: 0, scaler: 1.000000, use cuda graph: 1\n",
      "[12d60h51m21s][HUGECTR][INFO]: lr: 0.001000, warmup_steps: 1, decay_start: 0, decay_steps: 1, decay_power: 2.000000, end_lr: 0.000000\n",
      "[12d60h51m21s][HUGECTR][INFO]: Evaluation source file: wdl_data/file_list.2.txt\n",
      "[12d60h51m21s][HUGECTR][INFO]: --------------------Epoch 0, source file: wdl_data/file_list.0.txt--------------------\n",
      "[12d60h51m21s][HUGECTR][INFO]: Preparing embedding table for next pass [DONE]\n",
      "[12d60h51m30s][HUGECTR][INFO]: Iter: 500 Time(500 iters): 8.699051s Loss: 0.176726 lr:0.001000\n",
      "[12d60h51m38s][HUGECTR][INFO]: Iter: 1000 Time(500 iters): 8.482289s Loss: 0.147600 lr:0.001000\n",
      "[12d60h51m48s][HUGECTR][INFO]: Evaluation, AUC: 0.735672\n",
      "[12d60h51m48s][HUGECTR][INFO]: Eval Time for 5000 iters: 10.098063s\n",
      "[12d60h51m58s][HUGECTR][INFO]: Iter: 1500 Time(500 iters): 19.983562s Loss: 0.141354 lr:0.001000\n",
      "[12d60h52m70s][HUGECTR][INFO]: Iter: 2000 Time(500 iters): 9.089311s Loss: 0.119809 lr:0.001000\n",
      "[12d60h52m17s][HUGECTR][INFO]: Evaluation, AUC: 0.746285\n",
      "[12d60h52m17s][HUGECTR][INFO]: Eval Time for 5000 iters: 9.494486s\n",
      "[12d60h52m25s][HUGECTR][INFO]: Iter: 2500 Time(500 iters): 17.808452s Loss: 0.133760 lr:0.001000\n",
      "[12d60h52m34s][HUGECTR][INFO]: Iter: 3000 Time(500 iters): 9.427818s Loss: 0.141670 lr:0.001000\n",
      "[12d60h52m47s][HUGECTR][INFO]: Evaluation, AUC: 0.750942\n",
      "[12d60h52m47s][HUGECTR][INFO]: Eval Time for 5000 iters: 12.379570s\n",
      "[12d60h52m55s][HUGECTR][INFO]: Iter: 3500 Time(500 iters): 20.768373s Loss: 0.150279 lr:0.001000\n",
      "[12d60h53m30s][HUGECTR][INFO]: Iter: 4000 Time(500 iters): 8.327276s Loss: 0.120760 lr:0.001000\n",
      "[12d60h53m16s][HUGECTR][INFO]: Evaluation, AUC: 0.753285\n",
      "[12d60h53m16s][HUGECTR][INFO]: Eval Time for 5000 iters: 12.160583s\n",
      "[12d60h53m16s][HUGECTR][INFO]: --------------------Epoch 0, source file: wdl_data/file_list.1.txt--------------------\n",
      "[12d60h53m16s][HUGECTR][INFO]: Preparing embedding table for next pass [DONE]\n",
      "[12d60h53m28s][HUGECTR][INFO]: Iter: 4500 Time(500 iters): 24.970690s Loss: 0.121966 lr:0.001000\n",
      "[12d60h53m37s][HUGECTR][INFO]: Iter: 5000 Time(500 iters): 8.476193s Loss: 0.124643 lr:0.001000\n",
      "[12d60h53m47s][HUGECTR][INFO]: Evaluation, AUC: 0.749743\n",
      "[12d60h53m47s][HUGECTR][INFO]: Eval Time for 5000 iters: 10.200681s\n",
      "[12d60h53m55s][HUGECTR][INFO]: Iter: 5500 Time(500 iters): 18.543556s Loss: 0.113493 lr:0.001000\n",
      "[12d60h54m40s][HUGECTR][INFO]: Iter: 6000 Time(500 iters): 8.327074s Loss: 0.106302 lr:0.001000\n",
      "[12d60h54m14s][HUGECTR][INFO]: Evaluation, AUC: 0.753467\n",
      "[12d60h54m14s][HUGECTR][INFO]: Eval Time for 5000 iters: 10.208748s\n",
      "[12d60h54m22s][HUGECTR][INFO]: Iter: 6500 Time(500 iters): 18.529270s Loss: 0.146966 lr:0.001000\n",
      "[12d60h54m31s][HUGECTR][INFO]: Iter: 7000 Time(500 iters): 8.315973s Loss: 0.120101 lr:0.001000\n",
      "[12d60h54m41s][HUGECTR][INFO]: Evaluation, AUC: 0.755767\n",
      "[12d60h54m41s][HUGECTR][INFO]: Eval Time for 5000 iters: 10.419125s\n",
      "[12d60h54m49s][HUGECTR][INFO]: Iter: 7500 Time(500 iters): 18.728502s Loss: 0.128571 lr:0.001000\n",
      "[12d60h54m58s][HUGECTR][INFO]: Iter: 8000 Time(500 iters): 8.317057s Loss: 0.113243 lr:0.001000\n",
      "[12d60h55m80s][HUGECTR][INFO]: Evaluation, AUC: 0.756816\n",
      "[12d60h55m80s][HUGECTR][INFO]: Eval Time for 5000 iters: 10.772335s\n",
      "=====================================================Model Fit=====================================================\n",
      "[12d60h55m80s][HUGECTR][INFO]: Use model oversubscriber mode with number of training sources: 2, number of epochs: 1\n",
      "[12d60h55m80s][HUGECTR][INFO]: Training batchsize: 1024, evaluation batchsize: 1024\n",
      "[12d60h55m80s][HUGECTR][INFO]: Evaluation interval: 1000, snapshot interval: 10000\n",
      "[12d60h55m80s][HUGECTR][INFO]: Sparse embedding trainable: 1, dense network trainable: 1\n",
      "[12d60h55m80s][HUGECTR][INFO]: Use mixed precision: 0, scaler: 1.000000, use cuda graph: 1\n",
      "[12d60h55m80s][HUGECTR][INFO]: lr: 0.001000, warmup_steps: 1, decay_start: 0, decay_steps: 1, decay_power: 2.000000, end_lr: 0.000000\n",
      "[12d60h55m80s][HUGECTR][INFO]: Evaluation source file: wdl_data/file_list.5.txt\n",
      "[12d60h55m80s][HUGECTR][INFO]: --------------------Epoch 0, source file: wdl_data/file_list.3.txt--------------------\n",
      "[12d60h55m80s][HUGECTR][INFO]: Preparing embedding table for next pass [DONE]\n",
      "[12d60h55m18s][HUGECTR][INFO]: Iter: 500 Time(500 iters): 9.475969s Loss: 0.117018 lr:0.001000\n",
      "[12d60h55m26s][HUGECTR][INFO]: Iter: 1000 Time(500 iters): 8.324699s Loss: 0.136417 lr:0.001000\n",
      "[12d60h55m36s][HUGECTR][INFO]: Evaluation, AUC: 0.755714\n",
      "[12d60h55m36s][HUGECTR][INFO]: Eval Time for 5000 iters: 10.180895s\n",
      "[12d60h55m45s][HUGECTR][INFO]: Iter: 1500 Time(500 iters): 18.506583s Loss: 0.150341 lr:0.001000\n",
      "[12d60h55m53s][HUGECTR][INFO]: Iter: 2000 Time(500 iters): 8.334424s Loss: 0.123627 lr:0.001000\n",
      "[12d60h56m60s][HUGECTR][INFO]: Evaluation, AUC: 0.756438\n",
      "[12d60h56m60s][HUGECTR][INFO]: Eval Time for 5000 iters: 12.631103s\n",
      "[12d60h56m15s][HUGECTR][INFO]: Iter: 2500 Time(500 iters): 21.730326s Loss: 0.115956 lr:0.001000\n",
      "[12d60h56m24s][HUGECTR][INFO]: Iter: 3000 Time(500 iters): 9.273157s Loss: 0.143785 lr:0.001000\n",
      "[12d60h56m42s][HUGECTR][INFO]: Evaluation, AUC: 0.757469\n",
      "[12d60h56m42s][HUGECTR][INFO]: Eval Time for 5000 iters: 17.480155s\n",
      "[12d60h56m50s][HUGECTR][INFO]: Iter: 3500 Time(500 iters): 26.175531s Loss: 0.118859 lr:0.001000\n",
      "[12d60h56m59s][HUGECTR][INFO]: Iter: 4000 Time(500 iters): 8.555412s Loss: 0.146483 lr:0.001000\n",
      "[12d60h57m22s][HUGECTR][INFO]: Evaluation, AUC: 0.757729\n",
      "[12d60h57m22s][HUGECTR][INFO]: Eval Time for 5000 iters: 22.804618s\n",
      "[12d60h57m22s][HUGECTR][INFO]: --------------------Epoch 0, source file: wdl_data/file_list.4.txt--------------------\n",
      "[12d60h57m22s][HUGECTR][INFO]: Preparing embedding table for next pass [DONE]\n",
      "[12d60h57m39s][HUGECTR][INFO]: Iter: 4500 Time(500 iters): 40.631249s Loss: 0.118006 lr:0.001000\n",
      "[12d60h57m49s][HUGECTR][INFO]: Iter: 5000 Time(500 iters): 9.674694s Loss: 0.131756 lr:0.001000\n",
      "[12d60h58m11s][HUGECTR][INFO]: Evaluation, AUC: 0.756948\n",
      "[12d60h58m11s][HUGECTR][INFO]: Eval Time for 5000 iters: 22.203620s\n",
      "[12d60h58m21s][HUGECTR][INFO]: Iter: 5500 Time(500 iters): 31.737179s Loss: 0.099061 lr:0.001000\n",
      "[12d60h58m30s][HUGECTR][INFO]: Iter: 6000 Time(500 iters): 9.487701s Loss: 0.137508 lr:0.001000\n",
      "[12d60h58m56s][HUGECTR][INFO]: Evaluation, AUC: 0.756661\n",
      "[12d60h58m56s][HUGECTR][INFO]: Eval Time for 5000 iters: 25.365644s\n",
      "[12d60h59m50s][HUGECTR][INFO]: Iter: 6500 Time(500 iters): 34.776217s Loss: 0.148437 lr:0.001000\n",
      "[12d60h59m15s][HUGECTR][INFO]: Iter: 7000 Time(500 iters): 9.756645s Loss: 0.116976 lr:0.001000\n",
      "[12d60h59m40s][HUGECTR][INFO]: Evaluation, AUC: 0.758544\n",
      "[12d60h59m40s][HUGECTR][INFO]: Eval Time for 5000 iters: 24.751136s\n",
      "[12d60h59m49s][HUGECTR][INFO]: Iter: 7500 Time(500 iters): 34.162856s Loss: 0.147259 lr:0.001000\n",
      "[12d60h59m58s][HUGECTR][INFO]: Iter: 8000 Time(500 iters): 9.006998s Loss: 0.117763 lr:0.001000\n",
      "[12d70h00m23s][HUGECTR][INFO]: Evaluation, AUC: 0.758706\n",
      "[12d70h00m23s][HUGECTR][INFO]: Eval Time for 5000 iters: 24.513732s\n",
      "[12d70h00m32s][HUGECTR][INFO]: Updating sparse model in SSD [DONE]\n",
      "[12d70h00m36s][HUGECTR][INFO]: Updating sparse model in SSD [DONE]\n",
      "[12d70h00m37s][HUGECTR][INFO]: Dumping dense weights to file, successful\n",
      "[12d70h00m37s][HUGECTR][INFO]: Dumping dense optimizer states to file, successful\n",
      "[12d70h00m37s][HUGECTR][INFO]: Dumping untrainable weights to file, successful\n"
     ]
    }
   ],
   "source": [
    "!python3 wdl_train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pacific-tobacco",
   "metadata": {},
   "source": [
    "<a id=\"33\"></a>\n",
    "### 3.3 Fine-tune\n",
    "\n",
    "We can only load the sparse embedding layers their corresponding weights, and then construct a new dense network. The dense weights will be trained first and the sparse weights will be fine-tuned later. We can achieve this by doing the following with Python APIs:\n",
    "\n",
    "1. Create the solver, reader and optimizer, then initialize the model.\n",
    "2. Load the sparse embedding layers from the saved JSON file.\n",
    "3. Add the dense layers on top of the loaded model graph.\n",
    "4. Compile the model and have an overview of the model graph.\n",
    "5. Load the sparse weights and freeze the sparse embedding layers.\n",
    "6. Train the dense weights.\n",
    "7. Unfreeze the sparse embedding layers and freeze the dense layers, reset the learning rate scheduler with a small rate.\n",
    "8. Fine-tune the sparse weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "constant-attitude",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wdl_fine_tune.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wdl_fine_tune.py\n",
    "import hugectr\n",
    "from mpi4py import MPI\n",
    "solver = hugectr.CreateSolver(max_eval_batches = 5000,\n",
    "                              batchsize_eval = 1024,\n",
    "                              batchsize = 1024,\n",
    "                              vvgpu = [[3]],\n",
    "                              i64_input_key = False,\n",
    "                              use_mixed_precision = False,\n",
    "                              repeat_dataset = False,\n",
    "                              use_cuda_graph = True)\n",
    "reader = hugectr.DataReaderParams(data_reader_type = hugectr.DataReaderType_t.Norm,\n",
    "                          source = [\"wdl_data/file_list.6.txt\"],\n",
    "                          eval_source = \"wdl_data/file_list.7.txt\",\n",
    "                          check_type = hugectr.Check_t.Sum)\n",
    "optimizer = hugectr.CreateOptimizer(optimizer_type = hugectr.Optimizer_t.Adam)\n",
    "model = hugectr.Model(solver, reader, optimizer)\n",
    "model.construct_from_json(graph_config_file = \"wdl.json\", include_dense_network = False)\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Reshape,\n",
    "                            bottom_names = [\"sparse_embedding1\"],\n",
    "                            top_names = [\"reshape1\"],\n",
    "                            leading_dim=416))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Reshape,\n",
    "                            bottom_names = [\"sparse_embedding2\"],\n",
    "                            top_names = [\"reshape2\"],\n",
    "                            leading_dim=1))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Concat,\n",
    "                            bottom_names = [\"reshape1\", \"reshape2\", \"dense\"], top_names = [\"concat1\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"concat1\"],\n",
    "                            top_names = [\"fc1\"],\n",
    "                            num_output=1024))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc1\"],\n",
    "                            top_names = [\"relu1\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Dropout,\n",
    "                            bottom_names = [\"relu1\"],\n",
    "                            top_names = [\"dropout1\"],\n",
    "                            dropout_rate=0.5))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"dropout1\"],\n",
    "                            top_names = [\"fc2\"],\n",
    "                            num_output=1))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.BinaryCrossEntropyLoss,\n",
    "                            bottom_names = [\"fc2\", \"label\"],\n",
    "                            top_names = [\"loss\"]))\n",
    "model.compile()\n",
    "model.summary()\n",
    "model.load_sparse_weights([\"wdl_0_sparse_model\", \"wdl_1_sparse_model\"])\n",
    "model.freeze_embedding()\n",
    "model.fit(num_epochs = 1, display = 500, eval_interval = 1000, snapshot = 100000, snapshot_prefix = \"wdl\")\n",
    "model.unfreeze_embedding()\n",
    "model.freeze_dense()\n",
    "model.reset_learning_rate_scheduler(base_lr = 0.0001)\n",
    "model.fit(num_epochs = 2, display = 500, eval_interval = 1000, snapshot = 100000, snapshot_prefix = \"wdl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "reduced-government",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================Model Init=====================================================\n",
      "[12d07h00m41s][HUGECTR][INFO]: Global seed is 1903674398\n",
      "[12d07h00m45s][HUGECTR][INFO]: Peer-to-peer access cannot be fully enabled.\n",
      "Device 3: Tesla T4\n",
      "[12d07h00m45s][HUGECTR][INFO]: num of DataReader workers: 12\n",
      "[12d07h00m45s][HUGECTR][INFO]: max_vocabulary_size_per_gpu_=6029312\n",
      "[12d07h00m45s][HUGECTR][INFO]: max_vocabulary_size_per_gpu_=5865472\n",
      "[12d07h00m45s][HUGECTR][INFO]: Load the model graph from wdl.json, successful\n",
      "===================================================Model Compile===================================================\n",
      "[12d07h00m49s][HUGECTR][INFO]: gpu0 start to init embedding\n",
      "[12d07h00m49s][HUGECTR][INFO]: gpu0 init embedding done\n",
      "[12d07h00m49s][HUGECTR][INFO]: gpu0 start to init embedding\n",
      "[12d07h00m49s][HUGECTR][INFO]: gpu0 init embedding done\n",
      "===================================================Model Summary===================================================\n",
      "Label                                   Dense                         Sparse                        \n",
      "label                                   dense                          wide_data,deep_data           \n",
      "(None, 1)                               (None, 13)                              \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Layer Type                              Input Name                    Output Name                   Output Shape                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "DistributedSlotSparseEmbeddingHash      wide_data                     sparse_embedding2             (None, 1, 1)                  \n",
      "DistributedSlotSparseEmbeddingHash      deep_data                     sparse_embedding1             (None, 26, 16)                \n",
      "Reshape                                 sparse_embedding1             reshape1                      (None, 416)                   \n",
      "Reshape                                 sparse_embedding2             reshape2                      (None, 1)                     \n",
      "Concat                                  reshape1,reshape2,dense       concat1                       (None, 430)                   \n",
      "InnerProduct                            concat1                       fc1                           (None, 1024)                  \n",
      "ReLU                                    fc1                           relu1                         (None, 1024)                  \n",
      "Dropout                                 relu1                         dropout1                      (None, 1024)                  \n",
      "InnerProduct                            dropout1                      fc2                           (None, 1)                     \n",
      "BinaryCrossEntropyLoss                  fc2,label                     loss                                                        \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "[12d70h00m49s][HUGECTR][INFO]: Loading sparse model: wdl_0_sparse_model\n",
      "[12d70h00m49s][HUGECTR][INFO]: Loading sparse model: wdl_1_sparse_model\n",
      "=====================================================Model Fit=====================================================\n",
      "[12d70h00m49s][HUGECTR][INFO]: Use epoch mode with number of epochs: 1\n",
      "[12d70h00m49s][HUGECTR][INFO]: Training batchsize: 1024, evaluation batchsize: 1024\n",
      "[12d70h00m49s][HUGECTR][INFO]: Evaluation interval: 1000, snapshot interval: 100000\n",
      "[12d70h00m49s][HUGECTR][INFO]: Sparse embedding trainable: 0, dense network trainable: 1\n",
      "[12d70h00m49s][HUGECTR][INFO]: Use mixed precision: 0, scaler: 1.000000, use cuda graph: 1\n",
      "[12d70h00m49s][HUGECTR][INFO]: lr: 0.001000, warmup_steps: 1, decay_start: 0, decay_steps: 1, decay_power: 2.000000, end_lr: 0.000000\n",
      "[12d70h00m49s][HUGECTR][INFO]: Training source file: wdl_data/file_list.6.txt\n",
      "[12d70h00m49s][HUGECTR][INFO]: Evaluation source file: wdl_data/file_list.7.txt\n",
      "[12d70h00m49s][HUGECTR][INFO]: -----------------------------------Epoch 0-----------------------------------\n",
      "[12d70h00m58s][HUGECTR][INFO]: Iter: 500 Time(500 iters): 9.141537s Loss: 0.146767 lr:0.001000\n",
      "[12d70h10m70s][HUGECTR][INFO]: Iter: 1000 Time(500 iters): 8.989889s Loss: 0.133172 lr:0.001000\n",
      "[12d70h10m30s][HUGECTR][INFO]: Evaluation, AUC: 0.752226\n",
      "[12d70h10m30s][HUGECTR][INFO]: Eval Time for 5000 iters: 22.902317s\n",
      "[12d70h10m39s][HUGECTR][INFO]: Iter: 1500 Time(500 iters): 31.144250s Loss: 0.125819 lr:0.001000\n",
      "[12d70h10m47s][HUGECTR][INFO]: Iter: 2000 Time(500 iters): 8.332035s Loss: 0.153951 lr:0.001000\n",
      "[12d70h20m11s][HUGECTR][INFO]: Evaluation, AUC: 0.756261\n",
      "[12d70h20m11s][HUGECTR][INFO]: Eval Time for 5000 iters: 24.331560s\n",
      "[12d70h20m20s][HUGECTR][INFO]: Iter: 2500 Time(500 iters): 32.965709s Loss: 0.134148 lr:0.001000\n",
      "[12d70h20m28s][HUGECTR][INFO]: Iter: 3000 Time(500 iters): 8.191812s Loss: 0.134186 lr:0.001000\n",
      "[12d70h20m51s][HUGECTR][INFO]: Evaluation, AUC: 0.757734\n",
      "[12d70h20m51s][HUGECTR][INFO]: Eval Time for 5000 iters: 22.641689s\n",
      "[12d70h30m00s][HUGECTR][INFO]: Iter: 3500 Time(500 iters): 31.755957s Loss: 0.124338 lr:0.001000\n",
      "[12d70h30m80s][HUGECTR][INFO]: Iter: 4000 Time(500 iters): 8.380422s Loss: 0.128903 lr:0.001000\n",
      "[12d70h30m27s][HUGECTR][INFO]: Evaluation, AUC: 0.757878\n",
      "[12d70h30m27s][HUGECTR][INFO]: Eval Time for 5000 iters: 18.676145s\n",
      "=====================================================Model Fit=====================================================\n",
      "[12d70h30m27s][HUGECTR][INFO]: Use epoch mode with number of epochs: 2\n",
      "[12d70h30m27s][HUGECTR][INFO]: Training batchsize: 1024, evaluation batchsize: 1024\n",
      "[12d70h30m27s][HUGECTR][INFO]: Evaluation interval: 1000, snapshot interval: 100000\n",
      "[12d70h30m27s][HUGECTR][INFO]: Sparse embedding trainable: 1, dense network trainable: 0\n",
      "[12d70h30m27s][HUGECTR][INFO]: Use mixed precision: 0, scaler: 1.000000, use cuda graph: 1\n",
      "[12d70h30m27s][HUGECTR][INFO]: lr: 0.001000, warmup_steps: 1, decay_start: 0, decay_steps: 1, decay_power: 2.000000, end_lr: 0.000000\n",
      "[12d70h30m27s][HUGECTR][INFO]: Training source file: wdl_data/file_list.6.txt\n",
      "[12d70h30m27s][HUGECTR][INFO]: Evaluation source file: wdl_data/file_list.7.txt\n",
      "[12d70h30m27s][HUGECTR][INFO]: -----------------------------------Epoch 0-----------------------------------\n",
      "[12d70h30m36s][HUGECTR][INFO]: Iter: 500 Time(500 iters): 8.706647s Loss: 0.146495 lr:0.000100\n",
      "[12d70h30m44s][HUGECTR][INFO]: Iter: 1000 Time(500 iters): 8.830861s Loss: 0.129436 lr:0.000100\n",
      "[12d70h40m90s][HUGECTR][INFO]: Evaluation, AUC: 0.759677\n",
      "[12d70h40m90s][HUGECTR][INFO]: Eval Time for 5000 iters: 24.757133s\n",
      "[12d70h40m18s][HUGECTR][INFO]: Iter: 1500 Time(500 iters): 33.616535s Loss: 0.119807 lr:0.000100\n",
      "[12d70h40m27s][HUGECTR][INFO]: Iter: 2000 Time(500 iters): 9.121400s Loss: 0.153373 lr:0.000100\n",
      "[12d70h40m54s][HUGECTR][INFO]: Evaluation, AUC: 0.760351\n",
      "[12d70h40m54s][HUGECTR][INFO]: Eval Time for 5000 iters: 26.577320s\n",
      "[12d70h50m20s][HUGECTR][INFO]: Iter: 2500 Time(500 iters): 35.106229s Loss: 0.133495 lr:0.000100\n",
      "[12d70h50m11s][HUGECTR][INFO]: Iter: 3000 Time(500 iters): 8.613858s Loss: 0.131499 lr:0.000100\n",
      "[12d70h50m38s][HUGECTR][INFO]: Evaluation, AUC: 0.760879\n",
      "[12d70h50m38s][HUGECTR][INFO]: Eval Time for 5000 iters: 27.021191s\n",
      "[12d70h50m47s][HUGECTR][INFO]: Iter: 3500 Time(500 iters): 36.127296s Loss: 0.124276 lr:0.000100\n",
      "[12d70h50m56s][HUGECTR][INFO]: Iter: 4000 Time(500 iters): 8.893062s Loss: 0.127344 lr:0.000100\n",
      "[12d70h60m22s][HUGECTR][INFO]: Evaluation, AUC: 0.761186\n",
      "[12d70h60m22s][HUGECTR][INFO]: Eval Time for 5000 iters: 25.792418s\n",
      "[12d70h60m22s][HUGECTR][INFO]: -----------------------------------Epoch 1-----------------------------------\n",
      "[12d70h60m31s][HUGECTR][INFO]: Iter: 4500 Time(500 iters): 35.171177s Loss: 0.110136 lr:0.000100\n",
      "[12d70h60m40s][HUGECTR][INFO]: Iter: 5000 Time(500 iters): 9.050530s Loss: 0.124141 lr:0.000100\n",
      "[12d70h70m30s][HUGECTR][INFO]: Evaluation, AUC: 0.761529\n",
      "[12d70h70m30s][HUGECTR][INFO]: Eval Time for 5000 iters: 23.048880s\n",
      "[12d70h70m12s][HUGECTR][INFO]: Iter: 5500 Time(500 iters): 31.488029s Loss: 0.138744 lr:0.000100\n",
      "[12d70h70m20s][HUGECTR][INFO]: Iter: 6000 Time(500 iters): 8.954756s Loss: 0.123205 lr:0.000100\n",
      "[12d70h70m46s][HUGECTR][INFO]: Evaluation, AUC: 0.761771\n",
      "[12d70h70m46s][HUGECTR][INFO]: Eval Time for 5000 iters: 25.001289s\n",
      "[12d70h70m54s][HUGECTR][INFO]: Iter: 6500 Time(500 iters): 33.477563s Loss: 0.130592 lr:0.000100\n",
      "[12d70h80m20s][HUGECTR][INFO]: Iter: 7000 Time(500 iters): 8.389490s Loss: 0.108180 lr:0.000100\n",
      "[12d70h80m23s][HUGECTR][INFO]: Evaluation, AUC: 0.762032\n",
      "[12d70h80m23s][HUGECTR][INFO]: Eval Time for 5000 iters: 20.883214s\n",
      "[12d70h80m32s][HUGECTR][INFO]: Iter: 7500 Time(500 iters): 29.529080s Loss: 0.137966 lr:0.000100\n",
      "[12d70h80m41s][HUGECTR][INFO]: Iter: 8000 Time(500 iters): 8.714219s Loss: 0.127571 lr:0.000100\n",
      "[12d70h90m30s][HUGECTR][INFO]: Evaluation, AUC: 0.762133\n",
      "[12d70h90m30s][HUGECTR][INFO]: Eval Time for 5000 iters: 22.458788s\n"
     ]
    }
   ],
   "source": [
    "!python3 wdl_fine_tune.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technical-builder",
   "metadata": {},
   "source": [
    "<a id=\"34\"></a>\n",
    "### 3.4 Low-level Training\n",
    "\n",
    "The low-level training APIs are maintained in the enhanced HugeCTR Python interface. If you want to have precise control of each training iteration and each evaluation step, you may find it helpful to use these APIs. Since the data reader behavior is different in epoch mode and non-epoch mode, we should pay attention to how to tweak the data reader when using low-level training. We will denmonstrate how to write the low-level training scripts for non-epoch mode, epoch mode and model oversubscription mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "pleasant-cigarette",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wdl_non_epoch.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wdl_non_epoch.py\n",
    "import hugectr\n",
    "from mpi4py import MPI\n",
    "solver = hugectr.CreateSolver(max_eval_batches = 5000,\n",
    "                              batchsize_eval = 1024,\n",
    "                              batchsize = 1024,\n",
    "                              vvgpu = [[3]],\n",
    "                              i64_input_key = False,\n",
    "                              use_mixed_precision = False,\n",
    "                              repeat_dataset = True,\n",
    "                              use_cuda_graph = True)\n",
    "reader = hugectr.DataReaderParams(data_reader_type = hugectr.DataReaderType_t.Norm,\n",
    "                          source = [\"wdl_data/file_list.0.txt\"],\n",
    "                          eval_source = \"wdl_data/file_list.1.txt\",\n",
    "                          check_type = hugectr.Check_t.Sum)\n",
    "optimizer = hugectr.CreateOptimizer(optimizer_type = hugectr.Optimizer_t.Adam)\n",
    "model = hugectr.Model(solver, reader, optimizer)\n",
    "model.construct_from_json(graph_config_file = \"wdl.json\", include_dense_network = True)\n",
    "model.compile()\n",
    "model.start_data_reading()\n",
    "lr_sch = model.get_learning_rate_scheduler()\n",
    "max_iter = 2000\n",
    "for i in range(max_iter):\n",
    "    lr = lr_sch.get_next()\n",
    "    model.set_learning_rate(lr)\n",
    "    model.train()\n",
    "    if (i%100 == 0):\n",
    "        loss = model.get_current_loss()\n",
    "        print(\"[HUGECTR][INFO] iter: {}; loss: {}\".format(i, loss))\n",
    "    if (i%1000 == 0 and i != 0):\n",
    "        for _ in range(solver.max_eval_batches):\n",
    "            model.eval()\n",
    "        metrics = model.get_eval_metrics()\n",
    "        print(\"[HUGECTR][INFO] iter: {}, {}\".format(i, metrics))\n",
    "model.save_params_to_files(\"./\", max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "compliant-dakota",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================Model Init=====================================================\n",
      "[12d07h09m15s][HUGECTR][INFO]: Global seed is 1575926632\n",
      "[12d07h09m18s][HUGECTR][INFO]: Peer-to-peer access cannot be fully enabled.\n",
      "Device 3: Tesla T4\n",
      "[12d07h09m18s][HUGECTR][INFO]: num of DataReader workers: 12\n",
      "[12d07h09m18s][HUGECTR][INFO]: max_vocabulary_size_per_gpu_=6029312\n",
      "[12d07h09m18s][HUGECTR][INFO]: max_vocabulary_size_per_gpu_=5865472\n",
      "[12d07h09m18s][HUGECTR][INFO]: Load the model graph from wdl.json, successful\n",
      "===================================================Model Compile===================================================\n",
      "[12d07h09m27s][HUGECTR][INFO]: gpu0 start to init embedding\n",
      "[12d07h09m27s][HUGECTR][INFO]: gpu0 init embedding done\n",
      "[12d07h09m27s][HUGECTR][INFO]: gpu0 start to init embedding\n",
      "[12d07h09m27s][HUGECTR][INFO]: gpu0 init embedding done\n",
      "[HUGECTR][INFO] iter: 0; loss: 0.31456565856933594\n",
      "[HUGECTR][INFO] iter: 100; loss: 0.15977604687213898\n",
      "[HUGECTR][INFO] iter: 200; loss: 0.13649414479732513\n",
      "[HUGECTR][INFO] iter: 300; loss: 0.11457975953817368\n",
      "[HUGECTR][INFO] iter: 400; loss: 0.14135544002056122\n",
      "[HUGECTR][INFO] iter: 500; loss: 0.1747560203075409\n",
      "[HUGECTR][INFO] iter: 600; loss: 0.1470160335302353\n",
      "[HUGECTR][INFO] iter: 700; loss: 0.09667129814624786\n",
      "[HUGECTR][INFO] iter: 800; loss: 0.12410719692707062\n",
      "[HUGECTR][INFO] iter: 900; loss: 0.1237555667757988\n",
      "[HUGECTR][INFO] iter: 1000; loss: 0.14860115945339203\n",
      "[HUGECTR][INFO] iter: 1000, [('AUC', 0.7370145320892334)]\n",
      "[HUGECTR][INFO] iter: 1100; loss: 0.11791292577981949\n",
      "[HUGECTR][INFO] iter: 1200; loss: 0.14438574016094208\n",
      "[HUGECTR][INFO] iter: 1300; loss: 0.1295153647661209\n",
      "[HUGECTR][INFO] iter: 1400; loss: 0.1419382244348526\n",
      "[HUGECTR][INFO] iter: 1500; loss: 0.13936810195446014\n",
      "[HUGECTR][INFO] iter: 1600; loss: 0.10585300624370575\n",
      "[HUGECTR][INFO] iter: 1700; loss: 0.12812763452529907\n",
      "[HUGECTR][INFO] iter: 1800; loss: 0.13048861920833588\n",
      "[HUGECTR][INFO] iter: 1900; loss: 0.1299288272857666\n",
      "[12d07h10m31s][HUGECTR][INFO]: Rank0: Write hash table to file\n",
      "[12d07h10m31s][HUGECTR][INFO]: Rank0: Write hash table to file\n",
      "[12d07h10m31s][HUGECTR][INFO]: Dumping sparse weights to files, successful\n",
      "[12d07h10m31s][HUGECTR][INFO]: Rank0: Write optimzer state to file\n",
      "[12d07h10m31s][HUGECTR][INFO]: Done\n",
      "[12d07h10m31s][HUGECTR][INFO]: Rank0: Write optimzer state to file\n",
      "[12d07h10m31s][HUGECTR][INFO]: Done\n",
      "[12d07h10m32s][HUGECTR][INFO]: Rank0: Write optimzer state to file\n",
      "[12d07h10m32s][HUGECTR][INFO]: Done\n",
      "[12d07h10m33s][HUGECTR][INFO]: Rank0: Write optimzer state to file\n",
      "[12d07h10m33s][HUGECTR][INFO]: Done\n",
      "[12d07h10m37s][HUGECTR][INFO]: Dumping sparse optimzer states to files, successful\n",
      "[12d07h10m37s][HUGECTR][INFO]: Dumping dense weights to file, successful\n",
      "[12d07h10m37s][HUGECTR][INFO]: Dumping dense optimizer states to file, successful\n",
      "[12d07h10m37s][HUGECTR][INFO]: Dumping untrainable weights to file, successful\n"
     ]
    }
   ],
   "source": [
    "!python3 wdl_non_epoch.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "first-notice",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wdl_epoch.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wdl_epoch.py\n",
    "import hugectr\n",
    "from mpi4py import MPI\n",
    "solver = hugectr.CreateSolver(max_eval_batches = 5000,\n",
    "                              batchsize_eval = 1024,\n",
    "                              batchsize = 1024,\n",
    "                              vvgpu = [[3]],\n",
    "                              i64_input_key = False,\n",
    "                              use_mixed_precision = False,\n",
    "                              repeat_dataset = False,\n",
    "                              use_cuda_graph = True)\n",
    "reader = hugectr.DataReaderParams(data_reader_type = hugectr.DataReaderType_t.Norm,\n",
    "                          source = [\"wdl_data/file_list.0.txt\"],\n",
    "                          eval_source = \"wdl_data/file_list.1.txt\",\n",
    "                          check_type = hugectr.Check_t.Sum)\n",
    "optimizer = hugectr.CreateOptimizer(optimizer_type = hugectr.Optimizer_t.Adam)\n",
    "model = hugectr.Model(solver, reader, optimizer)\n",
    "model.construct_from_json(graph_config_file = \"wdl.json\", include_dense_network = True)\n",
    "model.compile()\n",
    "lr_sch = model.get_learning_rate_scheduler()\n",
    "data_reader_train = model.get_data_reader_train()\n",
    "data_reader_eval = model.get_data_reader_eval()\n",
    "data_reader_eval.set_source()\n",
    "data_reader_eval_flag = True\n",
    "iteration = 0\n",
    "for epoch in range(2):\n",
    "  print(\"[HUGECTR][INFO] epoch: \", epoch)\n",
    "  data_reader_train.set_source()\n",
    "  data_reader_train_flag = True\n",
    "  while True:\n",
    "    lr = lr_sch.get_next()\n",
    "    model.set_learning_rate(lr)\n",
    "    data_reader_train_flag = model.train()\n",
    "    if not data_reader_train_flag:\n",
    "      break\n",
    "    if iteration % 1000 == 0:\n",
    "      batches = 0\n",
    "      while data_reader_eval_flag:\n",
    "        if batches >= solver.max_eval_batches:\n",
    "          break\n",
    "        data_reader_eval_flag = model.eval()\n",
    "        batches += 1\n",
    "      if not data_reader_eval_flag:\n",
    "        data_reader_eval.set_source()\n",
    "        data_reader_eval_flag = True\n",
    "      metrics = model.get_eval_metrics()\n",
    "      print(\"[HUGECTR][INFO] iter: {}, metrics: {}\".format(iteration, metrics))\n",
    "    iteration += 1\n",
    "model.save_params_to_files(\"./\", iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "seventh-mechanics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================Model Init=====================================================\n",
      "[12d07h10m43s][HUGECTR][INFO]: Global seed is 3928091503\n",
      "[12d07h10m46s][HUGECTR][INFO]: Peer-to-peer access cannot be fully enabled.\n",
      "Device 3: Tesla T4\n",
      "[12d07h10m46s][HUGECTR][INFO]: num of DataReader workers: 12\n",
      "[12d07h10m46s][HUGECTR][INFO]: max_vocabulary_size_per_gpu_=6029312\n",
      "[12d07h10m46s][HUGECTR][INFO]: max_vocabulary_size_per_gpu_=5865472\n",
      "[12d07h10m46s][HUGECTR][INFO]: Load the model graph from wdl.json, successful\n",
      "===================================================Model Compile===================================================\n",
      "[12d07h10m54s][HUGECTR][INFO]: gpu0 start to init embedding\n",
      "[12d07h10m54s][HUGECTR][INFO]: gpu0 init embedding done\n",
      "[12d07h10m54s][HUGECTR][INFO]: gpu0 start to init embedding\n",
      "[12d07h10m54s][HUGECTR][INFO]: gpu0 init embedding done\n",
      "[HUGECTR][INFO] epoch:  0\n",
      "[HUGECTR][INFO] iter: 0, metrics: [('AUC', 0.4918196201324463)]\n",
      "[HUGECTR][INFO] iter: 1000, metrics: [('AUC', 0.735563337802887)]\n",
      "[HUGECTR][INFO] iter: 2000, metrics: [('AUC', 0.746255099773407)]\n",
      "[HUGECTR][INFO] iter: 3000, metrics: [('AUC', 0.7497995495796204)]\n",
      "[HUGECTR][INFO] epoch:  1\n",
      "[HUGECTR][INFO] iter: 4000, metrics: [('AUC', 0.751632809638977)]\n",
      "[HUGECTR][INFO] iter: 5000, metrics: [('AUC', 0.7169237732887268)]\n",
      "[HUGECTR][INFO] iter: 6000, metrics: [('AUC', 0.7138432860374451)]\n",
      "[HUGECTR][INFO] iter: 7000, metrics: [('AUC', 0.7201796174049377)]\n",
      "[12d07h16m13s][HUGECTR][INFO]: Rank0: Write hash table to file\n",
      "[12d07h16m13s][HUGECTR][INFO]: Rank0: Write hash table to file\n",
      "[12d07h16m13s][HUGECTR][INFO]: Dumping sparse weights to files, successful\n",
      "[12d07h16m13s][HUGECTR][INFO]: Rank0: Write optimzer state to file\n",
      "[12d07h16m13s][HUGECTR][INFO]: Done\n",
      "[12d07h16m13s][HUGECTR][INFO]: Rank0: Write optimzer state to file\n",
      "[12d07h16m13s][HUGECTR][INFO]: Done\n",
      "[12d07h16m14s][HUGECTR][INFO]: Rank0: Write optimzer state to file\n",
      "[12d07h16m14s][HUGECTR][INFO]: Done\n",
      "[12d07h16m15s][HUGECTR][INFO]: Rank0: Write optimzer state to file\n",
      "[12d07h16m15s][HUGECTR][INFO]: Done\n",
      "[12d07h16m15s][HUGECTR][INFO]: Dumping sparse optimzer states to files, successful\n",
      "[12d07h16m15s][HUGECTR][INFO]: Dumping dense weights to file, successful\n",
      "[12d07h16m15s][HUGECTR][INFO]: Dumping dense optimizer states to file, successful\n",
      "[12d07h16m15s][HUGECTR][INFO]: Dumping untrainable weights to file, successful\n"
     ]
    }
   ],
   "source": [
    "!python3 wdl_epoch.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "collective-newman",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wdl_mos.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wdl_mos.py\n",
    "import hugectr\n",
    "from mpi4py import MPI\n",
    "solver = hugectr.CreateSolver(max_eval_batches = 5000,\n",
    "                              batchsize_eval = 1024,\n",
    "                              batchsize = 1024,\n",
    "                              vvgpu = [[3]],\n",
    "                              i64_input_key = False,\n",
    "                              use_mixed_precision = False,\n",
    "                              repeat_dataset = False,\n",
    "                              use_cuda_graph = True)\n",
    "reader = hugectr.DataReaderParams(data_reader_type = hugectr.DataReaderType_t.Norm,\n",
    "                          source = [\"wdl_data/file_list.\"+str(i)+\".txt\" for i in range(2)],\n",
    "                          keyset = [\"wdl_data/file_list.\"+str(i)+\".keyset\" for i in range(2)],\n",
    "                          eval_source = \"wdl_data/file_list.2.txt\",\n",
    "                          check_type = hugectr.Check_t.Sum)\n",
    "optimizer = hugectr.CreateOptimizer(optimizer_type = hugectr.Optimizer_t.Adam)\n",
    "mos = hugectr.CreateMOS(train_from_scratch = True, use_host_memory_ps = True, dest_sparse_models = [\"./wdl_low_0_sparse_model\", \"./wdl_low_1_sparse_model\"])\n",
    "model = hugectr.Model(solver, reader, optimizer, mos)\n",
    "model.construct_from_json(graph_config_file = \"wdl.json\", include_dense_network = True)\n",
    "model.compile()\n",
    "lr_sch = model.get_learning_rate_scheduler()\n",
    "data_reader_train = model.get_data_reader_train()\n",
    "data_reader_eval = model.get_data_reader_eval()\n",
    "model_oversubscriber = model.get_model_oversubscriber()\n",
    "dataset = [(\"wdl_data/file_list.\"+str(i)+\".txt\", \"wdl_data/file_list.\"+str(i)+\".keyset\") for i in range(2)]\n",
    "data_reader_eval.set_source(\"wdl_data/file_list.2.txt\")\n",
    "data_reader_eval_flag = True\n",
    "iteration = 0\n",
    "for file_list, keyset_file in dataset:\n",
    "  data_reader_train.set_source(file_list)\n",
    "  data_reader_train_flag = True\n",
    "  model_oversubscriber.update(keyset_file)\n",
    "  while True:\n",
    "    lr = lr_sch.get_next()\n",
    "    model.set_learning_rate(lr)\n",
    "    data_reader_train_flag = model.train()\n",
    "    if not data_reader_train_flag:\n",
    "      break\n",
    "    if iteration % 1000 == 0:\n",
    "      batches = 0\n",
    "      while data_reader_eval_flag:\n",
    "        if batches >= solver.max_eval_batches:\n",
    "          break\n",
    "        data_reader_eval_flag = model.eval()\n",
    "        batches += 1\n",
    "      if not data_reader_eval_flag:\n",
    "        data_reader_eval.set_source()\n",
    "        data_reader_eval_flag = True\n",
    "      metrics = model.get_eval_metrics()\n",
    "      print(\"[HUGECTR][INFO] iter: {}, metrics: {}\".format(iteration, metrics))\n",
    "    iteration += 1\n",
    "  print(\"[HUGECTR][INFO] trained with data in {}\".format(file_list))\n",
    "model.save_params_to_files(\"wdl_mos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "provincial-proportion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================Model Init=====================================================\n",
      "[12d07h16m58s][HUGECTR][INFO]: Global seed is 448318010\n",
      "[12d07h17m01s][HUGECTR][INFO]: Peer-to-peer access cannot be fully enabled.\n",
      "Device 3: Tesla T4\n",
      "[12d07h17m01s][HUGECTR][INFO]: num of DataReader workers: 12\n",
      "[12d07h17m01s][HUGECTR][INFO]: max_vocabulary_size_per_gpu_=6029312\n",
      "[12d07h17m01s][HUGECTR][INFO]: max_vocabulary_size_per_gpu_=5865472\n",
      "[12d07h17m01s][HUGECTR][INFO]: Load the model graph from wdl.json, successful\n",
      "===================================================Model Compile===================================================\n",
      "[12d07h17m09s][HUGECTR][INFO]: gpu0 start to init embedding\n",
      "[12d07h17m09s][HUGECTR][INFO]: gpu0 init embedding done\n",
      "[12d07h17m09s][HUGECTR][INFO]: gpu0 start to init embedding\n",
      "[12d07h17m09s][HUGECTR][INFO]: gpu0 init embedding done\n",
      "[12d07h17m09s][HUGECTR][INFO]: Host MEM-based Parameter Server is enabled\n",
      "[12d07h17m09s][HUGECTR][INFO]: construct sparse models for model oversubscriber: ./wdl_low_0_sparse_model\n",
      "[12d07h17m09s][HUGECTR][INFO]: ./wdl_low_0_sparse_model not exist, create and train from scratch\n",
      "[12d07h17m09s][HUGECTR][INFO]: construct sparse models for model oversubscriber: ./wdl_low_1_sparse_model\n",
      "[12d07h17m09s][HUGECTR][INFO]: ./wdl_low_1_sparse_model not exist, create and train from scratch\n",
      "[12d07h17m10s][HUGECTR][INFO]: Preparing embedding table for next pass [DONE]\n",
      "[HUGECTR][INFO] iter: 0, metrics: [('AUC', 0.5099893808364868)]\n",
      "[HUGECTR][INFO] iter: 1000, metrics: [('AUC', 0.7376354932785034)]\n",
      "[HUGECTR][INFO] iter: 2000, metrics: [('AUC', 0.74676513671875)]\n",
      "[HUGECTR][INFO] iter: 3000, metrics: [('AUC', 0.7509114146232605)]\n",
      "[HUGECTR][INFO] trained with data in wdl_data/file_list.0.txt\n",
      "[12d07h19m50s][HUGECTR][INFO]: Preparing embedding table for next pass [DONE]\n",
      "[HUGECTR][INFO] iter: 4000, metrics: [('AUC', 0.7543617486953735)]\n",
      "[HUGECTR][INFO] iter: 5000, metrics: [('AUC', 0.7506358623504639)]\n",
      "[HUGECTR][INFO] iter: 6000, metrics: [('AUC', 0.7539882063865662)]\n",
      "[HUGECTR][INFO] iter: 7000, metrics: [('AUC', 0.756419837474823)]\n",
      "[HUGECTR][INFO] trained with data in wdl_data/file_list.1.txt\n",
      "[12d07h22m53s][HUGECTR][INFO]: Updating sparse model in SSD [DONE]\n",
      "[12d07h22m55s][HUGECTR][INFO]: Updating sparse model in SSD [DONE]\n",
      "[12d07h22m57s][HUGECTR][INFO]: Dumping dense weights to file, successful\n",
      "[12d07h22m57s][HUGECTR][INFO]: Dumping dense optimizer states to file, successful\n",
      "[12d07h22m57s][HUGECTR][INFO]: Dumping untrainable weights to file, successful\n"
     ]
    }
   ],
   "source": [
    "!python3 wdl_mos.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}