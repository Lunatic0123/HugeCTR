{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# HugeCTR Python Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "HugeCTR is a recommender specific framework which is capable of distributed training across multiple GPUs and nodes for Click-Through-Rate (CTR) estimation. It is a component of NVIDIA [Merlin](https://developer.nvidia.com/nvidia-merlin#getstarted), which is a framework accelerating the entire pipeline from data ingestion and training to deploying GPU-accelerated recommender systems.\n",
    "\n",
    "The Python interface is incorporated into HugeCTR in version 2.3, which supports setting data source and model oversubscribing during training. This notebook will introduce how to access HugeCTR Python interface and demontrate how to use it.The API signatures of Python interface will also be displayed.\n",
    "\n",
    "## Content\n",
    "1. [Build HugeCTR Python Interface](#1)\n",
    "1. [Wide&Deep Demo](#2)\n",
    "1. [API Signatures](#3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "## 1. Build HugeCTR Python Interface\n",
    "\n",
    "HugeCTR Python Interface takes form of the dynamic library. In order to use Python interface, you should enter the HugeCTR docker container and build HugeCTR using the following commands.\n",
    "```shell\n",
    "$ cd hugectr\n",
    "$ mkdir -p build && cd build\n",
    "$ cmake -DCMAKE_BUILD_TYPE=Release -DSM=70 .. # Target is NVIDIA V100\n",
    "$ make -j\n",
    "```\n",
    "After building, there will be a dynamic link library `hugectr.so` in the `hugectr/build/lib/` folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /hugectr/build/lib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can copy `hugectr.so` to the folder where you want to use Python interface. You can also install it to `/usr/local/hugectr/lib` and set the environment variable `export PYTHONPATH=/usr/local/hugectr/lib:$PYTHONPATH` if you want to use Python interface in the whole docker container environment. Then you can import hugectr and train your model simply with Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp /hugectr/build/lib/hugectr.so /hugectr/notebooks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hugectr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "## 2. Wide&Deep Demo\n",
    "\n",
    "### 2.1 Data Download and Preprocess\n",
    "Download dataset from [Kaggle Criteo datasets](http://labs.criteo.com/2014/02/kaggle-display-advertising-challenge-dataset/)\n",
    "```shell\n",
    "$ wget https://s3-eu-west-1.amazonaws.com/kaggle-display-advertising-challenge-dataset/dac.tar.gz\n",
    "```\n",
    "\n",
    "Extract the dataset\n",
    "```shell\n",
    "$ tar zxvf dac.tar.gz\n",
    "```\n",
    "\n",
    "Preprocess the data\n",
    "```shell\n",
    "$mkdir wdl_data\n",
    "$shuf train.txt > train.shuf.txt\n",
    "$python3 /hugectr/tools/criteo_script/preprocess.py --src_csv_path=train.shuf.txt --dst_csv_path=wdl_data/train.out.txt --normalize_dense=1 --feature_cross=1\n",
    "```\n",
    "\n",
    "Split the dataset\n",
    "```shell\n",
    "head -n 36672493 wdl_data/train.out.txt > wdl_data/train && \\\n",
    "tail -n 9168124 wdl_data/train.out.txt > wdl_data/valtest && \\\n",
    "head -n 4584062 wdl_data/valtest > wdl_data/val && \\\n",
    "tail -n 4584062 wdl_data/valtest > wdl_data/test\n",
    "```\n",
    "\n",
    "Then we can convert the dataset to HugeCTR Norm format. Specifically, we will generate `file_list.*.txt` and `file_list.*.keyset`, together with all training data `*.data` so that we can employ the features of `set source` and `model prefetch` during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile criteo2hugectr.sh\n",
    "mkdir -p wdl_data_hugectr/wdl_data_bin && \\\n",
    "cd wdl_data_hugectr && \\\n",
    "cp /hugectr/build/bin/criteo2hugectr ./ &&\n",
    "./criteo2hugectr /wdl_data/train wdl_data_bin/ file_list.txt 2 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash criteo2hugectr.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Train from scratch\n",
    "\n",
    "We can train scratch and store the trained dense model and embedding tables to files. A json file for Wide&Deep model should be created first. Please note that the `solver` clause is no longer needed in json file when using Python interface. Instead, we can configure the parameters using `hugectr.solver_parser_helper()` directly in Python interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile wdl_1gpu.json\n",
    "{\n",
    "  \"optimizer\": {\n",
    "    \"type\": \"Adam\",\n",
    "    \"update_type\": \"Global\",\n",
    "    \"adam_hparam\": {\n",
    "      \"learning_rate\": 0.001,\n",
    "      \"beta1\": 0.9,\n",
    "      \"beta2\": 0.999,\n",
    "      \"epsilon\": 0.0000001\n",
    "    }\n",
    "  },\n",
    "  \"layers\": [\n",
    "    {\n",
    "      \"name\": \"data\",\n",
    "      \"type\": \"Data\",\n",
    "      \"source\": \"./file_list.0.txt\",\n",
    "      \"eval_source\": \"./file_list.5.txt\",\n",
    "      \"check\": \"Sum\",\n",
    "      \"label\": {\n",
    "        \"top\": \"label\",\n",
    "        \"label_dim\": 1\n",
    "      },\n",
    "      \"dense\": {\n",
    "        \"top\": \"dense\",\n",
    "        \"dense_dim\": 13\n",
    "      },\n",
    "      \"sparse\": [\n",
    "        {\n",
    "          \"top\": \"wide_data\",\n",
    "          \"type\": \"DistributedSlot\",\n",
    "          \"max_feature_num_per_sample\": 30,\n",
    "          \"slot_num\": 1\n",
    "        },\n",
    "        {\n",
    "          \"top\": \"deep_data\",\n",
    "          \"type\": \"DistributedSlot\",\n",
    "          \"max_feature_num_per_sample\": 30,\n",
    "          \"slot_num\": 26\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"sparse_embedding2\",\n",
    "      \"type\": \"DistributedSlotSparseEmbeddingHash\",\n",
    "      \"bottom\": \"wide_data\",\n",
    "      \"top\": \"sparse_embedding2\",\n",
    "      \"sparse_embedding_hparam\": {\n",
    "        \"max_vocabulary_size_per_gpu\": 2322444,\n",
    "        \"embedding_vec_size\": 1,\n",
    "        \"combiner\": 0\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"sparse_embedding1\",\n",
    "      \"type\": \"DistributedSlotSparseEmbeddingHash\",\n",
    "      \"bottom\": \"deep_data\",\n",
    "      \"top\": \"sparse_embedding1\",\n",
    "      \"sparse_embedding_hparam\": {\n",
    "        \"max_vocabulary_size_per_gpu\": 2322444,\n",
    "        \"embedding_vec_size\": 16,\n",
    "        \"combiner\": 0\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"reshape1\",\n",
    "      \"type\": \"Reshape\",\n",
    "      \"bottom\": \"sparse_embedding1\",\n",
    "      \"top\": \"reshape1\",\n",
    "      \"leading_dim\": 416\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"reshape2\",\n",
    "      \"type\": \"Reshape\",\n",
    "      \"bottom\": \"sparse_embedding2\",\n",
    "      \"top\": \"reshape2\",\n",
    "      \"leading_dim\": 1\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"concat1\",\n",
    "      \"type\": \"Concat\",\n",
    "      \"bottom\": [\n",
    "        \"reshape1\",\n",
    "        \"dense\"\n",
    "      ],\n",
    "      \"top\": \"concat1\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"fc1\",\n",
    "      \"type\": \"InnerProduct\",\n",
    "      \"bottom\": \"concat1\",\n",
    "      \"top\": \"fc1\",\n",
    "      \"fc_param\": {\n",
    "        \"num_output\": 1024\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"relu1\",\n",
    "      \"type\": \"ReLU\",\n",
    "      \"bottom\": \"fc1\",\n",
    "      \"top\": \"relu1\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"dropout1\",\n",
    "      \"type\": \"Dropout\",\n",
    "      \"rate\": 0.5,\n",
    "      \"bottom\": \"relu1\",\n",
    "      \"top\": \"dropout1\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"fc2\",\n",
    "      \"type\": \"InnerProduct\",\n",
    "      \"bottom\": \"dropout1\",\n",
    "      \"top\": \"fc2\",\n",
    "      \"fc_param\": {\n",
    "        \"num_output\": 1024\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"relu2\",\n",
    "      \"type\": \"ReLU\",\n",
    "      \"bottom\": \"fc2\",\n",
    "      \"top\": \"relu2\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"dropout2\",\n",
    "      \"type\": \"Dropout\",\n",
    "      \"rate\": 0.5,\n",
    "      \"bottom\": \"relu2\",\n",
    "      \"top\": \"dropout2\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"fc4\",\n",
    "      \"type\": \"InnerProduct\",\n",
    "      \"bottom\": \"dropout2\",\n",
    "      \"top\": \"fc4\",\n",
    "      \"fc_param\": {\n",
    "        \"num_output\": 1\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"add1\",\n",
    "      \"type\": \"Add\",\n",
    "      \"bottom\": [\n",
    "        \"fc4\",\n",
    "        \"reshape2\"\n",
    "      ],\n",
    "      \"top\": \"add1\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"loss\",\n",
    "      \"type\": \"BinaryCrossEntropyLoss\",\n",
    "      \"bottom\": [\n",
    "        \"add1\",\n",
    "        \"label\"\n",
    "      ],\n",
    "      \"top\": \"loss\"\n",
    "    }\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can write the Python script to train a Wide&Deep model from scratch and store the trained model to files. Here we configure `repeat_dataset` to be `False`, which means we have to specify the file list before the first call to `sess.train()` or `sess.evaluation()`. Besides, we have to create a write-enabled directory for storing the temporary files of model oversubscribing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile wdl_from_scratch.py\n",
    "from hugectr import Session, solver_parser_helper\n",
    "import sys\n",
    "\n",
    "def train_from_scratch(json_file):\n",
    "  dataset = [(\"./file_list.\"+str(i)+\".txt\", \"./file_list.\"+str(i)+\".keyset\") for i in range(5)]\n",
    "  solver_config = solver_parser_helper(seed = 0,\n",
    "                                     batchsize = 16384,\n",
    "                                     batchsize_eval =16384,\n",
    "                                     model_file = \"\",\n",
    "                                     embedding_files = [],\n",
    "                                     vvgpu = [[0]],\n",
    "                                     use_mixed_precision = False,\n",
    "                                     scaler = 1.0,\n",
    "                                     i64_input_key = False,\n",
    "                                     use_algorithm_search = True,\n",
    "                                     use_cuda_graph = True,\n",
    "                                     repeat_dataset = False\n",
    "                                    )\n",
    "  sess = Session(solver_config, json_file, True, \"./temp_embedding\")\n",
    "  data_reader_train = sess.get_data_reader_train()\n",
    "  data_reader_eval = sess.get_data_reader_eval()\n",
    "  data_reader_eval.set_file_list_source(\"./file_list.5.txt\")\n",
    "  model_oversubscriber = sess.get_model_oversubscriber()\n",
    "  iteration = 0\n",
    "  for file_list, keyset_file in dataset:\n",
    "    data_reader_train.set_file_list_source(file_list)\n",
    "    model_oversubscriber.update(keyset_file)\n",
    "    while True:\n",
    "      good = sess.train()\n",
    "      if good == False:\n",
    "        break\n",
    "      if iteration % 100 == 0:\n",
    "        metrics = sess.evaluation()\n",
    "        print(\"[HUGECTR][INFO] iter: {}, metrics: {}\".format(iteration, metrics))\n",
    "      iteration += 1\n",
    "    print(\"[HUGECTR][INFO] trained with data in {}\".format(file_list))\n",
    "  sess.download_params_to_files(\"./\", iteration)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  json_file = sys.argv[1]\n",
    "  train_from_scratch(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile wdl_from_scratch.sh\n",
    "cd wdl_data_hugectr && \\\n",
    "mkdir -p temp_embedding && \\\n",
    "python3 ../wdl_from_scratch.py ../wdl_1gpu.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10d11h09m06s][HUGECTR][INFO]: Initial seed is 2834829260\n",
      "[10d11h09m07s][HUGECTR][INFO]: Peer-to-peer access cannot be fully enabled.\n",
      "Device 0: GeForce RTX 2080 Ti\n",
      "[10d11h09m07s][HUGECTR][INFO]: cache_eval_data is not specified using default: 0\n",
      "[10d11h09m07s][HUGECTR][INFO]: max_nnz is not specified using default: 30\n",
      "[10d11h09m07s][HUGECTR][INFO]: max_nnz is not specified using default: 30\n",
      "[10d11h09m07s][HUGECTR][INFO]: num_internal_buffers 1\n",
      "[10d11h09m07s][HUGECTR][INFO]: num_internal_buffers 1\n",
      "[10d11h09m07s][HUGECTR][INFO]: max_vocabulary_size_per_gpu_=2322444\n",
      "[10d11h09m07s][HUGECTR][INFO]: max_vocabulary_size_per_gpu_=2322444\n",
      "[10d11h09m08s][HUGECTR][INFO]: Traning from scratch, no snapshot file specified\n",
      "[10d11h09m08s][HUGECTR][INFO]: Write hash table <key,value> pairs to file\n",
      "[10d11h09m08s][HUGECTR][INFO]: Write hash table <key,value> pairs to file\n",
      "[10d11h09m08s][HUGECTR][INFO]: Start to upload embedding table file to GPUs, total loop_num: 0\n",
      "[10d11h09m08s][HUGECTR][INFO]: Start to upload embedding table file to GPUs, total loop_num: 0\n",
      "[HUGECTR][INFO] iter: 0, metrics: [('AUC', 0.5132519602775574)]\n",
      "[HUGECTR][INFO] iter: 100, metrics: [('AUC', 0.7764538526535034)]\n",
      "[HUGECTR][INFO] iter: 200, metrics: [('AUC', 0.7801059484481812)]\n",
      "[HUGECTR][INFO] trained with data in ./file_list.0.txt\n",
      "[10d11h09m13s][HUGECTR][INFO]: Dump hash table from GPU0\n",
      "[10d11h09m13s][HUGECTR][INFO]: Write hash table <key,value> pairs to file\n",
      "[10d11h09m14s][HUGECTR][INFO]: Dump hash table from GPU0\n",
      "[10d11h09m14s][HUGECTR][INFO]: Write hash table <key,value> pairs to file\n",
      "[10d11h09m14s][HUGECTR][INFO]: Start to upload embedding table file to GPUs, total loop_num: 249\n",
      "[10d11h09m15s][HUGECTR][INFO]: Start to upload embedding table file to GPUs, total loop_num: 938\n",
      "[HUGECTR][INFO] iter: 300, metrics: [('AUC', 0.7743158340454102)]\n",
      "[HUGECTR][INFO] iter: 400, metrics: [('AUC', 0.7908769845962524)]\n",
      "[HUGECTR][INFO] iter: 500, metrics: [('AUC', 0.7942564487457275)]\n",
      "[HUGECTR][INFO] trained with data in ./file_list.1.txt\n",
      "[10d11h09m21s][HUGECTR][INFO]: Dump hash table from GPU0\n",
      "[10d11h09m21s][HUGECTR][INFO]: Write hash table <key,value> pairs to file\n",
      "[10d11h09m21s][HUGECTR][INFO]: Dump hash table from GPU0\n",
      "[10d11h09m21s][HUGECTR][INFO]: Write hash table <key,value> pairs to file\n",
      "[10d11h09m21s][HUGECTR][INFO]: Start to upload embedding table file to GPUs, total loop_num: 302\n",
      "[10d11h09m23s][HUGECTR][INFO]: Start to upload embedding table file to GPUs, total loop_num: 1115\n",
      "[HUGECTR][INFO] iter: 600, metrics: [('AUC', 0.7939486503601074)]\n",
      "[HUGECTR][INFO] iter: 700, metrics: [('AUC', 0.7941162586212158)]\n",
      "[HUGECTR][INFO] trained with data in ./file_list.2.txt\n",
      "[10d11h09m28s][HUGECTR][INFO]: Dump hash table from GPU0\n",
      "[10d11h09m29s][HUGECTR][INFO]: Write hash table <key,value> pairs to file\n",
      "[10d11h09m29s][HUGECTR][INFO]: Dump hash table from GPU0\n",
      "[10d11h09m29s][HUGECTR][INFO]: Write hash table <key,value> pairs to file\n",
      "[10d11h09m30s][HUGECTR][INFO]: Start to upload embedding table file to GPUs, total loop_num: 324\n",
      "[10d11h09m31s][HUGECTR][INFO]: Start to upload embedding table file to GPUs, total loop_num: 1186\n",
      "[HUGECTR][INFO] iter: 800, metrics: [('AUC', 0.7932926416397095)]\n",
      "[HUGECTR][INFO] iter: 900, metrics: [('AUC', 0.7985045909881592)]\n",
      "[HUGECTR][INFO] iter: 1000, metrics: [('AUC', 0.7881430387496948)]\n",
      "[HUGECTR][INFO] trained with data in ./file_list.3.txt\n",
      "[10d11h09m37s][HUGECTR][INFO]: Dump hash table from GPU0\n",
      "[10d11h09m37s][HUGECTR][INFO]: Write hash table <key,value> pairs to file\n",
      "[10d11h09m37s][HUGECTR][INFO]: Dump hash table from GPU0\n",
      "[10d11h09m37s][HUGECTR][INFO]: Write hash table <key,value> pairs to file\n",
      "[10d11h09m38s][HUGECTR][INFO]: Start to upload embedding table file to GPUs, total loop_num: 334\n",
      "[10d11h09m40s][HUGECTR][INFO]: Start to upload embedding table file to GPUs, total loop_num: 1216\n",
      "[HUGECTR][INFO] iter: 1100, metrics: [('AUC', 0.7952668070793152)]\n",
      "[HUGECTR][INFO] iter: 1200, metrics: [('AUC', 0.7960667014122009)]\n",
      "[HUGECTR][INFO] trained with data in ./file_list.4.txt\n",
      "[10d11h09m45s][HUGECTR][INFO]: Rank0: Dump hash table from GPU0\n",
      "[10d11h09m45s][HUGECTR][INFO]: Rank0: Write hash table <key,value> pairs to file\n",
      "[10d11h09m45s][HUGECTR][INFO]: Done\n",
      "[10d11h09m45s][HUGECTR][INFO]: Rank0: Dump hash table from GPU0\n",
      "[10d11h09m46s][HUGECTR][INFO]: Rank0: Write hash table <key,value> pairs to file\n",
      "[10d11h09m46s][HUGECTR][INFO]: Done\n"
     ]
    }
   ],
   "source": [
    "!bash wdl_from_scratch.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Train from stored model\n",
    "\n",
    "We should check the stored model files, which will be used in the training. Dense model file embedding model files should be passed to `model_file` and `embedding_files` respectively when calling `sess.solver_parser_helper()`. We will use the same json file and training data as the previous section. Besides, all the other configurations for `solver_parser_helper` will also be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wdl_data_hugectr/0_sparse_1260.model  wdl_data_hugectr/_dense_1260.model\r\n",
      "wdl_data_hugectr/1_sparse_1260.model\r\n"
     ]
    }
   ],
   "source": [
    "!ls wdl_data_hugectr/*.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile wdl_from_stored.py\n",
    "from hugectr import Session, solver_parser_helper\n",
    "import sys\n",
    "\n",
    "def train_from_stored(json_file):\n",
    "  dataset = [(\"./file_list.\"+str(i)+\".txt\", \"./file_list.\"+str(i)+\".keyset\") for i in range(5)]\n",
    "  solver_config = solver_parser_helper(seed = 0,\n",
    "                                     batchsize = 16384,\n",
    "                                     batchsize_eval =16384,\n",
    "                                     model_file = \"_dense_1260.model\",\n",
    "                                     embedding_files = [\"0_sparse_1260.model\", \"1_sparse_1260.model\"],\n",
    "                                     vvgpu = [[0]],\n",
    "                                     use_mixed_precision = False,\n",
    "                                     scaler = 1.0,\n",
    "                                     i64_input_key = False,\n",
    "                                     use_algorithm_search = True,\n",
    "                                     use_cuda_graph = True,\n",
    "                                     repeat_dataset = False\n",
    "                                    )\n",
    "  sess = Session(solver_config, json_file, True, \"./temp_embedding\")\n",
    "  data_reader_train = sess.get_data_reader_train()\n",
    "  data_reader_eval = sess.get_data_reader_eval()\n",
    "  data_reader_eval.set_file_list_source(\"./file_list.5.txt\")\n",
    "  model_oversubscriber = sess.get_model_oversubscriber()\n",
    "  iteration = 1260\n",
    "  for file_list, keyset_file in dataset:\n",
    "    data_reader_train.set_file_list_source(file_list)\n",
    "    model_oversubscriber.update(keyset_file)\n",
    "    while True:\n",
    "      good = sess.train()\n",
    "      if good == False:\n",
    "        break\n",
    "      if iteration % 100 == 0:\n",
    "        metrics = sess.evaluation()\n",
    "        print(\"[HUGECTR][INFO] iter: {}, metrics: {}\".format(iteration, metrics))\n",
    "      iteration += 1\n",
    "    print(\"[HUGECTR][INFO] trained with data in {}\".format(file_list))\n",
    "  sess.download_params_to_files(\"./\", iteration)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  json_file = sys.argv[1]\n",
    "  train_from_stored(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile wdl_from_stored.sh\n",
    "cd wdl_data_hugectr && \\\n",
    "mkdir -p temp_embedding && \\\n",
    "python3 ../wdl_from_stored.py ../wdl_1gpu.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10d11h09m56s][HUGECTR][INFO]: Initial seed is 1957937766\n",
      "[10d11h09m57s][HUGECTR][INFO]: Peer-to-peer access cannot be fully enabled.\n",
      "Device 0: GeForce RTX 2080 Ti\n",
      "[10d11h09m57s][HUGECTR][INFO]: cache_eval_data is not specified using default: 0\n",
      "[10d11h09m57s][HUGECTR][INFO]: max_nnz is not specified using default: 30\n",
      "[10d11h09m57s][HUGECTR][INFO]: max_nnz is not specified using default: 30\n",
      "[10d11h09m57s][HUGECTR][INFO]: num_internal_buffers 1\n",
      "[10d11h09m57s][HUGECTR][INFO]: num_internal_buffers 1\n",
      "[10d11h09m57s][HUGECTR][INFO]: max_vocabulary_size_per_gpu_=2322444\n",
      "[10d11h09m57s][HUGECTR][INFO]: max_vocabulary_size_per_gpu_=2322444\n",
      "Loading dense model: _dense_1260.model\n",
      "[10d11h09m58s][HUGECTR][INFO]: Write hash table <key,value> pairs to file\n",
      "[10d11h09m59s][HUGECTR][INFO]: Write hash table <key,value> pairs to file\n",
      "[10d11h09m59s][HUGECTR][INFO]: Start to upload embedding table file to GPUs, total loop_num: 250\n",
      "[10d11h10m00s][HUGECTR][INFO]: Start to upload embedding table file to GPUs, total loop_num: 941\n",
      "[HUGECTR][INFO] iter: 1300, metrics: [('AUC', 0.7941128015518188)]\n",
      "[HUGECTR][INFO] iter: 1400, metrics: [('AUC', 0.801400899887085)]\n",
      "[HUGECTR][INFO] iter: 1500, metrics: [('AUC', 0.796938955783844)]\n",
      "[HUGECTR][INFO] trained with data in ./file_list.0.txt\n",
      "[10d11h10m05s][HUGECTR][INFO]: Dump hash table from GPU0\n",
      "[10d11h10m05s][HUGECTR][INFO]: Write hash table <key,value> pairs to file\n",
      "[10d11h10m05s][HUGECTR][INFO]: Dump hash table from GPU0\n",
      "[10d11h10m05s][HUGECTR][INFO]: Write hash table <key,value> pairs to file\n",
      "[10d11h10m06s][HUGECTR][INFO]: Start to upload embedding table file to GPUs, total loop_num: 303\n",
      "[10d11h10m08s][HUGECTR][INFO]: Start to upload embedding table file to GPUs, total loop_num: 1117\n",
      "[HUGECTR][INFO] iter: 1600, metrics: [('AUC', 0.7915458083152771)]\n",
      "[HUGECTR][INFO] iter: 1700, metrics: [('AUC', 0.8012568950653076)]\n",
      "[HUGECTR][INFO] trained with data in ./file_list.1.txt\n",
      "[10d11h10m13s][HUGECTR][INFO]: Dump hash table from GPU0\n",
      "[10d11h10m13s][HUGECTR][INFO]: Write hash table <key,value> pairs to file\n",
      "[10d11h10m13s][HUGECTR][INFO]: Dump hash table from GPU0\n",
      "[10d11h10m13s][HUGECTR][INFO]: Write hash table <key,value> pairs to file\n",
      "[10d11h10m14s][HUGECTR][INFO]: Start to upload embedding table file to GPUs, total loop_num: 324\n",
      "[10d11h10m16s][HUGECTR][INFO]: Start to upload embedding table file to GPUs, total loop_num: 1186\n",
      "[HUGECTR][INFO] iter: 1800, metrics: [('AUC', 0.7990601062774658)]\n",
      "[HUGECTR][INFO] iter: 1900, metrics: [('AUC', 0.8019447326660156)]\n",
      "[HUGECTR][INFO] iter: 2000, metrics: [('AUC', 0.8010233640670776)]\n",
      "[HUGECTR][INFO] trained with data in ./file_list.2.txt\n",
      "[10d11h10m22s][HUGECTR][INFO]: Dump hash table from GPU0\n",
      "[10d11h10m22s][HUGECTR][INFO]: Write hash table <key,value> pairs to file\n",
      "[10d11h10m22s][HUGECTR][INFO]: Dump hash table from GPU0\n",
      "[10d11h10m22s][HUGECTR][INFO]: Write hash table <key,value> pairs to file\n",
      "[10d11h10m23s][HUGECTR][INFO]: Start to upload embedding table file to GPUs, total loop_num: 335\n",
      "[10d11h10m25s][HUGECTR][INFO]: Start to upload embedding table file to GPUs, total loop_num: 1217\n",
      "[HUGECTR][INFO] iter: 2100, metrics: [('AUC', 0.8001001477241516)]\n",
      "[HUGECTR][INFO] iter: 2200, metrics: [('AUC', 0.8033344149589539)]\n",
      "[HUGECTR][INFO] trained with data in ./file_list.3.txt\n",
      "[10d11h10m30s][HUGECTR][INFO]: Dump hash table from GPU0\n",
      "[10d11h10m30s][HUGECTR][INFO]: Write hash table <key,value> pairs to file\n",
      "[10d11h10m30s][HUGECTR][INFO]: Dump hash table from GPU0\n",
      "[10d11h10m30s][HUGECTR][INFO]: Write hash table <key,value> pairs to file\n",
      "[10d11h10m32s][HUGECTR][INFO]: Start to upload embedding table file to GPUs, total loop_num: 350\n",
      "[10d11h10m33s][HUGECTR][INFO]: Start to upload embedding table file to GPUs, total loop_num: 1238\n",
      "[HUGECTR][INFO] iter: 2300, metrics: [('AUC', 0.7890748977661133)]\n",
      "[HUGECTR][INFO] iter: 2400, metrics: [('AUC', 0.7970585227012634)]\n",
      "[HUGECTR][INFO] iter: 2500, metrics: [('AUC', 0.7971604466438293)]\n",
      "[HUGECTR][INFO] trained with data in ./file_list.4.txt\n",
      "[10d11h10m39s][HUGECTR][INFO]: Rank0: Dump hash table from GPU0\n",
      "[10d11h10m39s][HUGECTR][INFO]: Rank0: Write hash table <key,value> pairs to file\n",
      "[10d11h10m39s][HUGECTR][INFO]: Done\n",
      "[10d11h10m39s][HUGECTR][INFO]: Rank0: Dump hash table from GPU0\n",
      "[10d11h10m39s][HUGECTR][INFO]: Rank0: Write hash table <key,value> pairs to file\n",
      "[10d11h10m39s][HUGECTR][INFO]: Done\n"
     ]
    }
   ],
   "source": [
    "!bash wdl_from_stored.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "## 3. API Signatures\n",
    "\n",
    "We will display all the API signatures of HugeCTR Python Interface that you need to get familiar with to train your own model. As you can see from the above example, we will cover `Session`, `DataReader`, `ModelPrefetcher` and `solver_parser_helper`.\n",
    "\n",
    "**Session**\n",
    "```bash\n",
    "class Session(pybind11_builtins.pybind11_object)\n",
    " |  Method resolution order:\n",
    " |      Session\n",
    " |      pybind11_builtins.pybind11_object\n",
    " |      builtins.object\n",
    " |  \n",
    " |  Methods defined here:\n",
    " |  \n",
    " |  __init__(...)\n",
    " |      __init__(self: hugectr.Session, solver_config: hugectr.SolverParser, config_file: str, use_model_oversubscriber: bool=False, temp_embedding_dir: str='') -> None\n",
    " |  \n",
    " |  check_overflow(...)\n",
    " |      check_overflow(self: hugectr.Session) -> None\n",
    " |  \n",
    " |  download_params_to_files(...)\n",
    " |      download_params_to_files(self: hugectr.Session, prefix: str, iter: int) -> hugectr.Error_t\n",
    " |  \n",
    " |  evaluation(...)\n",
    " |      evaluation(self: hugectr.Session) -> List[Tuple[str, float]]\n",
    " |  \n",
    " |  get_current_loss(...)\n",
    " |      get_current_loss(self: hugectr.Session) -> float\n",
    " |  \n",
    " |  get_data_reader_eval(...)\n",
    " |      get_data_reader_eval(self: hugectr.Session) -> hugectr.IDataReader\n",
    " |  \n",
    " |  get_data_reader_train(...)\n",
    " |      get_data_reader_train(self: hugectr.Session) -> hugectr.IDataReader\n",
    " |  \n",
    " |  get_model_oversubscriber(...)\n",
    " |      get_model_oversubscriber(self: hugectr.Session) -> hugectr.ModelPrefetcher\n",
    " |  \n",
    " |  set_learning_rate(...)\n",
    " |      set_learning_rate(self: hugectr.Session, lr: float) -> hugectr.Error_t\n",
    " |  \n",
    " |  start_data_reading(...)\n",
    " |      start_data_reading(self: hugectr.Session) -> None\n",
    " |  \n",
    " |  train(...)\n",
    " |      train(self: hugectr.Session) -> bool\n",
    " |  \n",
    "```\n",
    "\n",
    "**DataReader**\n",
    "```bash\n",
    "class DataReader32(IDataReader)\n",
    " |  Method resolution order:\n",
    " |      DataReader32\n",
    " |      IDataReader\n",
    " |      pybind11_builtins.pybind11_object\n",
    " |      builtins.object\n",
    " |  \n",
    " |  Methods defined here:\n",
    " |   \n",
    " |  set_file_list_source(...)\n",
    " |      set_file_list_source(self: hugectr.DataReader32, file_list: str='') -> None\n",
    " \n",
    "class DataReader64(IDataReader)\n",
    " |  Method resolution order:\n",
    " |      DataReader64\n",
    " |      IDataReader\n",
    " |      pybind11_builtins.pybind11_object\n",
    " |      builtins.object\n",
    " |  \n",
    " |  Methods defined here:\n",
    " |   \n",
    " |  set_file_list_source(...)\n",
    " |      set_file_list_source(self: hugectr.DataReader64, file_list: str='') -> None\n",
    "```\n",
    "\n",
    "**ModelPrefetcher**\n",
    "```bash\n",
    "class ModelPrefetcher(pybind11_builtins.pybind11_object)\n",
    " |  Method resolution order:\n",
    " |      ModelPrefetcher\n",
    " |      pybind11_builtins.pybind11_object\n",
    " |      builtins.object\n",
    " |  \n",
    " |  Methods defined here:\n",
    " |   \n",
    " |  update(...)\n",
    " |      update(*args, **kwargs)\n",
    " |      Overloaded function.\n",
    " |      \n",
    " |      1. update(self: hugectr.ModelPrefetcher, keyset_file: str) -> None\n",
    " |      \n",
    " |      2. update(self: hugectr.ModelPrefetcher, keyset_file_list: List[str]) -> None\n",
    " ```\n",
    " \n",
    " **solver_parser_helper**\n",
    " ```bash\n",
    " solver_parser_helper(...) method of builtins.PyCapsule instance\n",
    "    solver_parser_helper(seed: int=0, batchsize_eval: int=16384, batchsize: int=16384, model_file: str='', embedding_files: List[str]=[], vvgpu: List[List[int]]=[[0]], use_mixed_precision: bool=False, scaler: float=1.0, i64_input_key: bool=False, use_algorithm_search: bool=True, use_cuda_graph: bool=True, repeat_dataset: bool=False) -> hugectr.SolverParser\n",
    " ```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
