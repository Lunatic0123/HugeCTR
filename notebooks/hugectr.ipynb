{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "amber-netscape",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# HugeCTR Python Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "professional-burning",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In HugeCTR version 3.1, we provide an enhanced Python inferface, which supports continuous training and inference with high-level APIs. There are three main improvements. Firstly, the model graph can be constructed and dumped to the JSON file with Python code and it saves users from writing JSON configuration files. Secondly, we support the feature of model oversubscription with high-level APIs and extend it further for online training cases. Thirdly, the freezing method is provided for both sparse embedding and dense network, which enables transfer learning and fine-tune for CTR tasks.\n",
    "\n",
    "This notebook explains how to access and use the enhanced HugeCTR Python interface. Please NOTE that the low-level training APIs are still maintained for users who want to have precise control of each training iteration, while migrating to the high-level training APIs is strongly recommended. For more details of the usage of Python API, please refer to [HugeCTR Python Interface](../docs/python_interface.md).\n",
    "\n",
    "## Table of Contents\n",
    "-  [Access the HugeCTR Python Interface](#1)\n",
    "-  [DCN Demo](#2)\n",
    "   * [Download and Preprocess Data](#21)\n",
    "   * [Train from Scratch](#22)\n",
    "   * [Continue Training](#23)\n",
    "   * [Inference](#24)\n",
    "-  [Wide&Deep Demo](#3)\n",
    "   * [Download and Preprocess Data](#31)\n",
    "   * [Model Oversubscription](#32)\n",
    "   * [Fine-tune](#33)\n",
    "   * [Low-level Training](#34)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twenty-corpus",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "## 1. Access the HugeCTR Python Interface\n",
    "\n",
    "1. Please make sure that you have started the notebook inside the running NGC docker container: `nvcr.io/nvidia/merlin/merlin-inference:0.4`.\n",
    "\n",
    "   A dynamic link to the `hugectr.so` library is installed to the system path `/usr/local/hugectr/lib/`. Besides, this system path is added to the environment variable `PYTHONPATH`, which means that you can use the Python interface within the docker container environment. Check the dynamic link with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparable-hudson",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /usr/local/hugectr/lib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "underlying-filter",
   "metadata": {},
   "source": [
    "2. Import HugeCTR, in order to train your model with Python as shown here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "medieval-daniel",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hugectr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intelligent-peeing",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "## 2. DCN Demo\n",
    "\n",
    "<a id=\"21\"></a>\n",
    "### 2.1 Download and Preprocess Data\n",
    "1. Download the Kaggle Criteo dataset using the following command:\n",
    "   ```shell\n",
    "   $ cd ${project_root}/tools\n",
    "   $ wget http://azuremlsampleexperiments.blob.core.windows.net/criteo/day_1.gz\n",
    "   ```\n",
    "   \n",
    "   In preprocessing, we will further reduce the amounts of data to speedup the preprocessing, fill missing values, remove the feature values whose occurrences are very rare, etc. Here we choose pandas preprocessing method to make the dataset ready for HugeCTR training.\n",
    "\n",
    "2. Preprocessing by Pandas using the following command:\n",
    "   ```shell\n",
    "   $ bash preprocess.sh 1 dcn_data pandas 1 0\n",
    "   ```\n",
    "   \n",
    "   The first argument represents the dataset postfix. It is 1 here since day_1 is used. The second argument dcn_data is where the preprocessed data is stored. The fourth arguement (one after pandas) 1 embodies that the normalization is applied to dense features. The last argument 0 means that the feature crossing is not applied.\n",
    "\n",
    "3. Create a soft link to the dataset folder using the following command:\n",
    "   ```shell\n",
    "   $ ln ${project_root}/tools/dcn_data ${project_root}/notebooks/dcn_data\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broadband-dynamics",
   "metadata": {},
   "source": [
    "<a id=\"22\"></a>\n",
    "### 2.2 Train from Scratch\n",
    "\n",
    "We can train fom scratch, dump the model graph to a JSON file, and save the model weights and optimizer states by doing the following with Python APIs:\n",
    "\n",
    "1. Create the solver, reader and optimizer, then initialize the model.\n",
    "2. Construct the model graph by adding input, sparse embedding and dense layers in order.\n",
    "3. Compile the model and have an overview of the model graph.\n",
    "4. Dump the model graph to the JSON file.\n",
    "5. Fit the model, save the model weights and optimizer states implicitly.\n",
    "\n",
    "Please note that the training mode is determined by `repeat_dataset` within `hugectr.CreateSolver`. If it is True, the non-epoch mode training will be adopted and the maximum iterations should be specified by `max_iter` within `hugectr.Model.fit`. If it is False, then the epoch-mode training will be adopted and the number of epochs should be specified by `num_epochs` within `hugectr.Model.fit`.\n",
    "\n",
    "The optimizer that is used to initialize the model applies to the weights of dense layers, while the optimizer for each sparse embedding layer can be specified independently within `hugectr.SparseEmbedding`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "suited-input",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dcn_train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile dcn_train.py\n",
    "import hugectr\n",
    "from mpi4py import MPI\n",
    "solver = hugectr.CreateSolver(max_eval_batches = 1500,\n",
    "                              batchsize_eval = 4096,\n",
    "                              batchsize = 4096,\n",
    "                              i64_input_key = False,\n",
    "                              use_mixed_precision = False,\n",
    "                              repeat_dataset = True,\n",
    "                              use_cuda_graph = True)\n",
    "reader = hugectr.DataReaderParams(data_reader_type = hugectr.DataReaderType_t.Norm,\n",
    "                                  source = [\"./dcn_data/file_list.txt\"],\n",
    "                                  eval_source = \"./dcn_data/file_list_test.txt\",\n",
    "                                  check_type = hugectr.Check_t.Sum)\n",
    "optimizer = hugectr.CreateOptimizer(optimizer_type = hugectr.Optimizer_t.Adam)\n",
    "model = hugectr.Model(solver, reader, optimizer)\n",
    "model.add(hugectr.Input(label_dim = 1, label_name = \"label\",\n",
    "                        dense_dim = 13, dense_name = \"dense\",\n",
    "                        data_reader_sparse_param_array = \n",
    "                        [hugectr.DataReaderSparseParam(hugectr.DataReaderSparse_t.Distributed, 30, 1, 26)],\n",
    "                        sparse_names = [\"data1\"]))\n",
    "model.add(hugectr.SparseEmbedding(embedding_type = hugectr.Embedding_t.DistributedSlotSparseEmbeddingHash, \n",
    "                            max_vocabulary_size_per_gpu = 1447751,\n",
    "                            embedding_vec_size = 16,\n",
    "                            combiner = 0,\n",
    "                            sparse_embedding_name = \"sparse_embedding1\",\n",
    "                            bottom_name = \"data1\",\n",
    "                            optimizer = optimizer))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Reshape,\n",
    "                            bottom_names = [\"sparse_embedding1\"],\n",
    "                            top_names = [\"reshape1\"],\n",
    "                            leading_dim=416))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Concat,\n",
    "                            bottom_names = [\"reshape1\", \"dense\"], top_names = [\"concat1\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Slice,\n",
    "                            bottom_names = [\"concat1\"],\n",
    "                            top_names = [\"slice11\", \"slice12\"],\n",
    "                            ranges=[(0,429),(0,429)]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.MultiCross,\n",
    "                            bottom_names = [\"slice11\"],\n",
    "                            top_names = [\"multicross1\"],\n",
    "                            num_layers=6))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"slice12\"],\n",
    "                            top_names = [\"fc1\"],\n",
    "                            num_output=1024))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc1\"],\n",
    "                            top_names = [\"relu1\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Dropout,\n",
    "                            bottom_names = [\"relu1\"],\n",
    "                            top_names = [\"dropout1\"],\n",
    "                            dropout_rate=0.5))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"dropout1\"],\n",
    "                            top_names = [\"fc2\"],\n",
    "                            num_output=1024))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc2\"],\n",
    "                            top_names = [\"relu2\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Dropout,\n",
    "                            bottom_names = [\"relu2\"],\n",
    "                            top_names = [\"dropout2\"],\n",
    "                            dropout_rate=0.5))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Concat,\n",
    "                            bottom_names = [\"dropout2\", \"multicross1\"],\n",
    "                            top_names = [\"concat2\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"concat2\"],\n",
    "                            top_names = [\"fc3\"],\n",
    "                            num_output=1))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.BinaryCrossEntropyLoss,\n",
    "                            bottom_names = [\"fc3\", \"label\"],\n",
    "                            top_names = [\"loss\"]))\n",
    "model.compile()\n",
    "model.summary()\n",
    "model.graph_to_json(graph_config_file = \"dcn.json\")\n",
    "model.fit(max_iter = 12000, display = 500, eval_interval = 1000, snapshot = 10000, snapshot_prefix = \"dcn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "million-flower",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================Model Init=====================================================\n",
      "[08d06h25m16s][HUGECTR][INFO]: Global seed is 1933201219\n",
      "[08d06h25m17s][HUGECTR][INFO]: Peer-to-peer access cannot be fully enabled.\n",
      "Device 0: Tesla V100-SXM2-16GB\n",
      "[08d06h25m17s][HUGECTR][INFO]: num of DataReader workers: 12\n",
      "[08d06h25m17s][HUGECTR][INFO]: num_internal_buffers 1\n",
      "[08d06h25m17s][HUGECTR][INFO]: num_internal_buffers 1\n",
      "[08d06h25m17s][HUGECTR][INFO]: max_vocabulary_size_per_gpu_=1447751\n",
      "===================================================Model Compile===================================================\n",
      "[08d06h25m29s][HUGECTR][INFO]: gpu0 start to init embedding\n",
      "[08d06h25m29s][HUGECTR][INFO]: gpu0 init embedding done\n",
      "===================================================Model Summary===================================================\n",
      "Label                                   Dense                         Sparse                        \n",
      "label                                   dense                          data1                         \n",
      "(None, 1)                               (None, 13)                              \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Layer Type                              Input Name                    Output Name                   Output Shape                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "DistributedSlotSparseEmbeddingHash      data1                         sparse_embedding1             (None, 26, 16)                \n",
      "Reshape                                 sparse_embedding1             reshape1                      (None, 416)                   \n",
      "Concat                                  reshape1,dense                concat1                       (None, 429)                   \n",
      "Slice                                   concat1                       slice11,slice12                                             \n",
      "MultiCross                              slice11                       multicross1                   (None, 429)                   \n",
      "InnerProduct                            slice12                       fc1                           (None, 1024)                  \n",
      "ReLU                                    fc1                           relu1                         (None, 1024)                  \n",
      "Dropout                                 relu1                         dropout1                      (None, 1024)                  \n",
      "InnerProduct                            dropout1                      fc2                           (None, 1024)                  \n",
      "ReLU                                    fc2                           relu2                         (None, 1024)                  \n",
      "Dropout                                 relu2                         dropout2                      (None, 1024)                  \n",
      "Concat                                  dropout2,multicross1          concat2                       (None, 1453)                  \n",
      "InnerProduct                            concat2                       fc3                           (None, 1)                     \n",
      "BinaryCrossEntropyLoss                  fc3,label                     loss                                                        \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "[80d60h25m29s][HUGECTR][INFO]: Save the model graph to dcn.json, successful\n",
      "=====================================================Model Fit=====================================================\n",
      "[80d60h25m29s][HUGECTR][INFO]: Use non-epoch mode with number of iterations: 12000\n",
      "[80d60h25m29s][HUGECTR][INFO]: Training batchsize: 4096, evaluation batchsize: 4096\n",
      "[80d60h25m29s][HUGECTR][INFO]: Evaluation interval: 1000, snapshot interval: 10000\n",
      "[80d60h25m29s][HUGECTR][INFO]: Sparse embedding trainable: 1, dense network trainable: 1\n",
      "[80d60h25m29s][HUGECTR][INFO]: Use mixed precision: 0, scaler: 1.000000, use cuda graph: 1\n",
      "[80d60h25m29s][HUGECTR][INFO]: lr: 0.001000, warmup_steps: 1, decay_start: 0, decay_steps: 1, decay_power: 2.000000, end_lr: 0.000000\n",
      "[80d60h25m29s][HUGECTR][INFO]: Training source file: ./dcn_data/file_list.txt\n",
      "[80d60h25m29s][HUGECTR][INFO]: Evaluation source file: ./dcn_data/file_list_test.txt\n",
      "[80d60h25m33s][HUGECTR][INFO]: Iter: 500 Time(500 iters): 3.593163s Loss: 0.121543 lr:0.001000\n",
      "[80d60h25m36s][HUGECTR][INFO]: Iter: 1000 Time(500 iters): 3.590955s Loss: 0.130732 lr:0.001000\n",
      "[80d60h25m51s][HUGECTR][INFO]: Evaluation, AUC: 0.758551\n",
      "[80d60h25m51s][HUGECTR][INFO]: Eval Time for 1500 iters: 14.601934s\n",
      "[80d60h26m10s][HUGECTR][INFO]: Iter: 1500 Time(500 iters): 25.030089s Loss: 0.121245 lr:0.001000\n",
      "[80d60h26m90s][HUGECTR][INFO]: Iter: 2000 Time(500 iters): 7.417193s Loss: 0.128038 lr:0.001000\n",
      "[80d60h26m12s][HUGECTR][INFO]: Evaluation, AUC: 0.765862\n",
      "[80d60h26m12s][HUGECTR][INFO]: Eval Time for 1500 iters: 3.421184s\n",
      "[80d60h26m18s][HUGECTR][INFO]: Iter: 2500 Time(500 iters): 9.155218s Loss: 0.110623 lr:0.001000\n",
      "[80d60h26m24s][HUGECTR][INFO]: Iter: 3000 Time(500 iters): 6.101669s Loss: 0.128624 lr:0.001000\n",
      "[80d60h26m27s][HUGECTR][INFO]: Evaluation, AUC: 0.770762\n",
      "[80d60h26m27s][HUGECTR][INFO]: Eval Time for 1500 iters: 3.399687s\n",
      "[80d60h26m33s][HUGECTR][INFO]: Iter: 3500 Time(500 iters): 9.432641s Loss: 0.136130 lr:0.001000\n",
      "[80d60h26m39s][HUGECTR][INFO]: Iter: 4000 Time(500 iters): 5.928421s Loss: 0.127346 lr:0.001000\n",
      "[80d60h26m43s][HUGECTR][INFO]: Evaluation, AUC: 0.772694\n",
      "[80d60h26m43s][HUGECTR][INFO]: Eval Time for 1500 iters: 3.413713s\n",
      "[80d60h26m49s][HUGECTR][INFO]: Iter: 4500 Time(500 iters): 9.476760s Loss: 0.124997 lr:0.001000\n",
      "[80d60h26m55s][HUGECTR][INFO]: Iter: 5000 Time(500 iters): 5.896564s Loss: 0.103141 lr:0.001000\n",
      "[80d60h26m58s][HUGECTR][INFO]: Evaluation, AUC: 0.775047\n",
      "[80d60h26m58s][HUGECTR][INFO]: Eval Time for 1500 iters: 3.412804s\n",
      "[80d60h27m40s][HUGECTR][INFO]: Iter: 5500 Time(500 iters): 9.043147s Loss: 0.135033 lr:0.001000\n",
      "[80d60h27m10s][HUGECTR][INFO]: Iter: 6000 Time(500 iters): 6.124461s Loss: 0.137103 lr:0.001000\n",
      "[80d60h27m13s][HUGECTR][INFO]: Evaluation, AUC: 0.776392\n",
      "[80d60h27m13s][HUGECTR][INFO]: Eval Time for 1500 iters: 3.379163s\n",
      "[80d60h27m19s][HUGECTR][INFO]: Iter: 6500 Time(500 iters): 9.354628s Loss: 0.116106 lr:0.001000\n",
      "[80d60h27m25s][HUGECTR][INFO]: Iter: 7000 Time(500 iters): 6.092002s Loss: 0.142001 lr:0.001000\n",
      "[80d60h27m29s][HUGECTR][INFO]: Evaluation, AUC: 0.778978\n",
      "[80d60h27m29s][HUGECTR][INFO]: Eval Time for 1500 iters: 3.407731s\n",
      "[80d60h27m35s][HUGECTR][INFO]: Iter: 7500 Time(500 iters): 9.436527s Loss: 0.129961 lr:0.001000\n",
      "[80d60h27m41s][HUGECTR][INFO]: Iter: 8000 Time(500 iters): 6.077933s Loss: 0.133866 lr:0.001000\n",
      "[80d60h27m44s][HUGECTR][INFO]: Evaluation, AUC: 0.778490\n",
      "[80d60h27m44s][HUGECTR][INFO]: Eval Time for 1500 iters: 3.416607s\n",
      "[80d60h27m51s][HUGECTR][INFO]: Iter: 8500 Time(500 iters): 9.742521s Loss: 0.115134 lr:0.001000\n",
      "[80d60h27m56s][HUGECTR][INFO]: Iter: 9000 Time(500 iters): 5.353511s Loss: 0.140721 lr:0.001000\n",
      "[80d60h27m59s][HUGECTR][INFO]: Evaluation, AUC: 0.779438\n",
      "[80d60h27m59s][HUGECTR][INFO]: Eval Time for 1500 iters: 3.454514s\n",
      "[80d60h28m30s][HUGECTR][INFO]: Iter: 9500 Time(500 iters): 7.063022s Loss: 0.116547 lr:0.001000\n",
      "[80d60h28m70s][HUGECTR][INFO]: Iter: 10000 Time(500 iters): 3.606606s Loss: 0.144308 lr:0.001000\n",
      "[80d60h28m10s][HUGECTR][INFO]: Evaluation, AUC: 0.770993\n",
      "[80d60h28m10s][HUGECTR][INFO]: Eval Time for 1500 iters: 3.464358s\n",
      "[80d60h28m10s][HUGECTR][INFO]: Rank0: Dump hash table from GPU0\n",
      "[80d60h28m10s][HUGECTR][INFO]: Rank0: Write hash table <key,value> pairs to file\n",
      "[80d60h28m10s][HUGECTR][INFO]: Done\n",
      "[80d60h28m11s][HUGECTR][INFO]: Dumping sparse weights to files, successful\n",
      "[80d60h28m12s][HUGECTR][INFO]: Rank0: Write sparse optimzer state to file\n",
      "[80d60h28m12s][HUGECTR][INFO]: Done\n",
      "[80d60h28m12s][HUGECTR][INFO]: Rank0: Write sparse optimzer state to file\n",
      "[80d60h28m12s][HUGECTR][INFO]: Done\n",
      "[80d60h28m14s][HUGECTR][INFO]: Dumping sparse optimzer states to files, successful\n",
      "[80d60h28m14s][HUGECTR][INFO]: Dumping dense weights to file, successful\n",
      "[80d60h28m14s][HUGECTR][INFO]: Dumping dense optimizer states to file, successful\n",
      "[80d60h28m14s][HUGECTR][INFO]: Dumping untrainable weights to file, successful\n",
      "[80d60h28m18s][HUGECTR][INFO]: Iter: 10500 Time(500 iters): 11.338640s Loss: 0.111093 lr:0.001000\n",
      "[80d60h28m21s][HUGECTR][INFO]: Iter: 11000 Time(500 iters): 3.613000s Loss: 0.120038 lr:0.001000\n",
      "[80d60h28m25s][HUGECTR][INFO]: Evaluation, AUC: 0.767443\n",
      "[80d60h28m25s][HUGECTR][INFO]: Eval Time for 1500 iters: 3.396861s\n",
      "[80d60h28m28s][HUGECTR][INFO]: Iter: 11500 Time(500 iters): 7.017025s Loss: 0.121916 lr:0.001000\n"
     ]
    }
   ],
   "source": [
    "!python3 dcn_train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imposed-czech",
   "metadata": {},
   "source": [
    "<a id=\"23\"></a>\n",
    "### 2.3 Continue Training\n",
    "\n",
    "We can continue our training based on the saved model graph, model weights and optimizer states by doing the following with Python APIs:\n",
    "\n",
    "1. Create the solver, reader and optimizer, then initialize the model.\n",
    "2. Construct the model graph from the saved JSON file.\n",
    "3. Compile the model and have an overview of the model graph.\n",
    "4. Load the model weights and optimizer states.\n",
    "5. Fit the model, save the model weights and optimizer states implicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "supposed-intersection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dcn0_opt_sparse_10000.model  dcn_dense_10000.model\n",
      "dcn0_sparse_10000.model      dcn_opt_dense_10000.model\n"
     ]
    }
   ],
   "source": [
    "!ls *.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "verbal-representation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing dcn_continue.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile dcn_continue.py\n",
    "import hugectr\n",
    "from mpi4py import MPI\n",
    "solver = hugectr.CreateSolver(max_eval_batches = 1500,\n",
    "                              batchsize_eval = 4096,\n",
    "                              batchsize = 4096,\n",
    "                              i64_input_key = False,\n",
    "                              use_mixed_precision = False,\n",
    "                              repeat_dataset = True,\n",
    "                              use_cuda_graph = True)\n",
    "reader = hugectr.DataReaderParams(data_reader_type = hugectr.DataReaderType_t.Norm,\n",
    "                                  source = [\"./dcn_data/file_list.txt\"],\n",
    "                                  eval_source = \"./dcn_data/file_list_test.txt\",\n",
    "                                  check_type = hugectr.Check_t.Sum)\n",
    "optimizer = hugectr.CreateOptimizer(optimizer_type = hugectr.Optimizer_t.Adam)\n",
    "model = hugectr.Model(solver, reader, optimizer)\n",
    "model.construct_from_json(graph_config_file = \"dcn.json\", include_dense_network = True)\n",
    "model.compile()\n",
    "model.load_dense_weights(\"dcn_dense_10000.model\")\n",
    "model.load_sparse_weights([\"dcn0_sparse_10000.model\"])\n",
    "model.load_dense_optimizer_states(\"dcn_opt_dense_10000.model\")\n",
    "model.load_sparse_optimizer_states([\"dcn0_opt_sparse_10000.model\"])\n",
    "model.summary()\n",
    "model.fit(max_iter = 5000, display = 500, eval_interval = 1000, snapshot = 10000, snapshot_prefix = \"dcn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "floral-finland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================Model Init=====================================================\n",
      "[08d06h30m08s][HUGECTR][INFO]: Global seed is 1391592569\n",
      "[08d06h30m10s][HUGECTR][INFO]: Peer-to-peer access cannot be fully enabled.\n",
      "Device 0: Tesla V100-SXM2-16GB\n",
      "[08d06h30m10s][HUGECTR][INFO]: num of DataReader workers: 12\n",
      "[08d06h30m10s][HUGECTR][INFO]: num_internal_buffers 1\n",
      "[08d06h30m10s][HUGECTR][INFO]: num_internal_buffers 1\n",
      "[08d06h30m10s][HUGECTR][INFO]: max_vocabulary_size_per_gpu_=1447751\n",
      "[08d06h30m10s][HUGECTR][INFO]: Load the model graph from dcn.json, successful\n",
      "===================================================Model Compile===================================================\n",
      "[08d06h30m22s][HUGECTR][INFO]: gpu0 start to init embedding\n",
      "[08d06h30m22s][HUGECTR][INFO]: gpu0 init embedding done\n",
      "[08d06h30m22s][HUGECTR][INFO]: Loading dense model: dcn_dense_10000.model\n",
      "[08d06h30m22s][HUGECTR][INFO]: Loading sparse model: dcn0_sparse_10000.model\n",
      "[08d06h30m28s][HUGECTR][INFO]: Start to upload embedding table file to GPUs, total loop_num: 1447\n",
      "[08d06h30m29s][HUGECTR][INFO]: Done\n",
      "[08d06h30m29s][HUGECTR][INFO]: Loading dense opt states: dcn_opt_dense_10000.model\n",
      "[08d06h30m29s][HUGECTR][INFO]: Loading sparse optimizer states: dcn0_opt_sparse_10000.model\n",
      "[08d06h30m29s][HUGECTR][INFO]: Rank0: Read sparse optimzer state from file\n",
      "[08d06h30m34s][HUGECTR][INFO]: Done\n",
      "[08d06h30m34s][HUGECTR][INFO]: Rank0: Read sparse optimzer state from file\n",
      "[08d06h30m39s][HUGECTR][INFO]: Done\n",
      "===================================================Model Summary===================================================\n",
      "Label                                   Dense                         Sparse                        \n",
      "label                                   dense                          data1                         \n",
      "(None, 1)                               (None, 13)                              \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Layer Type                              Input Name                    Output Name                   Output Shape                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "DistributedSlotSparseEmbeddingHash      data1                         sparse_embedding1             (None, 26, 16)                \n",
      "Reshape                                 sparse_embedding1             reshape1                      (None, 416)                   \n",
      "Concat                                  reshape1,dense                concat1                       (None, 429)                   \n",
      "Slice                                   concat1                       slice11,slice12                                             \n",
      "MultiCross                              slice11                       multicross1                   (None, 429)                   \n",
      "InnerProduct                            slice12                       fc1                           (None, 1024)                  \n",
      "ReLU                                    fc1                           relu1                         (None, 1024)                  \n",
      "Dropout                                 relu1                         dropout1                      (None, 1024)                  \n",
      "InnerProduct                            dropout1                      fc2                           (None, 1024)                  \n",
      "ReLU                                    fc2                           relu2                         (None, 1024)                  \n",
      "Dropout                                 relu2                         dropout2                      (None, 1024)                  \n",
      "Concat                                  dropout2,multicross1          concat2                       (None, 1453)                  \n",
      "InnerProduct                            concat2                       fc3                           (None, 1)                     \n",
      "BinaryCrossEntropyLoss                  fc3,label                     loss                                                        \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "=====================================================Model Fit=====================================================\n",
      "[80d60h30m39s][HUGECTR][INFO]: Use non-epoch mode with number of iterations: 5000\n",
      "[80d60h30m39s][HUGECTR][INFO]: Training batchsize: 4096, evaluation batchsize: 4096\n",
      "[80d60h30m39s][HUGECTR][INFO]: Evaluation interval: 1000, snapshot interval: 10000\n",
      "[80d60h30m39s][HUGECTR][INFO]: Sparse embedding trainable: 1, dense network trainable: 1\n",
      "[80d60h30m39s][HUGECTR][INFO]: Use mixed precision: 0, scaler: 1.000000, use cuda graph: 1\n",
      "[80d60h30m39s][HUGECTR][INFO]: lr: 0.001000, warmup_steps: 1, decay_start: 0, decay_steps: 1, decay_power: 2.000000, end_lr: 0.000000\n",
      "[80d60h30m39s][HUGECTR][INFO]: Training source file: ./dcn_data/file_list.txt\n",
      "[80d60h30m39s][HUGECTR][INFO]: Evaluation source file: ./dcn_data/file_list_test.txt\n",
      "[80d60h30m43s][HUGECTR][INFO]: Iter: 500 Time(500 iters): 3.638434s Loss: 0.104756 lr:0.001000\n",
      "[80d60h30m46s][HUGECTR][INFO]: Iter: 1000 Time(500 iters): 3.619084s Loss: 0.124854 lr:0.001000\n",
      "[80d60h30m50s][HUGECTR][INFO]: Evaluation, AUC: 0.744351\n",
      "[80d60h30m50s][HUGECTR][INFO]: Eval Time for 1500 iters: 3.444402s\n",
      "[80d60h30m53s][HUGECTR][INFO]: Iter: 1500 Time(500 iters): 7.094341s Loss: 0.116637 lr:0.001000\n",
      "[80d60h30m57s][HUGECTR][INFO]: Iter: 2000 Time(500 iters): 3.640979s Loss: 0.123174 lr:0.001000\n",
      "[80d60h31m00s][HUGECTR][INFO]: Evaluation, AUC: 0.769421\n",
      "[80d60h31m00s][HUGECTR][INFO]: Eval Time for 1500 iters: 3.429878s\n",
      "[80d60h31m40s][HUGECTR][INFO]: Iter: 2500 Time(500 iters): 7.073610s Loss: 0.106462 lr:0.001000\n",
      "[80d60h31m80s][HUGECTR][INFO]: Iter: 3000 Time(500 iters): 3.640448s Loss: 0.122027 lr:0.001000\n",
      "[80d60h31m11s][HUGECTR][INFO]: Evaluation, AUC: 0.771562\n",
      "[80d60h31m11s][HUGECTR][INFO]: Eval Time for 1500 iters: 3.440140s\n",
      "[80d60h31m15s][HUGECTR][INFO]: Iter: 3500 Time(500 iters): 7.080441s Loss: 0.131565 lr:0.001000\n",
      "[80d60h31m18s][HUGECTR][INFO]: Iter: 4000 Time(500 iters): 3.637742s Loss: 0.121967 lr:0.001000\n",
      "[80d60h31m22s][HUGECTR][INFO]: Evaluation, AUC: 0.771255\n",
      "[80d60h31m22s][HUGECTR][INFO]: Eval Time for 1500 iters: 3.463637s\n",
      "[80d60h31m26s][HUGECTR][INFO]: Iter: 4500 Time(500 iters): 7.107942s Loss: 0.121558 lr:0.001000\n"
     ]
    }
   ],
   "source": [
    "!python3 dcn_continue.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fourth-stewart",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id=\"24\"></a>\n",
    "### 2.4 Inference\n",
    "\n",
    "The HugeCTR inference is enabled by `hugectr.inference.InferenceSession.predict` method of InferenceSession, which requires dense features, embedding columns and row pointers of slots as the input and gives the prediction result as the output. We need to convert the criteo data to inference format first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "unsigned-galaxy",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 ../tools/criteo_predict/criteo2predict.py --src_csv_path=dcn_data/val/test.txt --src_config=../tools/criteo_predict/dcn_data.json --dst_path=./dcn_csr.txt --batch_size=1024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italic-domain",
   "metadata": {},
   "source": [
    "We can then make inference based on the saved model graph and model weights by doing the following with Python APIs:\n",
    "\n",
    "1. Configure the inference related parameters.\n",
    "2. Create the inference session.\n",
    "3. Make inference with the `InferenceSession.predict` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "blond-joseph",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing dcn_inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile dcn_inference.py\n",
    "from hugectr.inference import InferenceParams, CreateInferenceSession\n",
    "from mpi4py import MPI\n",
    "\n",
    "def calculate_accuracy(labels, output):\n",
    "  num_samples = len(labels)\n",
    "  flags = [1 if ((labels[i] == 0 and output[i] <= 0.5) or (labels[i] == 1 and output[i] > 0.5)) else 0 for i in range(num_samples)]\n",
    "  correct_samples = sum(flags)\n",
    "  return float(correct_samples)/(float(num_samples)+1e-16)\n",
    "\n",
    "data_file = open(\"dcn_csr.txt\")\n",
    "config_file = \"dcn.json\"\n",
    "labels = [int(item) for item in data_file.readline().split(' ')]\n",
    "dense_features = [float(item) for item in data_file.readline().split(' ') if item!=\"\\n\"]\n",
    "embedding_columns = [int(item) for item in data_file.readline().split(' ')]\n",
    "row_ptrs = [int(item) for item in data_file.readline().split(' ')]\n",
    "\n",
    "# create parameter server, embedding cache and inference session\n",
    "inference_params = InferenceParams(model_name = \"dcn\",\n",
    "                                max_batchsize = 1024,\n",
    "                                hit_rate_threshold = 0.6,\n",
    "                                dense_model_file = \"./dcn_dense_10000.model\",\n",
    "                                sparse_model_files = [\"./dcn0_sparse_10000.model\"],\n",
    "                                device_id = 0,\n",
    "                                use_gpu_embedding_cache = True,\n",
    "                                cache_size_percentage = 0.9,\n",
    "                                i64_input_key = False,\n",
    "                                use_mixed_precision = False)\n",
    "inference_session = CreateInferenceSession(config_file, inference_params)\n",
    "output = inference_session.predict(dense_features, embedding_columns, row_ptrs)\n",
    "accuracy = calculate_accuracy(labels, output)\n",
    "print(\"[HUGECTR][INFO] number samples: {}, accuracy: {}\".format(len(labels), accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "authorized-shooting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08d06h32m29s][HUGECTR][INFO]: default_emb_vec_value is not specified using default: 0.000000\n",
      "[08d06h32m32s][HUGECTR][INFO]: Global seed is 2667697857\n",
      "[08d06h32m33s][HUGECTR][INFO]: Peer-to-peer access cannot be fully enabled.\n",
      "[08d06h32m33s][HUGECTR][INFO]: start create embedding for inference\n",
      "[08d06h32m33s][HUGECTR][INFO]: sparse_input name data1\n",
      "[08d06h32m33s][HUGECTR][INFO]: create embedding for inference success\n",
      "[08d06h32m33s][HUGECTR][INFO]: Inference stage skip BinaryCrossEntropyLoss layer, replaced by Sigmoid layer\n",
      "[HUGECTR][INFO] number samples: 1024, accuracy: 0.9736328125\n"
     ]
    }
   ],
   "source": [
    "!python3 dcn_inference.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "first-frederick",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "## 3. Wide&Deep Demo\n",
    "\n",
    "<a id=\"31\"></a>\n",
    "### 3.1 Download and Preprocess Data\n",
    "1. Download the Kaggle Criteo dataset using the following command:\n",
    "   ```shell\n",
    "   $ cd ${project_root}/tools\n",
    "   $ wget http://azuremlsampleexperiments.blob.core.windows.net/criteo/day_1.gz\n",
    "   ```\n",
    "   \n",
    "   In preprocessing, we will further reduce the amounts of data to speedup the preprocessing, fill missing values, remove the feature values whose occurrences are very rare, etc. Here we choose pandas preprocessing method to make the dataset ready for HugeCTR training.\n",
    "\n",
    "2. Preprocessing by Pandas using the following command:\n",
    "   ```shell\n",
    "   $ bash preprocess.sh 1 wdl_data pandas 1 1 100\n",
    "   ```\n",
    "   \n",
    "   The first argument represents the dataset postfix. It is 1 here since day_1 is used. The second argument wdl_data is where the preprocessed data is stored. The fourth arguement (one after pandas) 1 embodies that the normalization is applied to dense features. The fifth argument 1 means that the feature crossing is applied. The last argument 100 means the number of data files in each file list.\n",
    "   \n",
    "3. Create a soft link to the dataset folder using the following command:\n",
    "   ```shell\n",
    "   $ ln ${project_root}/tools/wdl_data ${project_root}/notebooks/wdl_data\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smart-bottom",
   "metadata": {},
   "source": [
    "<a id=\"32\"></a>\n",
    "### 3.2 Model Oversubscription\n",
    "\n",
    "We can train fom scratch using model oversubscriber, dump the model graph to a JSON file, and save the trained dense weights and sparse embedding weights by doing the following with Python APIs:\n",
    "\n",
    "1. Create the solver, reader and optimizer, then initialize the model.\n",
    "2. Construct the model graph by adding input, sparse embedding and dense layers in order.\n",
    "3. Compile the model and have an overview of the model graph.\n",
    "4. Dump the model graph to the JSON file.\n",
    "5. Fit the model, save the model weights and optimizer states implicitly.\n",
    "\n",
    "To use model oversubscription, we need to create a write-enabled folder to store the temporary embedding files. We should specify `repeat_dataset` as False, `use_model_oversubscriber` as True and `temp_embedding_dir` as the aforementioned folder. The data in each file list will be trained for one epoch under this mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "signed-fetish",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir mos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "permanent-google",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wdl_train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wdl_train.py\n",
    "import hugectr\n",
    "from mpi4py import MPI\n",
    "solver = hugectr.CreateSolver(max_eval_batches = 5000,\n",
    "                              batchsize_eval = 1024,\n",
    "                              batchsize = 1024,\n",
    "                              vvgpu = [[0]],\n",
    "                              i64_input_key = False,\n",
    "                              use_mixed_precision = False,\n",
    "                              repeat_dataset = False,\n",
    "                              use_cuda_graph = True,\n",
    "                              use_model_oversubscriber = True,\n",
    "                              temp_embedding_dir = \"mos\")\n",
    "reader = hugectr.DataReaderParams(data_reader_type = hugectr.DataReaderType_t.Norm,\n",
    "                          source = [\"wdl_data/file_list.\"+str(i)+\".txt\" for i in range(2)],\n",
    "                          keyset = [\"wdl_data/file_list.\"+str(i)+\".keyset\" for i in range(2)],\n",
    "                          eval_source = \"wdl_data/file_list.2.txt\",\n",
    "                          check_type = hugectr.Check_t.Sum)\n",
    "optimizer = hugectr.CreateOptimizer(optimizer_type = hugectr.Optimizer_t.Adam)\n",
    "model = hugectr.Model(solver, reader, optimizer)\n",
    "model.add(hugectr.Input(label_dim = 1, label_name = \"label\",\n",
    "                        dense_dim = 13, dense_name = \"dense\",\n",
    "                        data_reader_sparse_param_array = \n",
    "                        [hugectr.DataReaderSparseParam(hugectr.DataReaderSparse_t.Distributed, 30, 2, 1),\n",
    "                        hugectr.DataReaderSparseParam(hugectr.DataReaderSparse_t.Distributed, 30, 1, 26)],\n",
    "                        sparse_names = [\"wide_data\", \"deep_data\"]))\n",
    "model.add(hugectr.SparseEmbedding(embedding_type = hugectr.Embedding_t.DistributedSlotSparseEmbeddingHash, \n",
    "                            max_vocabulary_size_per_gpu = 5863985,\n",
    "                            embedding_vec_size = 1,\n",
    "                            combiner = 0,\n",
    "                            sparse_embedding_name = \"sparse_embedding2\",\n",
    "                            bottom_name = \"wide_data\",\n",
    "                            optimizer = optimizer))\n",
    "model.add(hugectr.SparseEmbedding(embedding_type = hugectr.Embedding_t.DistributedSlotSparseEmbeddingHash, \n",
    "                            max_vocabulary_size_per_gpu = 5863985,\n",
    "                            embedding_vec_size = 16,\n",
    "                            combiner = 0,\n",
    "                            sparse_embedding_name = \"sparse_embedding1\",\n",
    "                            bottom_name = \"deep_data\",\n",
    "                            optimizer = optimizer))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Reshape,\n",
    "                            bottom_names = [\"sparse_embedding1\"],\n",
    "                            top_names = [\"reshape1\"],\n",
    "                            leading_dim=416))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Reshape,\n",
    "                            bottom_names = [\"sparse_embedding2\"],\n",
    "                            top_names = [\"reshape2\"],\n",
    "                            leading_dim=1))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Concat,\n",
    "                            bottom_names = [\"reshape1\", \"dense\"], top_names = [\"concat1\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"concat1\"],\n",
    "                            top_names = [\"fc1\"],\n",
    "                            num_output=1024))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc1\"],\n",
    "                            top_names = [\"relu1\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Dropout,\n",
    "                            bottom_names = [\"relu1\"],\n",
    "                            top_names = [\"dropout1\"],\n",
    "                            dropout_rate=0.5))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"dropout1\"],\n",
    "                            top_names = [\"fc2\"],\n",
    "                            num_output=1024))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc2\"],\n",
    "                            top_names = [\"relu2\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Dropout,\n",
    "                            bottom_names = [\"relu2\"],\n",
    "                            top_names = [\"dropout2\"],\n",
    "                            dropout_rate=0.5))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"dropout2\"],\n",
    "                            top_names = [\"fc3\"],\n",
    "                            num_output=1))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Add,\n",
    "                            bottom_names = [\"fc3\", \"reshape2\"],\n",
    "                            top_names = [\"add1\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.BinaryCrossEntropyLoss,\n",
    "                            bottom_names = [\"add1\", \"label\"],\n",
    "                            top_names = [\"loss\"]))\n",
    "model.compile()\n",
    "model.summary()\n",
    "model.graph_to_json(graph_config_file = \"wdl.json\")\n",
    "model.fit(num_epochs = 2, display = 500, eval_interval = 1000, snapshot = 10000, snapshot_prefix = \"wdl\")\n",
    "model.set_source(source = [\"wdl_data/file_list.3.txt\", \"wdl_data/file_list.4.txt\"], keyset = [\"wdl_data/file_list.3.keyset\", \"wdl_data/file_list.4.keyset\"], eval_source = \"wdl_data/file_list.5.txt\")\n",
    "model.fit(num_epochs = 2, display = 500, eval_interval = 1000, snapshot = 10000, snapshot_prefix = \"wdl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "divided-damages",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================Model Init=====================================================\n",
      "[08d06h36m09s][HUGECTR][INFO]: Global seed is 227787138\n",
      "[08d06h36m10s][HUGECTR][INFO]: Peer-to-peer access cannot be fully enabled.\n",
      "Device 0: Tesla V100-SXM2-16GB\n",
      "[08d06h36m10s][HUGECTR][INFO]: num of DataReader workers: 12\n",
      "[08d06h36m10s][HUGECTR][INFO]: num_internal_buffers 1\n",
      "[08d06h36m10s][HUGECTR][INFO]: num_internal_buffers 1\n",
      "[08d06h36m10s][HUGECTR][INFO]: max_vocabulary_size_per_gpu_=5863985\n",
      "[08d06h36m10s][HUGECTR][INFO]: max_vocabulary_size_per_gpu_=5863985\n",
      "===================================================Model Compile===================================================\n",
      "[08d06h36m14s][HUGECTR][INFO]: gpu0 start to init embedding\n",
      "[08d06h36m14s][HUGECTR][INFO]: gpu0 init embedding done\n",
      "[08d06h36m14s][HUGECTR][INFO]: gpu0 start to init embedding\n",
      "[08d06h36m14s][HUGECTR][INFO]: gpu0 init embedding done\n",
      "[08d06h36m14s][HUGECTR][INFO]: Traning from scratch, no snapshot file specified\n",
      "===================================================Model Summary===================================================\n",
      "Label                                   Dense                         Sparse                        \n",
      "label                                   dense                          wide_data,deep_data           \n",
      "(None, 1)                               (None, 13)                              \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Layer Type                              Input Name                    Output Name                   Output Shape                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "DistributedSlotSparseEmbeddingHash      wide_data                     sparse_embedding2             (None, 1, 1)                  \n",
      "DistributedSlotSparseEmbeddingHash      deep_data                     sparse_embedding1             (None, 26, 16)                \n",
      "Reshape                                 sparse_embedding1             reshape1                      (None, 416)                   \n",
      "Reshape                                 sparse_embedding2             reshape2                      (None, 1)                     \n",
      "Concat                                  reshape1,dense                concat1                       (None, 429)                   \n",
      "InnerProduct                            concat1                       fc1                           (None, 1024)                  \n",
      "ReLU                                    fc1                           relu1                         (None, 1024)                  \n",
      "Dropout                                 relu1                         dropout1                      (None, 1024)                  \n",
      "InnerProduct                            dropout1                      fc2                           (None, 1024)                  \n",
      "ReLU                                    fc2                           relu2                         (None, 1024)                  \n",
      "Dropout                                 relu2                         dropout2                      (None, 1024)                  \n",
      "InnerProduct                            dropout2                      fc3                           (None, 1)                     \n",
      "Add                                     fc3,reshape2                  add1                          (None, 1)                     \n",
      "BinaryCrossEntropyLoss                  add1,label                    loss                                                        \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "[80d60h36m14s][HUGECTR][INFO]: Save the model graph to wdl.json, successful\n",
      "=====================================================Model Fit=====================================================\n",
      "[80d60h36m14s][HUGECTR][INFO]: Use model oversubscriber mode with number of training sources: 2, number of epochs: 2\n",
      "[80d60h36m14s][HUGECTR][INFO]: Training batchsize: 1024, evaluation batchsize: 1024\n",
      "[80d60h36m14s][HUGECTR][INFO]: Evaluation interval: 1000, snapshot interval: 10000\n",
      "[80d60h36m14s][HUGECTR][INFO]: Sparse embedding trainable: 1, dense network trainable: 1\n",
      "[80d60h36m14s][HUGECTR][INFO]: Use mixed precision: 0, scaler: 1.000000, use cuda graph: 1\n",
      "[80d60h36m14s][HUGECTR][INFO]: lr: 0.001000, warmup_steps: 1, decay_start: 0, decay_steps: 1, decay_power: 2.000000, end_lr: 0.000000\n",
      "[80d60h36m14s][HUGECTR][INFO]: Evaluation source file: wdl_data/file_list.2.txt\n",
      "[80d60h36m14s][HUGECTR][INFO]: --------------------Epoch 0, source file: wdl_data/file_list.0.txt--------------------\n",
      "[80d60h36m14s][HUGECTR][INFO]: Write hash table <key,value> pairs to file\n",
      "[80d60h36m14s][HUGECTR][INFO]: Write hash table <key,value> pairs to file\n",
      "[80d60h36m14s][HUGECTR][INFO]: Start to upload embedding table file to GPUs, total loop_num: 0\n",
      "[80d60h36m14s][HUGECTR][INFO]: Done\n",
      "[80d60h36m14s][HUGECTR][INFO]: Start to upload embedding table file to GPUs, total loop_num: 0\n",
      "[80d60h36m14s][HUGECTR][INFO]: Done\n",
      "[80d60h36m17s][HUGECTR][INFO]: Iter: 500 Time(500 iters): 3.263823s Loss: 0.142772 lr:0.001000\n",
      "[80d60h36m21s][HUGECTR][INFO]: Iter: 1000 Time(500 iters): 3.189304s Loss: 0.144630 lr:0.001000\n",
      "[80d60h36m25s][HUGECTR][INFO]: Evaluation, AUC: 0.738136\n",
      "[80d60h36m25s][HUGECTR][INFO]: Eval Time for 5000 iters: 4.617833s\n",
      "[80d60h36m28s][HUGECTR][INFO]: Iter: 1500 Time(500 iters): 7.857175s Loss: 0.149504 lr:0.001000\n",
      "[80d60h36m32s][HUGECTR][INFO]: Iter: 2000 Time(500 iters): 3.232913s Loss: 0.139425 lr:0.001000\n",
      "[80d60h36m36s][HUGECTR][INFO]: Evaluation, AUC: 0.747948\n",
      "[80d60h36m36s][HUGECTR][INFO]: Eval Time for 5000 iters: 4.415809s\n",
      "[80d60h36m39s][HUGECTR][INFO]: Iter: 2500 Time(500 iters): 7.645118s Loss: 0.159160 lr:0.001000\n",
      "[80d60h36m42s][HUGECTR][INFO]: Iter: 3000 Time(500 iters): 3.231524s Loss: 0.163397 lr:0.001000\n",
      "[80d60h36m47s][HUGECTR][INFO]: Evaluation, AUC: 0.752519\n",
      "[80d60h36m47s][HUGECTR][INFO]: Eval Time for 5000 iters: 4.478072s\n",
      "[80d60h36m50s][HUGECTR][INFO]: Iter: 3500 Time(500 iters): 7.706135s Loss: 0.134341 lr:0.001000\n",
      "[80d60h36m53s][HUGECTR][INFO]: Iter: 4000 Time(500 iters): 3.224341s Loss: 0.143769 lr:0.001000\n",
      "[80d60h36m58s][HUGECTR][INFO]: Evaluation, AUC: 0.755237\n",
      "[80d60h36m58s][HUGECTR][INFO]: Eval Time for 5000 iters: 4.623776s\n",
      "[80d60h36m58s][HUGECTR][INFO]: --------------------Epoch 0, source file: wdl_data/file_list.1.txt--------------------\n",
      "[80d60h36m58s][HUGECTR][INFO]: Dump hash table from GPU0\n",
      "[80d60h36m58s][HUGECTR][INFO]: Write hash table <key,value> pairs to file\n",
      "[80d60h36m58s][HUGECTR][INFO]: Dump hash table from GPU0\n",
      "[80d60h36m58s][HUGECTR][INFO]: Write hash table <key,value> pairs to file\n",
      "[80d60h37m00s][HUGECTR][INFO]: Start to upload embedding table file to GPUs, total loop_num: 442\n",
      "[80d60h37m00s][HUGECTR][INFO]: Done\n",
      "[80d60h37m10s][HUGECTR][INFO]: Start to upload embedding table file to GPUs, total loop_num: 821\n",
      "[80d60h37m10s][HUGECTR][INFO]: Done\n",
      "[80d60h37m40s][HUGECTR][INFO]: Iter: 4500 Time(500 iters): 10.820695s Loss: 0.152331 lr:0.001000\n",
      "[80d60h37m70s][HUGECTR][INFO]: Iter: 5000 Time(500 iters): 3.235542s Loss: 0.149925 lr:0.001000\n",
      "[80d60h37m12s][HUGECTR][INFO]: Evaluation, AUC: 0.750465\n",
      "[80d60h37m12s][HUGECTR][INFO]: Eval Time for 5000 iters: 4.456618s\n",
      "[80d60h37m15s][HUGECTR][INFO]: Iter: 5500 Time(500 iters): 7.689976s Loss: 0.151203 lr:0.001000\n",
      "[80d60h37m18s][HUGECTR][INFO]: Iter: 6000 Time(500 iters): 3.233111s Loss: 0.126599 lr:0.001000\n",
      "[80d60h37m23s][HUGECTR][INFO]: Evaluation, AUC: 0.753193\n",
      "[80d60h37m23s][HUGECTR][INFO]: Eval Time for 5000 iters: 4.488738s\n",
      "[80d60h37m26s][HUGECTR][INFO]: Iter: 6500 Time(500 iters): 7.719669s Loss: 0.132043 lr:0.001000\n",
      "[80d60h37m29s][HUGECTR][INFO]: Iter: 7000 Time(500 iters): 3.232309s Loss: 0.134156 lr:0.001000\n",
      "[80d60h37m34s][HUGECTR][INFO]: Evaluation, AUC: 0.757133\n",
      "[80d60h37m34s][HUGECTR][INFO]: Eval Time for 5000 iters: 4.596542s\n",
      "[80d60h37m37s][HUGECTR][INFO]: Iter: 7500 Time(500 iters): 7.827192s Loss: 0.113430 lr:0.001000\n",
      "[80d60h37m40s][HUGECTR][INFO]: Iter: 8000 Time(500 iters): 3.233058s Loss: 0.122702 lr:0.001000\n",
      "[80d60h37m45s][HUGECTR][INFO]: Evaluation, AUC: 0.758585\n",
      "[80d60h37m45s][HUGECTR][INFO]: Eval Time for 5000 iters: 4.674334s\n",
      "[80d60h37m45s][HUGECTR][INFO]: --------------------Epoch 1, source file: wdl_data/file_list.0.txt--------------------\n",
      "[80d60h37m45s][HUGECTR][INFO]: Dump hash table from GPU0\n",
      "[80d60h37m45s][HUGECTR][INFO]: Write hash table <key,value> pairs to file\n",
      "[80d60h37m46s][HUGECTR][INFO]: Dump hash table from GPU0\n",
      "[80d60h37m46s][HUGECTR][INFO]: Write hash table <key,value> pairs to file\n",
      "[80d60h37m50s][HUGECTR][INFO]: Start to upload embedding table file to GPUs, total loop_num: 1006\n",
      "[80d60h37m50s][HUGECTR][INFO]: Done\n",
      "[80d60h37m52s][HUGECTR][INFO]: Start to upload embedding table file to GPUs, total loop_num: 1054\n",
      "[80d60h37m52s][HUGECTR][INFO]: Done\n",
      "[80d60h37m55s][HUGECTR][INFO]: Iter: 8500 Time(500 iters): 15.009117s Loss: 0.135645 lr:0.001000\n",
      "[80d60h37m59s][HUGECTR][INFO]: Iter: 9000 Time(500 iters): 3.237817s Loss: 0.136890 lr:0.001000\n",
      "[80d60h38m40s][HUGECTR][INFO]: Evaluation, AUC: 0.739336\n",
      "[80d60h38m40s][HUGECTR][INFO]: Eval Time for 5000 iters: 4.878916s\n",
      "[80d60h38m70s][HUGECTR][INFO]: Iter: 9500 Time(500 iters): 8.111565s Loss: 0.132651 lr:0.001000\n",
      "[80d60h38m10s][HUGECTR][INFO]: Iter: 10000 Time(500 iters): 3.232818s Loss: 0.136931 lr:0.001000\n",
      "[80d60h38m15s][HUGECTR][INFO]: Evaluation, AUC: 0.739550\n",
      "[80d60h38m15s][HUGECTR][INFO]: Eval Time for 5000 iters: 4.621998s\n",
      "[80d60h38m15s][HUGECTR][INFO]: Rank0: Dump hash table from GPU0\n",
      "[80d60h38m15s][HUGECTR][INFO]: Rank0: Write hash table <key,value> pairs to file\n",
      "[80d60h38m15s][HUGECTR][INFO]: Done\n",
      "[80d60h38m15s][HUGECTR][INFO]: Rank0: Dump hash table from GPU0\n",
      "[80d60h38m15s][HUGECTR][INFO]: Rank0: Write hash table <key,value> pairs to file\n",
      "[80d60h38m15s][HUGECTR][INFO]: Done\n",
      "[80d60h38m16s][HUGECTR][INFO]: Dumping sparse weights to files, successful\n",
      "[80d60h38m16s][HUGECTR][INFO]: Rank0: Write sparse optimzer state to file\n",
      "[80d60h38m16s][HUGECTR][INFO]: Done\n",
      "[80d60h38m16s][HUGECTR][INFO]: Rank0: Write sparse optimzer state to file\n",
      "[80d60h38m16s][HUGECTR][INFO]: Done\n",
      "[80d60h38m17s][HUGECTR][INFO]: Rank0: Write sparse optimzer state to file\n",
      "[80d60h38m17s][HUGECTR][INFO]: Done\n",
      "[80d60h38m18s][HUGECTR][INFO]: Rank0: Write sparse optimzer state to file\n",
      "[80d60h38m18s][HUGECTR][INFO]: Done\n",
      "[80d60h38m34s][HUGECTR][INFO]: Dumping sparse optimzer states to files, successful\n",
      "[80d60h38m34s][HUGECTR][INFO]: Dumping dense weights to file, successful\n",
      "[80d60h38m34s][HUGECTR][INFO]: Dumping dense optimizer states to file, successful\n",
      "[80d60h38m34s][HUGECTR][INFO]: Dumping untrainable weights to file, successful\n",
      "[80d60h38m38s][HUGECTR][INFO]: Iter: 10500 Time(500 iters): 28.018333s Loss: 0.134930 lr:0.001000\n",
      "[80d60h38m41s][HUGECTR][INFO]: Iter: 11000 Time(500 iters): 3.236565s Loss: 0.148249 lr:0.001000\n",
      "[80d60h38m46s][HUGECTR][INFO]: Evaluation, AUC: 0.744961\n",
      "[80d60h38m46s][HUGECTR][INFO]: Eval Time for 5000 iters: 4.669087s\n",
      "[80d60h38m49s][HUGECTR][INFO]: Iter: 11500 Time(500 iters): 7.903684s Loss: 0.126846 lr:0.001000\n",
      "[80d60h38m52s][HUGECTR][INFO]: Iter: 12000 Time(500 iters): 3.233886s Loss: 0.141789 lr:0.001000\n",
      "[80d60h38m57s][HUGECTR][INFO]: Evaluation, AUC: 0.743840\n",
      "[80d60h38m57s][HUGECTR][INFO]: Eval Time for 5000 iters: 4.853125s\n",
      "[80d60h38m57s][HUGECTR][INFO]: --------------------Epoch 1, source file: wdl_data/file_list.1.txt--------------------\n",
      "[80d60h38m57s][HUGECTR][INFO]: Dump hash table from GPU0\n",
      "[80d60h38m57s][HUGECTR][INFO]: Write hash table <key,value> pairs to file\n",
      "[80d60h38m58s][HUGECTR][INFO]: Dump hash table from GPU0\n",
      "[80d60h38m58s][HUGECTR][INFO]: Write hash table <key,value> pairs to file\n",
      "[80d60h39m20s][HUGECTR][INFO]: Start to upload embedding table file to GPUs, total loop_num: 1007\n",
      "[80d60h39m20s][HUGECTR][INFO]: Done\n",
      "[80d60h39m30s][HUGECTR][INFO]: Start to upload embedding table file to GPUs, total loop_num: 1055\n",
      "[80d60h39m40s][HUGECTR][INFO]: Done\n",
      "[80d60h39m70s][HUGECTR][INFO]: Iter: 12500 Time(500 iters): 14.559590s Loss: 0.112391 lr:0.001000\n",
      "[80d60h39m10s][HUGECTR][INFO]: Iter: 13000 Time(500 iters): 3.243553s Loss: 0.128188 lr:0.001000\n",
      "[80d60h39m15s][HUGECTR][INFO]: Evaluation, AUC: 0.741290\n",
      "[80d60h39m15s][HUGECTR][INFO]: Eval Time for 5000 iters: 4.849729s\n",
      "[80d60h39m18s][HUGECTR][INFO]: Iter: 13500 Time(500 iters): 8.083732s Loss: 0.106730 lr:0.001000\n",
      "[80d60h39m22s][HUGECTR][INFO]: Iter: 14000 Time(500 iters): 3.233873s Loss: 0.115739 lr:0.001000\n",
      "[80d60h39m28s][HUGECTR][INFO]: Evaluation, AUC: 0.742929\n",
      "[80d60h39m28s][HUGECTR][INFO]: Eval Time for 5000 iters: 6.088415s\n",
      "[80d60h39m31s][HUGECTR][INFO]: Iter: 14500 Time(500 iters): 9.320862s Loss: 0.154744 lr:0.001000\n",
      "[80d60h39m34s][HUGECTR][INFO]: Iter: 15000 Time(500 iters): 3.234887s Loss: 0.135078 lr:0.001000\n",
      "[80d60h39m39s][HUGECTR][INFO]: Evaluation, AUC: 0.748753\n",
      "[80d60h39m39s][HUGECTR][INFO]: Eval Time for 5000 iters: 4.909415s\n",
      "[80d60h39m42s][HUGECTR][INFO]: Iter: 15500 Time(500 iters): 8.142764s Loss: 0.121046 lr:0.001000\n",
      "[80d60h39m45s][HUGECTR][INFO]: Iter: 16000 Time(500 iters): 3.235204s Loss: 0.158545 lr:0.001000\n",
      "[80d60h39m50s][HUGECTR][INFO]: Evaluation, AUC: 0.748870\n",
      "[80d60h39m50s][HUGECTR][INFO]: Eval Time for 5000 iters: 4.670383s\n",
      "=====================================================Model Fit=====================================================\n",
      "[80d60h39m50s][HUGECTR][INFO]: Use model oversubscriber mode with number of training sources: 2, number of epochs: 2\n",
      "[80d60h39m50s][HUGECTR][INFO]: Training batchsize: 1024, evaluation batchsize: 1024\n",
      "[80d60h39m50s][HUGECTR][INFO]: Evaluation interval: 1000, snapshot interval: 10000\n",
      "[80d60h39m50s][HUGECTR][INFO]: Sparse embedding trainable: 1, dense network trainable: 1\n",
      "[80d60h39m50s][HUGECTR][INFO]: Use mixed precision: 0, scaler: 1.000000, use cuda graph: 1\n",
      "[80d60h39m50s][HUGECTR][INFO]: lr: 0.001000, warmup_steps: 1, decay_start: 0, decay_steps: 1, decay_power: 2.000000, end_lr: 0.000000\n",
      "[80d60h39m50s][HUGECTR][INFO]: Evaluation source file: wdl_data/file_list.5.txt\n",
      "[80d60h39m50s][HUGECTR][INFO]: --------------------Epoch 0, source file: wdl_data/file_list.3.txt--------------------\n",
      "[80d60h39m50s][HUGECTR][INFO]: Dump hash table from GPU0\n",
      "[80d60h39m50s][HUGECTR][INFO]: Write hash table <key,value> pairs to file\n",
      "[80d60h39m51s][HUGECTR][INFO]: Dump hash table from GPU0\n",
      "[80d60h39m51s][HUGECTR][INFO]: Write hash table <key,value> pairs to file\n",
      "[80d60h39m54s][HUGECTR][INFO]: Start to upload embedding table file to GPUs, total loop_num: 555\n",
      "[80d60h39m54s][HUGECTR][INFO]: Done\n",
      "[80d60h39m56s][HUGECTR][INFO]: Start to upload embedding table file to GPUs, total loop_num: 961\n",
      "[80d60h39m56s][HUGECTR][INFO]: Done\n",
      "[80d60h39m59s][HUGECTR][INFO]: Iter: 500 Time(500 iters): 8.864572s Loss: 0.157700 lr:0.001000\n",
      "[80d60h40m20s][HUGECTR][INFO]: Iter: 1000 Time(500 iters): 3.236148s Loss: 0.109987 lr:0.001000\n",
      "[80d60h40m70s][HUGECTR][INFO]: Evaluation, AUC: 0.752112\n",
      "[80d60h40m70s][HUGECTR][INFO]: Eval Time for 5000 iters: 4.528256s\n",
      "[80d60h40m10s][HUGECTR][INFO]: Iter: 1500 Time(500 iters): 7.761364s Loss: 0.131304 lr:0.001000\n",
      "[80d60h40m13s][HUGECTR][INFO]: Iter: 2000 Time(500 iters): 3.235546s Loss: 0.121781 lr:0.001000\n",
      "[80d60h40m18s][HUGECTR][INFO]: Evaluation, AUC: 0.753567\n",
      "[80d60h40m18s][HUGECTR][INFO]: Eval Time for 5000 iters: 4.504443s\n",
      "[80d60h40m21s][HUGECTR][INFO]: Iter: 2500 Time(500 iters): 7.735346s Loss: 0.162253 lr:0.001000\n",
      "[80d60h40m24s][HUGECTR][INFO]: Iter: 3000 Time(500 iters): 3.257824s Loss: 0.143525 lr:0.001000\n",
      "[80d60h40m29s][HUGECTR][INFO]: Evaluation, AUC: 0.757707\n",
      "[80d60h40m29s][HUGECTR][INFO]: Eval Time for 5000 iters: 4.601868s\n",
      "[80d60h40m32s][HUGECTR][INFO]: Iter: 3500 Time(500 iters): 8.095025s Loss: 0.106167 lr:0.001000\n",
      "[80d60h40m36s][HUGECTR][INFO]: Iter: 4000 Time(500 iters): 3.233858s Loss: 0.119234 lr:0.001000\n",
      "[80d60h40m40s][HUGECTR][INFO]: Evaluation, AUC: 0.755538\n",
      "[80d60h40m40s][HUGECTR][INFO]: Eval Time for 5000 iters: 4.594295s\n",
      "[80d60h40m40s][HUGECTR][INFO]: --------------------Epoch 0, source file: wdl_data/file_list.4.txt--------------------\n",
      "[80d60h40m40s][HUGECTR][INFO]: Dump hash table from GPU0\n",
      "[80d60h40m40s][HUGECTR][INFO]: Write hash table <key,value> pairs to file\n",
      "[80d60h40m41s][HUGECTR][INFO]: Dump hash table from GPU0\n",
      "[80d60h40m41s][HUGECTR][INFO]: Write hash table <key,value> pairs to file\n",
      "[80d60h40m44s][HUGECTR][INFO]: Start to upload embedding table file to GPUs, total loop_num: 619\n",
      "[80d60h40m44s][HUGECTR][INFO]: Done\n",
      "[80d60h40m45s][HUGECTR][INFO]: Start to upload embedding table file to GPUs, total loop_num: 1015\n",
      "[80d60h40m45s][HUGECTR][INFO]: Done\n",
      "[80d60h40m49s][HUGECTR][INFO]: Iter: 4500 Time(500 iters): 13.189117s Loss: 0.168372 lr:0.001000\n",
      "[80d60h40m56s][HUGECTR][INFO]: Iter: 5000 Time(500 iters): 7.087215s Loss: 0.122658 lr:0.001000\n",
      "[80d60h41m10s][HUGECTR][INFO]: Evaluation, AUC: 0.755292\n",
      "[80d60h41m10s][HUGECTR][INFO]: Eval Time for 5000 iters: 4.997958s\n",
      "[80d60h41m10s][HUGECTR][INFO]: Iter: 5500 Time(500 iters): 14.245865s Loss: 0.123561 lr:0.001000\n",
      "[80d60h41m22s][HUGECTR][INFO]: Iter: 6000 Time(500 iters): 11.917457s Loss: 0.100292 lr:0.001000\n",
      "[80d60h41m27s][HUGECTR][INFO]: Evaluation, AUC: 0.756192\n",
      "[80d60h41m27s][HUGECTR][INFO]: Eval Time for 5000 iters: 5.070873s\n",
      "[80d60h41m33s][HUGECTR][INFO]: Iter: 6500 Time(500 iters): 10.689807s Loss: 0.095751 lr:0.001000\n",
      "[80d60h41m39s][HUGECTR][INFO]: Iter: 7000 Time(500 iters): 6.587650s Loss: 0.172640 lr:0.001000\n",
      "[80d60h41m44s][HUGECTR][INFO]: Evaluation, AUC: 0.757792\n",
      "[80d60h41m44s][HUGECTR][INFO]: Eval Time for 5000 iters: 4.525835s\n",
      "[80d60h41m51s][HUGECTR][INFO]: Iter: 7500 Time(500 iters): 12.125169s Loss: 0.112021 lr:0.001000\n",
      "[80d60h42m00s][HUGECTR][INFO]: Iter: 8000 Time(500 iters): 9.063389s Loss: 0.125007 lr:0.001000\n",
      "[80d60h42m50s][HUGECTR][INFO]: Evaluation, AUC: 0.756753\n",
      "[80d60h42m50s][HUGECTR][INFO]: Eval Time for 5000 iters: 4.706225s\n",
      "[80d60h42m50s][HUGECTR][INFO]: --------------------Epoch 1, source file: wdl_data/file_list.3.txt--------------------\n",
      "[80d60h42m50s][HUGECTR][INFO]: Dump hash table from GPU0\n",
      "[80d60h42m50s][HUGECTR][INFO]: Write hash table <key,value> pairs to file\n",
      "[80d60h42m70s][HUGECTR][INFO]: Dump hash table from GPU0\n",
      "[80d60h42m70s][HUGECTR][INFO]: Write hash table <key,value> pairs to file\n",
      "[80d60h42m12s][HUGECTR][INFO]: Start to upload embedding table file to GPUs, total loop_num: 1006\n",
      "[80d60h42m12s][HUGECTR][INFO]: Done\n",
      "[80d60h42m14s][HUGECTR][INFO]: Start to upload embedding table file to GPUs, total loop_num: 1054\n",
      "[80d60h42m14s][HUGECTR][INFO]: Done\n",
      "[80d60h42m17s][HUGECTR][INFO]: Iter: 8500 Time(500 iters): 16.533005s Loss: 0.135722 lr:0.001000\n",
      "[80d60h42m20s][HUGECTR][INFO]: Iter: 9000 Time(500 iters): 3.233654s Loss: 0.139661 lr:0.001000\n",
      "[80d60h42m25s][HUGECTR][INFO]: Evaluation, AUC: 0.754620\n",
      "[80d60h42m25s][HUGECTR][INFO]: Eval Time for 5000 iters: 4.729530s\n",
      "[80d60h42m28s][HUGECTR][INFO]: Iter: 9500 Time(500 iters): 7.961448s Loss: 0.139330 lr:0.001000\n",
      "[80d60h42m31s][HUGECTR][INFO]: Iter: 10000 Time(500 iters): 3.232744s Loss: 0.139592 lr:0.001000\n",
      "[80d60h42m36s][HUGECTR][INFO]: Evaluation, AUC: 0.750562\n",
      "[80d60h42m36s][HUGECTR][INFO]: Eval Time for 5000 iters: 4.775031s\n",
      "[80d60h42m36s][HUGECTR][INFO]: Rank0: Dump hash table from GPU0\n",
      "[80d60h42m36s][HUGECTR][INFO]: Rank0: Write hash table <key,value> pairs to file\n",
      "[80d60h42m36s][HUGECTR][INFO]: Done\n",
      "[80d60h42m37s][HUGECTR][INFO]: Rank0: Dump hash table from GPU0\n",
      "[80d60h42m37s][HUGECTR][INFO]: Rank0: Write hash table <key,value> pairs to file\n",
      "[80d60h42m37s][HUGECTR][INFO]: Done\n",
      "[80d60h42m40s][HUGECTR][INFO]: Dumping sparse weights to files, successful\n",
      "[80d60h42m40s][HUGECTR][INFO]: Rank0: Write sparse optimzer state to file\n",
      "[80d60h42m40s][HUGECTR][INFO]: Done\n",
      "[80d60h42m40s][HUGECTR][INFO]: Rank0: Write sparse optimzer state to file\n",
      "[80d60h42m40s][HUGECTR][INFO]: Done\n",
      "[80d60h42m42s][HUGECTR][INFO]: Rank0: Write sparse optimzer state to file\n",
      "[80d60h42m42s][HUGECTR][INFO]: Done\n",
      "[80d60h42m42s][HUGECTR][INFO]: Rank0: Write sparse optimzer state to file\n",
      "[80d60h42m42s][HUGECTR][INFO]: Done\n",
      "[80d60h43m10s][HUGECTR][INFO]: Dumping sparse optimzer states to files, successful\n",
      "[80d60h43m10s][HUGECTR][INFO]: Dumping dense weights to file, successful\n",
      "[80d60h43m10s][HUGECTR][INFO]: Dumping dense optimizer states to file, successful\n",
      "[80d60h43m10s][HUGECTR][INFO]: Dumping untrainable weights to file, successful\n",
      "[80d60h43m14s][HUGECTR][INFO]: Iter: 10500 Time(500 iters): 42.413877s Loss: 0.121902 lr:0.001000\n",
      "[80d60h43m17s][HUGECTR][INFO]: Iter: 11000 Time(500 iters): 3.236125s Loss: 0.116652 lr:0.001000\n",
      "[80d60h43m22s][HUGECTR][INFO]: Evaluation, AUC: 0.755103\n",
      "[80d60h43m22s][HUGECTR][INFO]: Eval Time for 5000 iters: 4.638808s\n",
      "[80d60h43m25s][HUGECTR][INFO]: Iter: 11500 Time(500 iters): 7.871220s Loss: 0.114863 lr:0.001000\n",
      "[80d60h43m28s][HUGECTR][INFO]: Iter: 12000 Time(500 iters): 3.236363s Loss: 0.150865 lr:0.001000\n",
      "[80d60h43m33s][HUGECTR][INFO]: Evaluation, AUC: 0.752881\n",
      "[80d60h43m33s][HUGECTR][INFO]: Eval Time for 5000 iters: 4.656602s\n",
      "[80d60h43m33s][HUGECTR][INFO]: --------------------Epoch 1, source file: wdl_data/file_list.4.txt--------------------\n",
      "[80d60h43m33s][HUGECTR][INFO]: Dump hash table from GPU0\n",
      "[80d60h43m33s][HUGECTR][INFO]: Write hash table <key,value> pairs to file\n",
      "[80d60h43m34s][HUGECTR][INFO]: Dump hash table from GPU0\n",
      "[80d60h43m34s][HUGECTR][INFO]: Write hash table <key,value> pairs to file\n",
      "[80d60h43m43s][HUGECTR][INFO]: Start to upload embedding table file to GPUs, total loop_num: 1007\n",
      "[80d60h43m43s][HUGECTR][INFO]: Done\n",
      "[80d60h43m44s][HUGECTR][INFO]: Start to upload embedding table file to GPUs, total loop_num: 1055\n",
      "[80d60h43m44s][HUGECTR][INFO]: Done\n",
      "[80d60h43m48s][HUGECTR][INFO]: Iter: 12500 Time(500 iters): 19.508059s Loss: 0.132507 lr:0.001000\n",
      "[80d60h43m51s][HUGECTR][INFO]: Iter: 13000 Time(500 iters): 3.234530s Loss: 0.125967 lr:0.001000\n",
      "[80d60h43m56s][HUGECTR][INFO]: Evaluation, AUC: 0.748452\n",
      "[80d60h43m56s][HUGECTR][INFO]: Eval Time for 5000 iters: 4.629869s\n",
      "[80d60h43m59s][HUGECTR][INFO]: Iter: 13500 Time(500 iters): 7.862711s Loss: 0.119613 lr:0.001000\n",
      "[80d60h44m20s][HUGECTR][INFO]: Iter: 14000 Time(500 iters): 3.233940s Loss: 0.126138 lr:0.001000\n",
      "[80d60h44m70s][HUGECTR][INFO]: Evaluation, AUC: 0.752581\n",
      "[80d60h44m70s][HUGECTR][INFO]: Eval Time for 5000 iters: 4.611344s\n",
      "[80d60h44m10s][HUGECTR][INFO]: Iter: 14500 Time(500 iters): 7.844346s Loss: 0.132942 lr:0.001000\n",
      "[80d60h44m13s][HUGECTR][INFO]: Iter: 15000 Time(500 iters): 3.233681s Loss: 0.133689 lr:0.001000\n",
      "[80d60h44m18s][HUGECTR][INFO]: Evaluation, AUC: 0.752274\n",
      "[80d60h44m18s][HUGECTR][INFO]: Eval Time for 5000 iters: 4.600851s\n",
      "[80d60h44m21s][HUGECTR][INFO]: Iter: 15500 Time(500 iters): 7.830382s Loss: 0.140141 lr:0.001000\n",
      "[80d60h44m24s][HUGECTR][INFO]: Iter: 16000 Time(500 iters): 3.230055s Loss: 0.127727 lr:0.001000\n",
      "[80d60h44m29s][HUGECTR][INFO]: Evaluation, AUC: 0.750045\n",
      "[80d60h44m29s][HUGECTR][INFO]: Eval Time for 5000 iters: 4.642095s\n"
     ]
    }
   ],
   "source": [
    "!python3 wdl_train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pacific-tobacco",
   "metadata": {},
   "source": [
    "<a id=\"33\"></a>\n",
    "### 3.3 Fine-tune\n",
    "\n",
    "We can only load the sparse embedding layers their corresponding weights, and then construct a new dense network. The dense weights will be trained first and the sparse weights will be fine-tuned later. We can achieve this by doing the following with Python APIs:\n",
    "\n",
    "1. Create the solver, reader and optimizer, then initialize the model.\n",
    "2. Load the sparse embedding layers from the saved JSON file.\n",
    "3. Add the dense layers on top of the loaded model graph.\n",
    "4. Compile the model and have an overview of the model graph.\n",
    "5. Load the sparse weights and freeze the sparse embedding layers.\n",
    "6. Train the dense weights.\n",
    "7. Unfreeze the sparse embedding layers and freeze the dense layers, reset the learning rate scheduler with a small rate.\n",
    "8. Fine-tune the sparse weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "constant-attitude",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing wdl_fine_tune.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wdl_fine_tune.py\n",
    "import hugectr\n",
    "from mpi4py import MPI\n",
    "solver = hugectr.CreateSolver(max_eval_batches = 5000,\n",
    "                              batchsize_eval = 1024,\n",
    "                              batchsize = 1024,\n",
    "                              vvgpu = [[0]],\n",
    "                              i64_input_key = False,\n",
    "                              use_mixed_precision = False,\n",
    "                              repeat_dataset = False,\n",
    "                              use_cuda_graph = True)\n",
    "reader = hugectr.DataReaderParams(data_reader_type = hugectr.DataReaderType_t.Norm,\n",
    "                          source = [\"wdl_data/file_list.6.txt\"],\n",
    "                          eval_source = \"wdl_data/file_list.7.txt\",\n",
    "                          check_type = hugectr.Check_t.Sum)\n",
    "optimizer = hugectr.CreateOptimizer(optimizer_type = hugectr.Optimizer_t.Adam)\n",
    "model = hugectr.Model(solver, reader, optimizer)\n",
    "model.construct_from_json(graph_config_file = \"wdl.json\", include_dense_network = False)\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Reshape,\n",
    "                            bottom_names = [\"sparse_embedding1\"],\n",
    "                            top_names = [\"reshape1\"],\n",
    "                            leading_dim=416))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Reshape,\n",
    "                            bottom_names = [\"sparse_embedding2\"],\n",
    "                            top_names = [\"reshape2\"],\n",
    "                            leading_dim=1))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Concat,\n",
    "                            bottom_names = [\"reshape1\", \"reshape2\", \"dense\"], top_names = [\"concat1\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"concat1\"],\n",
    "                            top_names = [\"fc1\"],\n",
    "                            num_output=1024))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc1\"],\n",
    "                            top_names = [\"relu1\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Dropout,\n",
    "                            bottom_names = [\"relu1\"],\n",
    "                            top_names = [\"dropout1\"],\n",
    "                            dropout_rate=0.5))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"dropout1\"],\n",
    "                            top_names = [\"fc2\"],\n",
    "                            num_output=1))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.BinaryCrossEntropyLoss,\n",
    "                            bottom_names = [\"fc2\", \"label\"],\n",
    "                            top_names = [\"loss\"]))\n",
    "model.compile()\n",
    "model.summary()\n",
    "model.load_sparse_weights([\"wdl0_sparse_10000.model\", \"wdl1_sparse_10000.model\"])\n",
    "model.freeze_embedding()\n",
    "model.fit(num_epochs = 1, display = 500, eval_interval = 1000, snapshot = 100000, snapshot_prefix = \"wdl\")\n",
    "model.unfreeze_embedding()\n",
    "model.freeze_dense()\n",
    "model.reset_learning_rate_scheduler(base_lr = 0.0001)\n",
    "model.fit(num_epochs = 2, display = 500, eval_interval = 1000, snapshot = 100000, snapshot_prefix = \"wdl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "reduced-government",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================Model Init=====================================================\n",
      "[08d06h50m40s][HUGECTR][INFO]: Global seed is 1676969399\n",
      "[08d06h50m41s][HUGECTR][INFO]: Peer-to-peer access cannot be fully enabled.\n",
      "Device 0: Tesla V100-SXM2-16GB\n",
      "[08d06h50m41s][HUGECTR][INFO]: num of DataReader workers: 12\n",
      "[08d06h50m41s][HUGECTR][INFO]: num_internal_buffers 1\n",
      "[08d06h50m41s][HUGECTR][INFO]: num_internal_buffers 1\n",
      "[08d06h50m41s][HUGECTR][INFO]: max_vocabulary_size_per_gpu_=5863985\n",
      "[08d06h50m41s][HUGECTR][INFO]: max_vocabulary_size_per_gpu_=5863985\n",
      "[08d06h50m41s][HUGECTR][INFO]: Load the model graph from wdl.json, successful\n",
      "===================================================Model Compile===================================================\n",
      "[08d06h50m43s][HUGECTR][INFO]: gpu0 start to init embedding\n",
      "[08d06h50m43s][HUGECTR][INFO]: gpu0 init embedding done\n",
      "[08d06h50m43s][HUGECTR][INFO]: gpu0 start to init embedding\n",
      "[08d06h50m43s][HUGECTR][INFO]: gpu0 init embedding done\n",
      "===================================================Model Summary===================================================\n",
      "Label                                   Dense                         Sparse                        \n",
      "label                                   dense                          wide_data,deep_data           \n",
      "(None, 1)                               (None, 13)                              \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Layer Type                              Input Name                    Output Name                   Output Shape                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "DistributedSlotSparseEmbeddingHash      wide_data                     sparse_embedding2             (None, 1, 1)                  \n",
      "DistributedSlotSparseEmbeddingHash      deep_data                     sparse_embedding1             (None, 26, 16)                \n",
      "Reshape                                 sparse_embedding1             reshape1                      (None, 416)                   \n",
      "Reshape                                 sparse_embedding2             reshape2                      (None, 1)                     \n",
      "Concat                                  reshape1,reshape2,dense       concat1                       (None, 430)                   \n",
      "InnerProduct                            concat1                       fc1                           (None, 1024)                  \n",
      "ReLU                                    fc1                           relu1                         (None, 1024)                  \n",
      "Dropout                                 relu1                         dropout1                      (None, 1024)                  \n",
      "InnerProduct                            dropout1                      fc2                           (None, 1)                     \n",
      "BinaryCrossEntropyLoss                  fc2,label                     loss                                                        \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "[80d60h50m43s][HUGECTR][INFO]: Loading sparse model: wdl0_sparse_10000.model\n",
      "[80d60h50m43s][HUGECTR][INFO]: Start to upload embedding table file to GPUs, total loop_num: 1006\n",
      "[80d60h50m43s][HUGECTR][INFO]: Done\n",
      "[80d60h50m43s][HUGECTR][INFO]: Loading sparse model: wdl1_sparse_10000.model\n",
      "[80d60h50m45s][HUGECTR][INFO]: Start to upload embedding table file to GPUs, total loop_num: 1054\n",
      "[80d60h50m45s][HUGECTR][INFO]: Done\n",
      "=====================================================Model Fit=====================================================\n",
      "[80d60h50m45s][HUGECTR][INFO]: Use epoch mode with number of epochs: 1\n",
      "[80d60h50m45s][HUGECTR][INFO]: Training batchsize: 1024, evaluation batchsize: 1024\n",
      "[80d60h50m45s][HUGECTR][INFO]: Evaluation interval: 1000, snapshot interval: 100000\n",
      "[80d60h50m45s][HUGECTR][INFO]: Sparse embedding trainable: 0, dense network trainable: 1\n",
      "[80d60h50m45s][HUGECTR][INFO]: Use mixed precision: 0, scaler: 1.000000, use cuda graph: 1\n",
      "[80d60h50m45s][HUGECTR][INFO]: lr: 0.001000, warmup_steps: 1, decay_start: 0, decay_steps: 1, decay_power: 2.000000, end_lr: 0.000000\n",
      "[80d60h50m45s][HUGECTR][INFO]: Training source file: wdl_data/file_list.6.txt\n",
      "[80d60h50m45s][HUGECTR][INFO]: Evaluation source file: wdl_data/file_list.7.txt\n",
      "[80d60h50m45s][HUGECTR][INFO]: -----------------------------------Epoch 0-----------------------------------\n",
      "[80d60h50m48s][HUGECTR][INFO]: Iter: 500 Time(500 iters): 2.961473s Loss: 0.108930 lr:0.001000\n",
      "[80d60h50m51s][HUGECTR][INFO]: Iter: 1000 Time(500 iters): 2.839396s Loss: 0.125089 lr:0.001000\n",
      "[80d60h51m40s][HUGECTR][INFO]: Evaluation, AUC: 0.748101\n",
      "[80d60h51m40s][HUGECTR][INFO]: Eval Time for 5000 iters: 12.735171s\n",
      "[80d60h51m60s][HUGECTR][INFO]: Iter: 1500 Time(500 iters): 15.639109s Loss: 0.152266 lr:0.001000\n",
      "[80d60h51m90s][HUGECTR][INFO]: Iter: 2000 Time(500 iters): 2.899893s Loss: 0.128567 lr:0.001000\n",
      "[80d60h51m13s][HUGECTR][INFO]: Evaluation, AUC: 0.753144\n",
      "[80d60h51m13s][HUGECTR][INFO]: Eval Time for 5000 iters: 4.062013s\n",
      "[80d60h51m16s][HUGECTR][INFO]: Iter: 2500 Time(500 iters): 6.963181s Loss: 0.125676 lr:0.001000\n",
      "[80d60h51m19s][HUGECTR][INFO]: Iter: 3000 Time(500 iters): 2.902803s Loss: 0.136227 lr:0.001000\n",
      "[80d60h51m23s][HUGECTR][INFO]: Evaluation, AUC: 0.754586\n",
      "[80d60h51m23s][HUGECTR][INFO]: Eval Time for 5000 iters: 4.149117s\n",
      "[80d60h51m27s][HUGECTR][INFO]: Iter: 3500 Time(500 iters): 7.890176s Loss: 0.147179 lr:0.001000\n",
      "[80d60h51m31s][HUGECTR][INFO]: Iter: 4000 Time(500 iters): 3.438144s Loss: 0.148604 lr:0.001000\n",
      "[80d60h51m35s][HUGECTR][INFO]: Evaluation, AUC: 0.756132\n",
      "[80d60h51m35s][HUGECTR][INFO]: Eval Time for 5000 iters: 4.288588s\n",
      "=====================================================Model Fit=====================================================\n",
      "[80d60h51m35s][HUGECTR][INFO]: Use epoch mode with number of epochs: 2\n",
      "[80d60h51m35s][HUGECTR][INFO]: Training batchsize: 1024, evaluation batchsize: 1024\n",
      "[80d60h51m35s][HUGECTR][INFO]: Evaluation interval: 1000, snapshot interval: 100000\n",
      "[80d60h51m35s][HUGECTR][INFO]: Sparse embedding trainable: 1, dense network trainable: 0\n",
      "[80d60h51m35s][HUGECTR][INFO]: Use mixed precision: 0, scaler: 1.000000, use cuda graph: 1\n",
      "[80d60h51m35s][HUGECTR][INFO]: lr: 0.001000, warmup_steps: 1, decay_start: 0, decay_steps: 1, decay_power: 2.000000, end_lr: 0.000000\n",
      "[80d60h51m35s][HUGECTR][INFO]: Training source file: wdl_data/file_list.6.txt\n",
      "[80d60h51m35s][HUGECTR][INFO]: Evaluation source file: wdl_data/file_list.7.txt\n",
      "[80d60h51m35s][HUGECTR][INFO]: -----------------------------------Epoch 0-----------------------------------\n",
      "[80d60h51m38s][HUGECTR][INFO]: Iter: 500 Time(500 iters): 2.915702s Loss: 0.182584 lr:0.000100\n",
      "[80d60h51m41s][HUGECTR][INFO]: Iter: 1000 Time(500 iters): 2.900289s Loss: 0.169668 lr:0.000100\n",
      "[80d60h51m45s][HUGECTR][INFO]: Evaluation, AUC: 0.757154\n",
      "[80d60h51m45s][HUGECTR][INFO]: Eval Time for 5000 iters: 4.350775s\n",
      "[80d60h51m48s][HUGECTR][INFO]: Iter: 1500 Time(500 iters): 7.251964s Loss: 0.128987 lr:0.000100\n",
      "[80d60h51m51s][HUGECTR][INFO]: Iter: 2000 Time(500 iters): 2.900217s Loss: 0.125391 lr:0.000100\n",
      "[80d60h51m55s][HUGECTR][INFO]: Evaluation, AUC: 0.757876\n",
      "[80d60h51m55s][HUGECTR][INFO]: Eval Time for 5000 iters: 4.259053s\n",
      "[80d60h51m58s][HUGECTR][INFO]: Iter: 2500 Time(500 iters): 7.164383s Loss: 0.137650 lr:0.000100\n",
      "[80d60h52m10s][HUGECTR][INFO]: Iter: 3000 Time(500 iters): 2.907831s Loss: 0.145674 lr:0.000100\n",
      "[80d60h52m50s][HUGECTR][INFO]: Evaluation, AUC: 0.758475\n",
      "[80d60h52m50s][HUGECTR][INFO]: Eval Time for 5000 iters: 4.236534s\n",
      "[80d60h52m80s][HUGECTR][INFO]: Iter: 3500 Time(500 iters): 7.138199s Loss: 0.110185 lr:0.000100\n",
      "[80d60h52m11s][HUGECTR][INFO]: Iter: 4000 Time(500 iters): 2.902719s Loss: 0.144028 lr:0.000100\n",
      "[80d60h52m15s][HUGECTR][INFO]: Evaluation, AUC: 0.758838\n",
      "[80d60h52m15s][HUGECTR][INFO]: Eval Time for 5000 iters: 4.269163s\n",
      "[80d60h52m15s][HUGECTR][INFO]: -----------------------------------Epoch 1-----------------------------------\n",
      "[80d60h52m18s][HUGECTR][INFO]: Iter: 4500 Time(500 iters): 7.178340s Loss: 0.091034 lr:0.000100\n",
      "[80d60h52m21s][HUGECTR][INFO]: Iter: 5000 Time(500 iters): 2.898825s Loss: 0.123581 lr:0.000100\n",
      "[80d60h52m25s][HUGECTR][INFO]: Evaluation, AUC: 0.759134\n",
      "[80d60h52m25s][HUGECTR][INFO]: Eval Time for 5000 iters: 4.266120s\n",
      "[80d60h52m28s][HUGECTR][INFO]: Iter: 5500 Time(500 iters): 7.168957s Loss: 0.120453 lr:0.000100\n",
      "[80d60h52m31s][HUGECTR][INFO]: Iter: 6000 Time(500 iters): 2.898585s Loss: 0.140030 lr:0.000100\n",
      "[80d60h52m35s][HUGECTR][INFO]: Evaluation, AUC: 0.759451\n",
      "[80d60h52m35s][HUGECTR][INFO]: Eval Time for 5000 iters: 4.323869s\n",
      "[80d60h52m38s][HUGECTR][INFO]: Iter: 6500 Time(500 iters): 7.223872s Loss: 0.111762 lr:0.000100\n",
      "[80d60h52m41s][HUGECTR][INFO]: Iter: 7000 Time(500 iters): 2.900088s Loss: 0.136742 lr:0.000100\n",
      "[80d60h52m45s][HUGECTR][INFO]: Evaluation, AUC: 0.759784\n",
      "[80d60h52m45s][HUGECTR][INFO]: Eval Time for 5000 iters: 4.241443s\n",
      "[80d60h52m48s][HUGECTR][INFO]: Iter: 7500 Time(500 iters): 7.141746s Loss: 0.141152 lr:0.000100\n",
      "[80d60h52m51s][HUGECTR][INFO]: Iter: 8000 Time(500 iters): 2.907059s Loss: 0.141178 lr:0.000100\n",
      "[80d60h52m55s][HUGECTR][INFO]: Evaluation, AUC: 0.759954\n",
      "[80d60h52m55s][HUGECTR][INFO]: Eval Time for 5000 iters: 4.220712s\n"
     ]
    }
   ],
   "source": [
    "!python3 wdl_fine_tune.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technical-builder",
   "metadata": {},
   "source": [
    "<a id=\"34\"></a>\n",
    "### 3.4 Low-level Training\n",
    "\n",
    "The low-level training APIs are maintained in the enhanced HugeCTR Python interface. If you want to have precise control of each training iteration and each evaluation step, you may find it helpful to use these APIs. Since the data reader behavior is different in epoch mode and non-epoch mode, we should pay attention to how to tweak the data reader when using low-level training. We will denmonstrate how to write the low-level training scripts for non-epoch mode, epoch mode and model oversubscription mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleasant-cigarette",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile wdl_non_epoch.py\n",
    "import hugectr\n",
    "from mpi4py import MPI\n",
    "solver = hugectr.CreateSolver(max_eval_batches = 5000,\n",
    "                              batchsize_eval = 1024,\n",
    "                              batchsize = 1024,\n",
    "                              vvgpu = [[0]],\n",
    "                              i64_input_key = False,\n",
    "                              use_mixed_precision = False,\n",
    "                              repeat_dataset = True,\n",
    "                              use_cuda_graph = True)\n",
    "reader = hugectr.DataReaderParams(data_reader_type = hugectr.DataReaderType_t.Norm,\n",
    "                          source = [\"wdl_data/file_list.0.txt\"],\n",
    "                          eval_source = \"wdl_data/file_list.1.txt\",\n",
    "                          check_type = hugectr.Check_t.Sum)\n",
    "optimizer = hugectr.CreateOptimizer(optimizer_type = hugectr.Optimizer_t.Adam)\n",
    "model = hugectr.Model(solver, reader, optimizer)\n",
    "model.construct_from_json(graph_config_file = \"wdl.json\", include_dense_network = True)\n",
    "model.compile()\n",
    "model.start_data_reading()\n",
    "lr_sch = model.get_learning_rate_scheduler()\n",
    "max_iter = 2000\n",
    "for i in range(max_iter):\n",
    "  lr = lr_sch.get_next()\n",
    "  model.set_learning_rate(lr)\n",
    "  model.train()\n",
    "  if (i%100 == 0):\n",
    "    loss = model.get_current_loss()\n",
    "    print(\"[HUGECTR][INFO] iter: {}; loss: {}\".format(i, loss))\n",
    "  if (i%1000 == 0 and i != 0):\n",
    "    for _ in range(solver.max_eval_batches):\n",
    "      model.eval()\n",
    "    metrics = model.get_eval_metrics()\n",
    "    print(\"[HUGECTR][INFO] iter: {}, {}\".format(i, metrics))\n",
    "model.save_params_to_files(\"./\", max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compliant-dakota",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 wdl_non_epoch.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "first-notice",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile wdl_epoch.py\n",
    "import hugectr\n",
    "from mpi4py import MPI\n",
    "solver = hugectr.CreateSolver(max_eval_batches = 5000,\n",
    "                              batchsize_eval = 1024,\n",
    "                              batchsize = 1024,\n",
    "                              vvgpu = [[0]],\n",
    "                              i64_input_key = False,\n",
    "                              use_mixed_precision = False,\n",
    "                              repeat_dataset = False,\n",
    "                              use_cuda_graph = True)\n",
    "reader = hugectr.DataReaderParams(data_reader_type = hugectr.DataReaderType_t.Norm,\n",
    "                          source = [\"wdl_data/file_list.0.txt\"],\n",
    "                          eval_source = \"wdl_data/file_list.1.txt\",\n",
    "                          check_type = hugectr.Check_t.Sum)\n",
    "optimizer = hugectr.CreateOptimizer(optimizer_type = hugectr.Optimizer_t.Adam)\n",
    "model = hugectr.Model(solver, reader, optimizer)\n",
    "model.construct_from_json(graph_config_file = \"wdl.json\", include_dense_network = True)\n",
    "model.compile()\n",
    "lr_sch = model.get_learning_rate_scheduler()\n",
    "data_reader_train = model.get_data_reader_train()\n",
    "data_reader_eval = model.get_data_reader_eval()\n",
    "data_reader_eval.set_source()\n",
    "iteration = 0\n",
    "for epoch in range(2):\n",
    "  print(\"[HUGECTR][INFO] epoch: \", epoch)\n",
    "  data_reader_train.set_source()\n",
    "  while True:\n",
    "    lr = lr_sch.get_next()\n",
    "    model.set_learning_rate(lr)\n",
    "    model.train()\n",
    "    if data_reader_train.is_eof():\n",
    "      break\n",
    "    if iteration % 1000 == 0:\n",
    "      batches = 0\n",
    "      while True:\n",
    "        batches += 1\n",
    "        model.eval()\n",
    "        if batches >= solver.max_eval_batches or data_reader_eval.is_eof():\n",
    "          break\n",
    "      if data_reader_eval.is_eof():\n",
    "        data_reader_eval.set_source()\n",
    "      metrics = model.get_eval_metrics()\n",
    "      print(\"[HUGECTR][INFO] iter: {}, metrics: {}\".format(iteration, metrics))\n",
    "    iteration += 1\n",
    "model.save_params_to_files(\"./\", iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seventh-mechanics",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 wdl_epoch.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collective-newman",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile wdl_mos.py\n",
    "import hugectr\n",
    "from mpi4py import MPI\n",
    "solver = hugectr.CreateSolver(max_eval_batches = 5000,\n",
    "                              batchsize_eval = 1024,\n",
    "                              batchsize = 1024,\n",
    "                              vvgpu = [[0]],\n",
    "                              i64_input_key = False,\n",
    "                              use_mixed_precision = False,\n",
    "                              repeat_dataset = False,\n",
    "                              use_cuda_graph = True,\n",
    "                              use_model_oversubscriber = True,\n",
    "                              temp_embedding_dir = \"mos\")\n",
    "reader = hugectr.DataReaderParams(data_reader_type = hugectr.DataReaderType_t.Norm,\n",
    "                          source = [\"wdl_data/file_list.\"+str(i)+\".txt\" for i in range(2)],\n",
    "                          keyset = [\"wdl_data/file_list.\"+str(i)+\".keyset\" for i in range(2)],\n",
    "                          eval_source = \"wdl_data/file_list.2.txt\",\n",
    "                          check_type = hugectr.Check_t.Sum)\n",
    "optimizer = hugectr.CreateOptimizer(optimizer_type = hugectr.Optimizer_t.Adam)\n",
    "model = hugectr.Model(solver, reader, optimizer)\n",
    "model.construct_from_json(graph_config_file = \"wdl.json\", include_dense_network = True)\n",
    "model.compile()\n",
    "lr_sch = model.get_learning_rate_scheduler()\n",
    "data_reader_train = model.get_data_reader_train()\n",
    "data_reader_eval = model.get_data_reader_eval()\n",
    "model_oversubscriber = model.get_model_oversubscriber()\n",
    "dataset = [(\"wdl_data/file_list.\"+str(i)+\".txt\", \"wdl_data/file_list.\"+str(i)+\".keyset\") for i in range(2)]\n",
    "data_reader_eval.set_source(\"wdl_data/file_list.2.txt\")\n",
    "iteration = 0\n",
    "for file_list, keyset_file in dataset:\n",
    "  data_reader_train.set_source(file_list)\n",
    "  model_oversubscriber.update(keyset_file)\n",
    "  while True:\n",
    "    lr = lr_sch.get_next()\n",
    "    model.set_learning_rate(lr)\n",
    "    model.train()\n",
    "    if data_reader_train.is_eof():\n",
    "      break\n",
    "    if iteration % 1000 == 0:\n",
    "      batches = 0\n",
    "      while True:\n",
    "        batches += 1\n",
    "        model.eval()\n",
    "        if batches >= solver.max_eval_batches or data_reader_eval.is_eof():\n",
    "          break\n",
    "      if data_reader_eval.is_eof():\n",
    "        data_reader_eval.set_source()\n",
    "      metrics = model.get_eval_metrics()\n",
    "      print(\"[HUGECTR][INFO] iter: {}, metrics: {}\".format(iteration, metrics))\n",
    "    iteration += 1\n",
    "  print(\"[HUGECTR][INFO] trained with data in {}\".format(file_list))\n",
    "model.save_params_to_files(\"./\", iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "provincial-proportion",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 wdl_mos.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
