{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NVIDIA Merlin on Microsoft's News Dataset (MIND)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial notebook, we would be using the [Microsoft's News Dataset (MIND)](https://msnews.github.io/) to demonstrate NVTabular for ETL the data and HugeCTR for training Deep Neural Network models for building a Recommender System.\n",
    "\n",
    "The MIND dataset contains 15M impressions generated by 1M users over 160k news articles. Our goal from this jupyter notebook would be to train a model that can predict whether a user would click on a news article or not.\n",
    "\n",
    "In order to build a Recommender System, we would be first cleaning and pre-processing the data, then developing simple time based and complex target & count encoded features to finally train and evaluate Deep Learning Recommendation Model (DLRM)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please remember to run this jupyter notebook in the [merlin-training:0.6](https://ngc.nvidia.com/catalog/containers/nvidia:merlin:merlin-training) docker container."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import libraries and create directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (4.61.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.1.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "unzip is already the newest version (6.0-25ubuntu1).\n",
      "wget is already the newest version (1.20.3-1ubuntu1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 4 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "# Install packages required for this notebook\n",
    "!pip install tqdm\n",
    "!apt install wget unzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, glob, shutil, sys, os, pickle, json\n",
    "from tqdm import tqdm\n",
    "\n",
    "import cupy as cp          # CuPy is an implementation of NumPy-compatible multi-dimensional array on GPU\n",
    "import cudf                # cuDF is an implementation of Pandas-like Dataframe on GPU\n",
    "import rmm                 # library for pre-allocating memory on GPU\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# NVTabular is the core library we will use here for feature engineering/preprocessing on GPU\n",
    "from nvtabular.ops import Operator\n",
    "import nvtabular as nvt\n",
    "from nvtabular.utils import device_mem_size\n",
    "\n",
    "# Dask is the backend job scheduler used by NVTabular\n",
    "import dask   \n",
    "import dask_cudf\n",
    "from dask_cuda import LocalCUDACluster\n",
    "from dask.distributed import Client\n",
    "from dask.distributed import wait\n",
    "from dask.utils import parse_bytes\n",
    "from dask.delayed import delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is often a good idea to set-aside (fast) dedicated disk space for dask workers to spill data and logging information. To make things simple, we will perform all IO within a single `BASE_DIR` for this example. Feel free to set this variable yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define \"fast\" root directory for this example\n",
    "BASE_DIR = os.environ.get(\"BASE_DIR\", \"./basedir\")\n",
    "\n",
    "# Define worker/output directories\n",
    "dask_workdir = os.path.join(BASE_DIR, \"workdir\")\n",
    "\n",
    "# Directory to store the raw downloaded dataset\n",
    "data_input_path = os.path.join(BASE_DIR, \"dataset\")\n",
    "data_path_train = os.path.join(data_input_path, \"train\")\n",
    "data_path_valid = os.path.join(data_input_path, \"valid\")\n",
    "\n",
    "# Directory to store NVTabular's processed dataset\n",
    "data_output_path = os.path.join(BASE_DIR, \"processed_nvt\")\n",
    "output_train_path = os.path.join(data_output_path, \"train\")\n",
    "output_valid_path = os.path.join(data_output_path, \"valid\")\n",
    "\n",
    "# Directory to store HugeCTR's train configurations and weights\n",
    "config_output_path = os.path.join(BASE_DIR, \"configs\")\n",
    "weights_path = os.path.join(BASE_DIR, \"weights\")\n",
    "\n",
    "#Creating and cleaning our worker/output directories\n",
    "try:\n",
    "    # Ensure BASE_DIR exists\n",
    "    if not os.path.isdir(BASE_DIR):\n",
    "        os.mkdir(BASE_DIR)\n",
    "\n",
    "    # Make sure we have a clean worker space for Dask\n",
    "    if os.path.isdir(dask_workdir):\n",
    "        shutil.rmtree(dask_workdir)\n",
    "    os.mkdir(dask_workdir)\n",
    "\n",
    "    # Make sure we have a clean path for downloading dataset and preprocessing\n",
    "    if os.path.isdir(data_input_path):\n",
    "        shutil.rmtree(data_input_path)\n",
    "    os.mkdir(data_input_path)\n",
    "    os.mkdir(data_path_train)\n",
    "    os.mkdir(data_path_valid)\n",
    "\n",
    "    # Make sure we have a clean output path\n",
    "    if os.path.isdir(data_output_path):\n",
    "        shutil.rmtree(data_output_path)\n",
    "    os.mkdir(data_output_path)\n",
    "    os.mkdir(output_train_path)\n",
    "    os.mkdir(output_valid_path)\n",
    "    \n",
    "    # Make sure we have a clean configs and weights path\n",
    "    if os.path.isdir(config_output_path):\n",
    "        shutil.rmtree(config_output_path)\n",
    "    os.mkdir(config_output_path)    \n",
    "        \n",
    "    if os.path.isdir(weights_path):\n",
    "        shutil.rmtree(weights_path)\n",
    "    os.mkdir(weights_path)\n",
    "\n",
    "except OSError:\n",
    "    print (\"Creation of the directories failed\")\n",
    "else:\n",
    "    print (\"Successfully created the directories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following directory structure has been created and would be used to store everything concerning this tutorial:\n",
    "\n",
    "basedir <br>\n",
    "&emsp; |--- workdir    \n",
    "&emsp; |--- dataset <br>\n",
    "&emsp; &emsp; |--- train <br>\n",
    "&emsp; &emsp; |--- valid  <br>\n",
    "&emsp; |--- processed_nvt <br>\n",
    "&emsp;  &emsp; |--- train <br>\n",
    "&emsp;  &emsp; |--- valid  <br>\n",
    "&emsp; |--- configs <br>\n",
    "&emsp; |--- weights <br>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Deploy a Distributed-Dask cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jun 22 22:41:58 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.119.04   Driver Version: 450.119.04   CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:06:00.0 Off |                    0 |\n",
      "| N/A   36C    P0    42W / 300W |      3MiB / 16160MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  On   | 00000000:07:00.0 Off |                    0 |\n",
      "| N/A   40C    P0    46W / 300W |      3MiB / 16160MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2...  On   | 00000000:0A:00.0 Off |                    0 |\n",
      "| N/A   38C    P0    42W / 300W |      3MiB / 16160MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Check the GPUs that are available to this notebook\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Dask GPU cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:44281</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>3</li>\n",
       "  <li><b>Cores: </b>3</li>\n",
       "  <li><b>Memory: </b>503.79 GiB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:44281' processes=3 threads=3, memory=503.79 GiB>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_GPUS = [0,1,2] # Set this to the GPU IDs that are observed from the above cell\n",
    "\n",
    "# Dask dashboard\n",
    "dashboard_port = \"8787\"\n",
    "\n",
    "# Deploy a single-machine multi-GPU cluster\n",
    "protocol = \"tcp\"             # \"tcp\" or \"ucx\"\n",
    "visible_devices = \",\".join([str(n) for n in NUM_GPUS])  # Detect devices to place workers\n",
    "device_spill_frac = 0.9      # Spill GPU-Worker memory to host at this limit.\n",
    "                             # Reduce if spilling fails to prevent\n",
    "                             # device memory errors.\n",
    "\n",
    "# Get device memory capacity\n",
    "capacity = device_mem_size(kind=\"total\") \n",
    "\n",
    "# Check if any device memory is already occupied\n",
    "\"\"\"\n",
    "for dev in visible_devices.split(\",\"):\n",
    "    fmem = _pynvml_mem_size(kind=\"free\", index=int(dev))\n",
    "    used = (device_size - fmem) / 1e9\n",
    "    if used > 1.0:\n",
    "        warnings.warn(f\"BEWARE - {used} GB is already occupied on device {int(dev)}!\")\n",
    "\"\"\"\n",
    "\n",
    "cluster = None               # (Optional) Specify existing scheduler port\n",
    "if cluster is None:\n",
    "    cluster = LocalCUDACluster(\n",
    "        protocol = protocol,\n",
    "        n_workers=len(visible_devices.split(\",\")),\n",
    "        CUDA_VISIBLE_DEVICES = visible_devices,\n",
    "        device_memory_limit = capacity * device_spill_frac,\n",
    "        local_directory=dask_workdir,\n",
    "        dashboard_address=\":\" + dashboard_port,\n",
    "    )\n",
    "\n",
    "# Create the distributed client\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tcp://127.0.0.1:33443': None,\n",
       " 'tcp://127.0.0.1:34239': None,\n",
       " 'tcp://127.0.0.1:37923': None}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize RMM pool on ALL workers\n",
    "def _rmm_pool():\n",
    "    rmm.reinitialize(\n",
    "        pool_allocator=True,\n",
    "        initial_pool_size=None, # Use default size\n",
    "    )\n",
    "    \n",
    "client.run(_rmm_pool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3:  Download & explore MIND dataset\n",
    "\n",
    "MIcrosoft News Dataset (MIND) is a large-scale dataset for news recommendation research. It was collected from anonymized behavior logs of Microsoft News website.\n",
    "\n",
    "Please read and accept the Microsoft Research License Terms before downloading.\n",
    "\n",
    "Let's download the train and validation set, and unzip them to their respective directories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://mind201910small.blob.core.windows.net/release/MINDlarge_train.zip\n",
    "!wget https://mind201910small.blob.core.windows.net/release/MINDlarge_dev.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!unzip MINDlarge_train.zip -d $BASE_DIR/dataset/train\n",
    "!unzip MINDlarge_dev.zip -d  $BASE_DIR/dataset/valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MIND dataset for news recommendation was collected from anonymized behavior logs of Microsoft News website. To protect user's privacy, each user is de-linked from the production system when securely hashed into an anonymized ID. \n",
    "MIND dataset team has randomly sampled 1M users who had at least 5 news clicks from October 12 to November 22, 2019 (6 weeks).\n",
    "\n",
    "Microsoft has provided train, validation and test sets of this data but we are going to use the train and validation set for this tutorial.\n",
    "\n",
    "### Dataset format \n",
    "\n",
    "Each set of this data contains the following 4 files:\n",
    "\n",
    "1. behaviors.tsv - The click history and impression logs of users\n",
    "2. news.tsv - Details of news articles mapped with the news ID\n",
    "3. entity_embedding.vec - The embeddings of entities in news extracted from knowledge graph\n",
    "4. relation_embedding.vec - The embeddings of relations between entities extracted from knowledge graph\n",
    "\n",
    "Let's take a look at both these TSV files and understand how we can utilise them for our Recommendation System. <br>\n",
    "Note - For the ease of this tutorial, we are ignoring the embeddings provided by the MIND team."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Behaviors data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>U87243</td>\n",
       "      <td>11/10/2019 11:30:54 AM</td>\n",
       "      <td>N8668 N39081 N65259 N79529 N73408 N43615 N2937...</td>\n",
       "      <td>N78206-0 N26368-0 N7578-0 N58592-0 N19858-0 N5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>U598644</td>\n",
       "      <td>11/12/2019 1:45:29 PM</td>\n",
       "      <td>N56056 N8726 N70353 N67998 N83823 N111108 N107...</td>\n",
       "      <td>N47996-0 N82719-0 N117066-0 N8491-0 N123784-0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>U532401</td>\n",
       "      <td>11/13/2019 11:23:03 AM</td>\n",
       "      <td>N128643 N87446 N122948 N9375 N82348 N129412 N5...</td>\n",
       "      <td>N103852-0 N53474-0 N127836-0 N47925-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>U593596</td>\n",
       "      <td>11/12/2019 12:24:09 PM</td>\n",
       "      <td>N31043 N39592 N4104 N8223 N114581 N92747 N1207...</td>\n",
       "      <td>N38902-0 N76434-0 N71593-0 N100073-0 N108736-0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>U239687</td>\n",
       "      <td>11/14/2019 8:03:01 PM</td>\n",
       "      <td>N65250 N122359 N71723 N53796 N41663 N41484 N11...</td>\n",
       "      <td>N76209-0 N48841-0 N67937-0 N62235-0 N6307-0 N3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0        1                       2  \\\n",
       "0  1   U87243  11/10/2019 11:30:54 AM   \n",
       "1  2  U598644   11/12/2019 1:45:29 PM   \n",
       "2  3  U532401  11/13/2019 11:23:03 AM   \n",
       "3  4  U593596  11/12/2019 12:24:09 PM   \n",
       "4  5  U239687   11/14/2019 8:03:01 PM   \n",
       "\n",
       "                                                   3  \\\n",
       "0  N8668 N39081 N65259 N79529 N73408 N43615 N2937...   \n",
       "1  N56056 N8726 N70353 N67998 N83823 N111108 N107...   \n",
       "2  N128643 N87446 N122948 N9375 N82348 N129412 N5...   \n",
       "3  N31043 N39592 N4104 N8223 N114581 N92747 N1207...   \n",
       "4  N65250 N122359 N71723 N53796 N41663 N41484 N11...   \n",
       "\n",
       "                                                   4  \n",
       "0  N78206-0 N26368-0 N7578-0 N58592-0 N19858-0 N5...  \n",
       "1  N47996-0 N82719-0 N117066-0 N8491-0 N123784-0 ...  \n",
       "2              N103852-0 N53474-0 N127836-0 N47925-1  \n",
       "3  N38902-0 N76434-0 N71593-0 N100073-0 N108736-0...  \n",
       "4  N76209-0 N48841-0 N67937-0 N62235-0 N6307-0 N3...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "behaviors_train = cudf.read_csv(os.path.join(data_path_train , 'behaviors.tsv'), \n",
    "                                header=None, \n",
    "                                sep='\\t',)\n",
    "behaviors_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row in this data file represents one instance of an impression generated by the user. The columns of behaviors data are represented as:<br>\n",
    "\n",
    "[Impression ID] [User ID] [Time until when Impression Recorded] [User Click History] [Impression News]\n",
    "\n",
    "**Column 0**: Impression ID (int64)<br>\n",
    "This is the ID of the impression generated.<br>\n",
    "e.g. 1,2,3,4,5\n",
    "        \n",
    "**Column 1**: User ID (string)<br>\n",
    "The anonymous ID of a user who has generated that impression.<br>\n",
    "e.g. U89 , U395 , U60005, U3965770\n",
    "        \n",
    "**Column 2**: Time (timestamp)<br>\n",
    "The impression time with format `MM/DD/YYYY HH:MM:SS AM/PM` <br>\n",
    "This is the point of time upto which the user's impression have been captured. \n",
    "            \n",
    "**Column 3**: History (string)<br>\n",
    "The news click history of this user before this impression. The clicked news articles are ordered by time.<br>\n",
    "e.g. N106403 N71977 N97080 N102132 N97212 N121652\n",
    "        \n",
    "**Column 4**: Impressions (string)<br>\n",
    "List of news displayed to the user and user's click behaviors on them (1 for click and 0 for non-click).<br>\n",
    "e.g. N129416-0 N26703-1 N120089-1 N53018-0 N89764-0 N91737-0 N29160-0\n",
    "   \n",
    "The corresponding details of news ID in history and impression columns would be present in the news.tsv file.\n",
    "\n",
    "For more details on dataset: Official MIND Dataset Description, click [Official Dataset Description](https://github.com/msnews/msnews.github.io/blob/master/assets/doc/introduction.md)\n",
    "\n",
    "Let's reload the data with their respective column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "behaviors_columns = ['impression_id', 'uid', 'time', 'history', 'impressions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>impression_id</th>\n",
       "      <th>uid</th>\n",
       "      <th>time</th>\n",
       "      <th>history</th>\n",
       "      <th>impressions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>U87243</td>\n",
       "      <td>11/10/2019 11:30:54 AM</td>\n",
       "      <td>N8668 N39081 N65259 N79529 N73408 N43615 N2937...</td>\n",
       "      <td>N78206-0 N26368-0 N7578-0 N58592-0 N19858-0 N5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>U598644</td>\n",
       "      <td>11/12/2019 1:45:29 PM</td>\n",
       "      <td>N56056 N8726 N70353 N67998 N83823 N111108 N107...</td>\n",
       "      <td>N47996-0 N82719-0 N117066-0 N8491-0 N123784-0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>U532401</td>\n",
       "      <td>11/13/2019 11:23:03 AM</td>\n",
       "      <td>N128643 N87446 N122948 N9375 N82348 N129412 N5...</td>\n",
       "      <td>N103852-0 N53474-0 N127836-0 N47925-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>U593596</td>\n",
       "      <td>11/12/2019 12:24:09 PM</td>\n",
       "      <td>N31043 N39592 N4104 N8223 N114581 N92747 N1207...</td>\n",
       "      <td>N38902-0 N76434-0 N71593-0 N100073-0 N108736-0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>U239687</td>\n",
       "      <td>11/14/2019 8:03:01 PM</td>\n",
       "      <td>N65250 N122359 N71723 N53796 N41663 N41484 N11...</td>\n",
       "      <td>N76209-0 N48841-0 N67937-0 N62235-0 N6307-0 N3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   impression_id      uid                    time  \\\n",
       "0              1   U87243  11/10/2019 11:30:54 AM   \n",
       "1              2  U598644   11/12/2019 1:45:29 PM   \n",
       "2              3  U532401  11/13/2019 11:23:03 AM   \n",
       "3              4  U593596  11/12/2019 12:24:09 PM   \n",
       "4              5  U239687   11/14/2019 8:03:01 PM   \n",
       "\n",
       "                                             history  \\\n",
       "0  N8668 N39081 N65259 N79529 N73408 N43615 N2937...   \n",
       "1  N56056 N8726 N70353 N67998 N83823 N111108 N107...   \n",
       "2  N128643 N87446 N122948 N9375 N82348 N129412 N5...   \n",
       "3  N31043 N39592 N4104 N8223 N114581 N92747 N1207...   \n",
       "4  N65250 N122359 N71723 N53796 N41663 N41484 N11...   \n",
       "\n",
       "                                         impressions  \n",
       "0  N78206-0 N26368-0 N7578-0 N58592-0 N19858-0 N5...  \n",
       "1  N47996-0 N82719-0 N117066-0 N8491-0 N123784-0 ...  \n",
       "2              N103852-0 N53474-0 N127836-0 N47925-1  \n",
       "3  N38902-0 N76434-0 N71593-0 N100073-0 N108736-0...  \n",
       "4  N76209-0 N48841-0 N67937-0 N62235-0 N6307-0 N3...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "behaviors_train = cudf.read_csv(os.path.join(data_path_train , 'behaviors.tsv'), \n",
    "                          header=None, \n",
    "                          names=behaviors_columns,\n",
    "                    sep='\\t',)\n",
    "behaviors_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>impression_id</th>\n",
       "      <th>uid</th>\n",
       "      <th>time</th>\n",
       "      <th>history</th>\n",
       "      <th>impressions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>U134050</td>\n",
       "      <td>11/15/2019 8:55:22 AM</td>\n",
       "      <td>N12246 N128820 N119226 N4065 N67770 N33446 N10...</td>\n",
       "      <td>N91737-0 N30206-0 N54368-0 N117802-0 N18190-0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>U254959</td>\n",
       "      <td>11/15/2019 11:42:35 AM</td>\n",
       "      <td>N34011 N9375 N67397 N7936 N118985 N109453 N103...</td>\n",
       "      <td>N119999-0 N24958-0 N104054-0 N33901-0 N9250-0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>U499841</td>\n",
       "      <td>11/15/2019 9:08:21 AM</td>\n",
       "      <td>N63858 N26834 N6379 N85484 N15229 N65119 N1047...</td>\n",
       "      <td>N18190-0 N89764-0 N91737-0 N54368-0 N49978-1 N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>U107107</td>\n",
       "      <td>11/15/2019 5:50:31 AM</td>\n",
       "      <td>N12959 N8085 N18389 N3758 N9740 N90543 N129790...</td>\n",
       "      <td>N122944-1 N18190-0 N55801-0 N59297-0 N128045-0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>U492344</td>\n",
       "      <td>11/15/2019 5:02:25 AM</td>\n",
       "      <td>N109183 N48453 N85005 N45706 N98923 N46069 N35...</td>\n",
       "      <td>N64785-0 N82503-0 N32993-0 N122944-0 N29160-0 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   impression_id      uid                    time  \\\n",
       "0              1  U134050   11/15/2019 8:55:22 AM   \n",
       "1              2  U254959  11/15/2019 11:42:35 AM   \n",
       "2              3  U499841   11/15/2019 9:08:21 AM   \n",
       "3              4  U107107   11/15/2019 5:50:31 AM   \n",
       "4              5  U492344   11/15/2019 5:02:25 AM   \n",
       "\n",
       "                                             history  \\\n",
       "0  N12246 N128820 N119226 N4065 N67770 N33446 N10...   \n",
       "1  N34011 N9375 N67397 N7936 N118985 N109453 N103...   \n",
       "2  N63858 N26834 N6379 N85484 N15229 N65119 N1047...   \n",
       "3  N12959 N8085 N18389 N3758 N9740 N90543 N129790...   \n",
       "4  N109183 N48453 N85005 N45706 N98923 N46069 N35...   \n",
       "\n",
       "                                         impressions  \n",
       "0  N91737-0 N30206-0 N54368-0 N117802-0 N18190-0 ...  \n",
       "1  N119999-0 N24958-0 N104054-0 N33901-0 N9250-0 ...  \n",
       "2  N18190-0 N89764-0 N91737-0 N54368-0 N49978-1 N...  \n",
       "3  N122944-1 N18190-0 N55801-0 N59297-0 N128045-0...  \n",
       "4  N64785-0 N82503-0 N32993-0 N122944-0 N29160-0 ...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "behaviors_valid = cudf.read_csv(os.path.join(data_path_valid , 'behaviors.tsv'), \n",
    "                          header=None, \n",
    "                          names=behaviors_columns,\n",
    "                    sep='\\t',)\n",
    "behaviors_valid.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### News data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N88753</td>\n",
       "      <td>lifestyle</td>\n",
       "      <td>lifestyleroyals</td>\n",
       "      <td>The Brands Queen Elizabeth, Prince Charles, an...</td>\n",
       "      <td>Shop the notebooks, jackets, and more that the...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAGH0ET.html</td>\n",
       "      <td>[{\"Label\": \"Prince Philip, Duke of Edinburgh\",...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>N45436</td>\n",
       "      <td>news</td>\n",
       "      <td>newsscienceandtechnology</td>\n",
       "      <td>Walmart Slashes Prices on Last-Generation iPads</td>\n",
       "      <td>Apple's new iPad releases bring big deals on l...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AABmf2I.html</td>\n",
       "      <td>[{\"Label\": \"IPad\", \"Type\": \"J\", \"WikidataId\": ...</td>\n",
       "      <td>[{\"Label\": \"IPad\", \"Type\": \"J\", \"WikidataId\": ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>N23144</td>\n",
       "      <td>health</td>\n",
       "      <td>weightloss</td>\n",
       "      <td>50 Worst Habits For Belly Fat</td>\n",
       "      <td>These seemingly harmless habits are holding yo...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAB19MK.html</td>\n",
       "      <td>[{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...</td>\n",
       "      <td>[{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N86255</td>\n",
       "      <td>health</td>\n",
       "      <td>medical</td>\n",
       "      <td>Dispose of unwanted prescription drugs during ...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAISxPN.html</td>\n",
       "      <td>[{\"Label\": \"Drug Enforcement Administration\", ...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>N93187</td>\n",
       "      <td>news</td>\n",
       "      <td>newsworld</td>\n",
       "      <td>The Cost of Trump's Aid Freeze in the Trenches...</td>\n",
       "      <td>Lt. Ivan Molchanets peeked over a parapet of s...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAJgNsz.html</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{\"Label\": \"Ukraine\", \"Type\": \"G\", \"WikidataId...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0          1                         2  \\\n",
       "0  N88753  lifestyle           lifestyleroyals   \n",
       "1  N45436       news  newsscienceandtechnology   \n",
       "2  N23144     health                weightloss   \n",
       "3  N86255     health                   medical   \n",
       "4  N93187       news                 newsworld   \n",
       "\n",
       "                                                   3  \\\n",
       "0  The Brands Queen Elizabeth, Prince Charles, an...   \n",
       "1    Walmart Slashes Prices on Last-Generation iPads   \n",
       "2                      50 Worst Habits For Belly Fat   \n",
       "3  Dispose of unwanted prescription drugs during ...   \n",
       "4  The Cost of Trump's Aid Freeze in the Trenches...   \n",
       "\n",
       "                                                   4  \\\n",
       "0  Shop the notebooks, jackets, and more that the...   \n",
       "1  Apple's new iPad releases bring big deals on l...   \n",
       "2  These seemingly harmless habits are holding yo...   \n",
       "3                                               <NA>   \n",
       "4  Lt. Ivan Molchanets peeked over a parapet of s...   \n",
       "\n",
       "                                               5  \\\n",
       "0  https://assets.msn.com/labs/mind/AAGH0ET.html   \n",
       "1  https://assets.msn.com/labs/mind/AABmf2I.html   \n",
       "2  https://assets.msn.com/labs/mind/AAB19MK.html   \n",
       "3  https://assets.msn.com/labs/mind/AAISxPN.html   \n",
       "4  https://assets.msn.com/labs/mind/AAJgNsz.html   \n",
       "\n",
       "                                                   6  \\\n",
       "0  [{\"Label\": \"Prince Philip, Duke of Edinburgh\",...   \n",
       "1  [{\"Label\": \"IPad\", \"Type\": \"J\", \"WikidataId\": ...   \n",
       "2  [{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...   \n",
       "3  [{\"Label\": \"Drug Enforcement Administration\", ...   \n",
       "4                                                 []   \n",
       "\n",
       "                                                   7  \n",
       "0                                                 []  \n",
       "1  [{\"Label\": \"IPad\", \"Type\": \"J\", \"WikidataId\": ...  \n",
       "2  [{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...  \n",
       "3                                                 []  \n",
       "4  [{\"Label\": \"Ukraine\", \"Type\": \"G\", \"WikidataId...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_train = cudf.read_csv(os.path.join(data_path_train , 'news.tsv'), \n",
    "                          header=None, \n",
    "                          sep='\\t',)\n",
    "news_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row in this data file represents a news article and its attributes. The columns of this data file are:\n",
    "\n",
    "[News ID] [Category] [Subcategory] [News Title] [News Abstract] [News Url] [Entities in News Title] [Entities in News Abstract]\n",
    "\n",
    "**Column 0**: News ID (string)<br>\n",
    "This is the ID of the news article<br>\n",
    "e.g. N89 , N395 , N60005, N3965770\n",
    "        \n",
    "**Column 1**: Category (string)<br>\n",
    "Category of the news. There are 18 categories<br>\n",
    "e.g. sports , health , news ... etc\n",
    "        \n",
    "**Column 2**: SubCategory (string)<br>\n",
    "Sub-category of the news. There are 242 unique sub-categories.<br>\n",
    "e.g. golf, newsscienceandtechnology, medical, newsworld ... etc\n",
    "            \n",
    "**Column 3**: Title (string)<br>\n",
    "Title of the news article<br>\n",
    "e.g. PGA Tour winners, 50 Worst Habits For Belly Fats ... etc\n",
    "        \n",
    "**Column 4**: Abstract (string)<br>\n",
    "Abstract of the news article<br>\n",
    "e.g. A gallery of recent winners on the PGA Tour, These seemingly harmless habits are holding\n",
    "          \n",
    "**Column 5**: URL (string)<br>\n",
    "URL to the MSN site where the news article was published.<br>\n",
    "e.g. https://www.msn.com/en-us/sports/golf/pga-tour-winners/ss-AAjnQjj?ocid=chopendata\n",
    "        \n",
    "**Column 6**: Title Entities (string)<br>\n",
    "Entities present in the title\n",
    "        \n",
    "**Column 7**: Abstract Entites (string)<br>\n",
    "Entites present in the abstract\n",
    "\n",
    "Let's reload the data with their respective column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_columns = ['did', 'cat', 'sub_cat', 'title', 'abstract', 'url', 'title_entities', 'abstract_entities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>did</th>\n",
       "      <th>cat</th>\n",
       "      <th>sub_cat</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>url</th>\n",
       "      <th>title_entities</th>\n",
       "      <th>abstract_entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N88753</td>\n",
       "      <td>lifestyle</td>\n",
       "      <td>lifestyleroyals</td>\n",
       "      <td>The Brands Queen Elizabeth, Prince Charles, an...</td>\n",
       "      <td>Shop the notebooks, jackets, and more that the...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAGH0ET.html</td>\n",
       "      <td>[{\"Label\": \"Prince Philip, Duke of Edinburgh\",...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>N45436</td>\n",
       "      <td>news</td>\n",
       "      <td>newsscienceandtechnology</td>\n",
       "      <td>Walmart Slashes Prices on Last-Generation iPads</td>\n",
       "      <td>Apple's new iPad releases bring big deals on l...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AABmf2I.html</td>\n",
       "      <td>[{\"Label\": \"IPad\", \"Type\": \"J\", \"WikidataId\": ...</td>\n",
       "      <td>[{\"Label\": \"IPad\", \"Type\": \"J\", \"WikidataId\": ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>N23144</td>\n",
       "      <td>health</td>\n",
       "      <td>weightloss</td>\n",
       "      <td>50 Worst Habits For Belly Fat</td>\n",
       "      <td>These seemingly harmless habits are holding yo...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAB19MK.html</td>\n",
       "      <td>[{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...</td>\n",
       "      <td>[{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N86255</td>\n",
       "      <td>health</td>\n",
       "      <td>medical</td>\n",
       "      <td>Dispose of unwanted prescription drugs during ...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAISxPN.html</td>\n",
       "      <td>[{\"Label\": \"Drug Enforcement Administration\", ...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>N93187</td>\n",
       "      <td>news</td>\n",
       "      <td>newsworld</td>\n",
       "      <td>The Cost of Trump's Aid Freeze in the Trenches...</td>\n",
       "      <td>Lt. Ivan Molchanets peeked over a parapet of s...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAJgNsz.html</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{\"Label\": \"Ukraine\", \"Type\": \"G\", \"WikidataId...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      did        cat                   sub_cat  \\\n",
       "0  N88753  lifestyle           lifestyleroyals   \n",
       "1  N45436       news  newsscienceandtechnology   \n",
       "2  N23144     health                weightloss   \n",
       "3  N86255     health                   medical   \n",
       "4  N93187       news                 newsworld   \n",
       "\n",
       "                                               title  \\\n",
       "0  The Brands Queen Elizabeth, Prince Charles, an...   \n",
       "1    Walmart Slashes Prices on Last-Generation iPads   \n",
       "2                      50 Worst Habits For Belly Fat   \n",
       "3  Dispose of unwanted prescription drugs during ...   \n",
       "4  The Cost of Trump's Aid Freeze in the Trenches...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  Shop the notebooks, jackets, and more that the...   \n",
       "1  Apple's new iPad releases bring big deals on l...   \n",
       "2  These seemingly harmless habits are holding yo...   \n",
       "3                                               <NA>   \n",
       "4  Lt. Ivan Molchanets peeked over a parapet of s...   \n",
       "\n",
       "                                             url  \\\n",
       "0  https://assets.msn.com/labs/mind/AAGH0ET.html   \n",
       "1  https://assets.msn.com/labs/mind/AABmf2I.html   \n",
       "2  https://assets.msn.com/labs/mind/AAB19MK.html   \n",
       "3  https://assets.msn.com/labs/mind/AAISxPN.html   \n",
       "4  https://assets.msn.com/labs/mind/AAJgNsz.html   \n",
       "\n",
       "                                      title_entities  \\\n",
       "0  [{\"Label\": \"Prince Philip, Duke of Edinburgh\",...   \n",
       "1  [{\"Label\": \"IPad\", \"Type\": \"J\", \"WikidataId\": ...   \n",
       "2  [{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...   \n",
       "3  [{\"Label\": \"Drug Enforcement Administration\", ...   \n",
       "4                                                 []   \n",
       "\n",
       "                                   abstract_entities  \n",
       "0                                                 []  \n",
       "1  [{\"Label\": \"IPad\", \"Type\": \"J\", \"WikidataId\": ...  \n",
       "2  [{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...  \n",
       "3                                                 []  \n",
       "4  [{\"Label\": \"Ukraine\", \"Type\": \"G\", \"WikidataId...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_train = cudf.read_csv(os.path.join(data_path_train , 'news.tsv'), \n",
    "                          header=None, \n",
    "                          names=news_columns,\n",
    "                    sep='\\t',)\n",
    "news_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>did</th>\n",
       "      <th>cat</th>\n",
       "      <th>sub_cat</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>url</th>\n",
       "      <th>title_entities</th>\n",
       "      <th>abstract_entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N88753</td>\n",
       "      <td>lifestyle</td>\n",
       "      <td>lifestyleroyals</td>\n",
       "      <td>The Brands Queen Elizabeth, Prince Charles, an...</td>\n",
       "      <td>Shop the notebooks, jackets, and more that the...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAGH0ET.html</td>\n",
       "      <td>[{\"Label\": \"Prince Philip, Duke of Edinburgh\",...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>N23144</td>\n",
       "      <td>health</td>\n",
       "      <td>weightloss</td>\n",
       "      <td>50 Worst Habits For Belly Fat</td>\n",
       "      <td>These seemingly harmless habits are holding yo...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAB19MK.html</td>\n",
       "      <td>[{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...</td>\n",
       "      <td>[{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>N86255</td>\n",
       "      <td>health</td>\n",
       "      <td>medical</td>\n",
       "      <td>Dispose of unwanted prescription drugs during ...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAISxPN.html</td>\n",
       "      <td>[{\"Label\": \"Drug Enforcement Administration\", ...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N93187</td>\n",
       "      <td>news</td>\n",
       "      <td>newsworld</td>\n",
       "      <td>The Cost of Trump's Aid Freeze in the Trenches...</td>\n",
       "      <td>Lt. Ivan Molchanets peeked over a parapet of s...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAJgNsz.html</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{\"Label\": \"Ukraine\", \"Type\": \"G\", \"WikidataId...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>N75236</td>\n",
       "      <td>health</td>\n",
       "      <td>voices</td>\n",
       "      <td>I Was An NBA Wife. Here's How It Affected My M...</td>\n",
       "      <td>I felt like I was a fraud, and being an NBA wi...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AACk2N6.html</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{\"Label\": \"National Basketball Association\", ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      did        cat          sub_cat  \\\n",
       "0  N88753  lifestyle  lifestyleroyals   \n",
       "1  N23144     health       weightloss   \n",
       "2  N86255     health          medical   \n",
       "3  N93187       news        newsworld   \n",
       "4  N75236     health           voices   \n",
       "\n",
       "                                               title  \\\n",
       "0  The Brands Queen Elizabeth, Prince Charles, an...   \n",
       "1                      50 Worst Habits For Belly Fat   \n",
       "2  Dispose of unwanted prescription drugs during ...   \n",
       "3  The Cost of Trump's Aid Freeze in the Trenches...   \n",
       "4  I Was An NBA Wife. Here's How It Affected My M...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  Shop the notebooks, jackets, and more that the...   \n",
       "1  These seemingly harmless habits are holding yo...   \n",
       "2                                               <NA>   \n",
       "3  Lt. Ivan Molchanets peeked over a parapet of s...   \n",
       "4  I felt like I was a fraud, and being an NBA wi...   \n",
       "\n",
       "                                             url  \\\n",
       "0  https://assets.msn.com/labs/mind/AAGH0ET.html   \n",
       "1  https://assets.msn.com/labs/mind/AAB19MK.html   \n",
       "2  https://assets.msn.com/labs/mind/AAISxPN.html   \n",
       "3  https://assets.msn.com/labs/mind/AAJgNsz.html   \n",
       "4  https://assets.msn.com/labs/mind/AACk2N6.html   \n",
       "\n",
       "                                      title_entities  \\\n",
       "0  [{\"Label\": \"Prince Philip, Duke of Edinburgh\",...   \n",
       "1  [{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...   \n",
       "2  [{\"Label\": \"Drug Enforcement Administration\", ...   \n",
       "3                                                 []   \n",
       "4                                                 []   \n",
       "\n",
       "                                   abstract_entities  \n",
       "0                                                 []  \n",
       "1  [{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...  \n",
       "2                                                 []  \n",
       "3  [{\"Label\": \"Ukraine\", \"Type\": \"G\", \"WikidataId...  \n",
       "4  [{\"Label\": \"National Basketball Association\", ...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_valid = cudf.read_csv(os.path.join(data_path_valid , 'news.tsv'), \n",
    "                          header=None, \n",
    "                          names=news_columns,\n",
    "                    sep='\\t',)\n",
    "news_valid.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 : Initial pre-processing to prepare dataset for feature engineering\n",
    "\n",
    "Before we use the data in NVTabular for pre-processing and feature engineering, we have to make a few changes to make it efficient for GPU operations.<br>\n",
    "The changes we have to make in the behaviours data file are:\n",
    "   - The history column is a long string and not a list. NVTabular support multi-hot categorical features but HugeCTR parquet reader does not. Thus we need to extend the dataframe with multiple history columns, capturing each element in this long string. While extending the history columns, we have to make sure we pick the most recent history (in reverse chronological order).\n",
    "\n",
    "\n",
    "  - The impression column contains a long string of unique negative and positive values for the same impression event. Each of these unique values in this column is a data point for our model to learn from. Thus, these unique positive & negative entries should be unrolled into multiple rows. The row expansion operation is not supported in NVtabular and hence we're going to perform it with cuDF.\n",
    "\n",
    "As for the news data file, we would just be using the news id, category and sub-category columns.<br>\n",
    "Their are many ways to use the other columns (title, abstract, entities etc.) as features but we would leave it up to you to explore those.\n",
    "\n",
    "In a nutshell, we are going to take the raw downloaded dataset, do these basic pre-processing using cuDF, generate a new train dataset which will then be used for further processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process 1: Drop columns from the news dataset\n",
    "\n",
    "The columns that we would drop from the news.tsv are: 'title', 'abstract', 'url', 'title_entities', 'abstract_entities'\n",
    "\n",
    "We encourage you to explore using 'title_entities' and 'abstract_entities' as categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>did</th>\n",
       "      <th>cat</th>\n",
       "      <th>sub_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N1</td>\n",
       "      <td>sports</td>\n",
       "      <td>football_nfl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>N100</td>\n",
       "      <td>finance</td>\n",
       "      <td>markets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>N1000</td>\n",
       "      <td>weather</td>\n",
       "      <td>weathertopstories</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N10000</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>celebrity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>N100000</td>\n",
       "      <td>sports</td>\n",
       "      <td>football_nfl</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       did            cat            sub_cat\n",
       "0       N1         sports       football_nfl\n",
       "1     N100        finance            markets\n",
       "2    N1000        weather  weathertopstories\n",
       "3   N10000  entertainment          celebrity\n",
       "4  N100000         sports       football_nfl"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_train = news_train.drop(['title', 'abstract', 'url', 'title_entities', 'abstract_entities'],axis = 1)\n",
    "news_valid = news_valid.drop(['title', 'abstract', 'url', 'title_entities', 'abstract_entities'],axis = 1)\n",
    "\n",
    "# Merging news train/valid dataset to have a single view of news and their attributes\n",
    "news = cudf.concat([news_train,news_valid]).drop_duplicates().reset_index().drop(['index'],axis=1)\n",
    "\n",
    "# Freeing up memory by nulling the variables\n",
    "news_train = None\n",
    "news_valid = None\n",
    "\n",
    "news.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Pre-process 2:  Label encoding for categorical variables\n",
    "\n",
    "Strings require significant amount of memory as compared to integers. As an example, representing the string `cfcd208495d565ef66e7dff9f98764` as integer `0` can save upto 90% memory.\n",
    "\n",
    "Thus, we would be label encoding the categorical variables in our dataset so that the downstream pre-preprocessing and feature engineering pipelines doesn't consume a high amount of memory.<br>\n",
    "We will also label encode low cardinality columns in news.tsv like the news_categories and news_subcategories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750434/750434 [00:03<00:00, 207351.53it/s]\n"
     ]
    }
   ],
   "source": [
    "# Encoding user id from both train and validation dataframe\n",
    "user_index = {} \n",
    "\n",
    "temp = cudf.concat([behaviors_train['uid'],behaviors_valid['uid']]).unique().to_pandas() \n",
    "for i in tqdm(range(len(temp)),total = len(temp)):\n",
    "    user_index[temp[i]] = i + 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing uid in the dataset with their respective indexes\n",
    "\n",
    "behaviors_train['uid'] = behaviors_train['uid'].replace([i for i in user_index],[str(user_index[i]) for i in user_index]).astype('int')\n",
    "behaviors_valid['uid'] = behaviors_valid['uid'].replace([i for i in user_index],[str(user_index[i]) for i in user_index]).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 104151/104151 [00:09<00:00, 10495.89it/s]\n"
     ]
    }
   ],
   "source": [
    "# Encoding news id from the combined news dataframe\n",
    "news_index = {}\n",
    "\n",
    "for n,data in tqdm(news.to_pandas().iterrows(),total = len(news)):\n",
    "    news_index[data['did']] = n + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:00<00:00, 54.26it/s]\n",
      "100%|██████████| 285/285 [00:00<00:00, 357.35it/s]\n"
     ]
    }
   ],
   "source": [
    "# Encoding new's category and subcategories\n",
    "cat = {}\n",
    "subcat = {}\n",
    "\n",
    "temp = news['cat'].unique()\n",
    "for i in tqdm(range(len(temp)),total = len(temp)):\n",
    "    cat[temp[i]] = i + 1\n",
    "    \n",
    "temp = news['sub_cat'].unique()\n",
    "for i in tqdm(range(len(temp)),total = len(temp)):\n",
    "    subcat[temp[i]] = i + 1\n",
    "\n",
    "# Replacing did, cat and sub_cate with their respective indexes in the news dataframe\n",
    "news = news.replace({'did': [i for i in news_index], 'cat': [i for i in cat], 'sub_cat': [i for i in subcat]},{'did': [str(news_index[i]) for i in news_index], 'cat': [str(cat[i]) for i in cat], 'sub_cat': [str(subcat[i]) for i in subcat]}).astype('int')\n",
    "news = news.set_index('did').to_pandas().T.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will replace the news id with their corresponding news_index in the behaviours dataframe in the pre-process step 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Pre-process 3: Unroll items in history column\n",
    "\n",
    "As an example, consider the below row in behaviours dataframe\n",
    "\n",
    "|impression_id | uid | time | history | impressions |\n",
    "| :-: | :-: | :-: |:-: |:-: |\n",
    "| 1 | U64099 | 11/19/2019 11:37:45 AM |\tN121133 N104200 N43255 N55860 N128965 N38014 | N78206-0 N26368-1 N7578-1 N58592-0 N19858-0 |\n",
    "\n",
    "We have to convert one history column with many news id to multiple history columns with single news id. \n",
    "\n",
    "| hist_0 | hist_1 | hist_2 | hist_3 | hist_4 | hist_5 |\n",
    "| :-: | :-: | :-: | :-: | :-: | :-: |\n",
    "|\tN121133 | N104200 | N43255 | N55860 | N128965 | N38014 |\n",
    "\n",
    "Finally, we will add the news category and subcategory for these news ids. The row after these transformations would look like this:\n",
    "\n",
    "|impression_id | uid | time | hist_cat_0 | hist_cat_1 | hist_cat_2 | ... | hist_subcat_3 | hist_subcat_4 | hist_subcat_5 | impressions |\n",
    "| :-: | :-: | :-: |:-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |\n",
    "| 1 | U64099 | 11/19/2019 11:37:45 AM |\tsports | finance | entertainment | ... | markets | celebrity | football_nfl | N78206-0 N26368-1 N7578-1 N58592-0 N19858-0 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observed that the maximum number of items in the history column is 400 & the mean is 30.<br>\n",
    "We have to unroll the same number of history items for each row and thus would define a variable that will control this number. Feel free to increase this number to include more items.\n",
    "\n",
    "For this tutorial, `max_hist` i.e. the number of history columns to be unrolled is set to 10. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_hist = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets expand the history column into individual columns of histories with the limit `max_hist`. \n",
    "During expansion, we will use the last `max_hist` items from history column as those items would be the most recent ones (since the news id in this column is ordered by time).\n",
    "\n",
    "In addition, we're also saving the length of history in a seperate column which could be used as a feature too.\n",
    "\n",
    "We will also replace the news id with their news indexes in the behaviours dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a new gdf for storing history\n",
    "hist = cudf.DataFrame() \n",
    "\n",
    "# Splitting the long string of history into several columns\n",
    "hist[['hist_'+str(i) for i in range(max_hist)]] = behaviors_train.history.str.rsplit(n=max_hist,expand=True).fillna(0)[[i for i in range(1,max_hist+1)]]\n",
    "\n",
    "# Replacing string news id in history with respective indexes\n",
    "hist = hist.replace([i for i in news_index],[str(news_index[i]) for i in news_index]).astype('int')\n",
    "\n",
    "# Appending news category corresponding to these newly created history columns\n",
    "behaviors_train[['hist_cat_'+str(i) for i in range(max_hist)]] = hist.replace([int(i) for i in news],[int(news[i]['cat']) for i in news])\n",
    "\n",
    "# Appending news sub-category corresponding to these newly created history columns\n",
    "behaviors_train[['hist_subcat_'+str(i) for i in range(max_hist)]] = hist.replace([int(i) for i in news],[int(news[i]['sub_cat']) for i in news])\n",
    "\n",
    "# Creating a column for the length of history \n",
    "behaviors_train['hist_count'] = behaviors_train.history.str.count(\" \")+1\n",
    "\n",
    "# Dropping the long string history column\n",
    "behaviors_train = behaviors_train.drop(['history'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeating the same for validation set\n",
    "hist = cudf.DataFrame()\n",
    "\n",
    "hist[['hist_'+str(i) for i in range(max_hist)]] = behaviors_valid.history.str.rsplit(n=max_hist,expand=True).fillna(0)[[i for i in range(1,max_hist+1)]]\n",
    "\n",
    "hist = hist.replace([i for i in news_index],[str(news_index[i]) for i in news_index]).astype('int')\n",
    "\n",
    "behaviors_valid[['hist_cat_'+str(i) for i in range(max_hist)]] = hist.replace([int(i) for i in news],[int(news[i]['cat']) for i in news])\n",
    "\n",
    "behaviors_valid[['hist_subcat_'+str(i) for i in range(max_hist)]] = hist.replace([int(i) for i in news],[int(news[i]['sub_cat']) for i in news])\n",
    "\n",
    "behaviors_valid['hist_count'] = behaviors_valid.history.str.count(\" \")+1\n",
    "\n",
    "behaviors_valid = behaviors_valid.drop(['history'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process 4 : Unroll items in impression column\n",
    "\n",
    "As an example, consider the below expanded history column row from the behaviours dataframe:\n",
    "\n",
    "|impression_id | uid | time | hist_cat_0 | hist_cat_1 | hist_cat_2 | ... | hist_subcat_3 | hist_subcat_4 | hist_subcat_5 | impressions |\n",
    "| :-: | :-: | :-: |:-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |\n",
    "| 1 | U64099 | 11/19/2019 11:37:45 AM |\tsports | finance | entertainment | ... | markets | celebrity | football_nfl | N78206-0 N26368-1 N7578-1 N58592-0 N19858-0 |\n",
    "\n",
    "The impression column contains the positive and negetive samples as a long string.\n",
    "\n",
    "After unrolling one row of impressions into multiple rows, the resulting dataframe would look like this:\n",
    "\n",
    "|impression_id | uid | time | hist_cat_0 | hist_cat_1 | hist_cat_2 | ... | hist_subcat_3 | hist_subcat_4 | hist_subcat_5 | impressions | label |\n",
    "| :-: | :-: | :-: |:-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |\n",
    "| 1 | U64099 | 11/19/2019 11:37:45 AM |\tsports | finance | entertainment | ... | markets | celebrity | football_nfl | N78206 | 0 |\n",
    "| 1 | U64099 | 11/19/2019 11:37:45 AM |\tsports | finance | entertainment | ... | markets | celebrity | football_nfl | N26368 | 1 |\n",
    "| 1 | U64099 | 11/19/2019 11:37:45 AM |\tsports | finance | entertainment | ... | markets | celebrity | football_nfl | N7578 | 1 |\n",
    "| 1 | U64099 | 11/19/2019 11:37:45 AM |\tsports | finance | entertainment | ... | markets | celebrity | football_nfl | N58592 | 0 |\n",
    "| 1 | U64099 | 11/19/2019 11:37:45 AM |\tsports | finance | entertainment | ... | markets | celebrity | football_nfl | N19858 | 0 |\n",
    "\n",
    "Note that all the 5 generated rows have the same impression_id, uid, time and history data columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have observed that the maximum number of items in impression column is 105 and the mean is 40. <br>\n",
    "We will limit the items to unroll from impression column by defining the variable `max_impr` and set it to 100. Feel free to increase or decrease this value.\n",
    "\n",
    "**Note** - Make sure you're using a GPU with atleast 16GB memory to avoid OOM errors with the below set values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_impr = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Row expansion is a memory and I/O intensive operation thus, we will perform it in 2 steps. We will first create a dictionary with impression-label and later merge it with the train set.\n",
    "\n",
    "Let's convert impression column as dictionary of list with impression id as key and the impression items as value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For train dataset\n",
    "impr_train = behaviors_train.set_index('impression_id').impressions.to_pandas().str.split()\n",
    "impr_train = impr_train.to_dict()\n",
    "behaviors_train = behaviors_train.drop(['impressions'],axis=1)\n",
    "\n",
    "# For validation dataset\n",
    "impr_valid = behaviors_valid.set_index('impression_id').impressions.to_pandas().str.split()\n",
    "impr_valid = impr_valid.to_dict()\n",
    "behaviors_valid = behaviors_valid.drop(['impressions'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the number of negative samples (labelled with 0) are greater than the positive samples, we can define a ratio between the negatives and positives to sampling a balanced distribution. <br>\n",
    "For now, let's set this variable to -1 to include all the samples from the impression column. Feel free to set this variable to a value greater than 1 to downsample the negative samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_ratio = -1 # ratio of neg-to-pos samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterating over the above dictionary to create a new dataframe with individual impression news in a new row with its corresponding label.<br>\n",
    "This is a time consuming operation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2232748/2232748 [07:20<00:00, 5064.69it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imp_id</th>\n",
       "      <th>label</th>\n",
       "      <th>impr_cat</th>\n",
       "      <th>impr_subcat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>84769</td>\n",
       "      <td>84769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>38819</td>\n",
       "      <td>38819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>82639</td>\n",
       "      <td>82639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>67404</td>\n",
       "      <td>67404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>32985</td>\n",
       "      <td>32985</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   imp_id label impr_cat impr_subcat\n",
       "0       1     0    84769       84769\n",
       "1       1     0    38819       38819\n",
       "2       1     0    82639       82639\n",
       "3       1     0    67404       67404\n",
       "4       1     0    32985       32985"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For train set\n",
    "\n",
    "imp_id = []\n",
    "imp_list = []\n",
    "imp_label = []\n",
    "for i in tqdm(impr_train,total = len(impr_train)):\n",
    "    imp, label = np.transpose([[news_index[imp.split('-')[0]],imp.split('-')[1]] for imp in impr_train[i]])\n",
    "    pos = (label == '1').sum()\n",
    "    neg = 0\n",
    "    for j in range(min(len(imp),max_impr)):\n",
    "        if label[j] == '0' and np_ratio > -1:\n",
    "            if neg <= pos*np_ratio :\n",
    "                imp_id.append(i)\n",
    "                imp_list.append(imp[j])\n",
    "                imp_label.append(label[j])\n",
    "                neg+=1\n",
    "        else:\n",
    "            imp_id.append(i)\n",
    "            imp_list.append(imp[j])\n",
    "            imp_label.append(label[j])\n",
    "\n",
    "impr_train = None \n",
    "\n",
    "# Creating a new gdf with impression id, news id and its label\n",
    "impressions_train = cudf.DataFrame({'imp_id': imp_id,'impr': imp_list,'label': imp_label})\n",
    "\n",
    "# Appending news category corresponding to above impression news in the above created DataFrame\n",
    "impressions_train['impr_cat'] = impressions_train['impr'].replace([int(i) for i in news],[int(news[i]['cat']) for i in news])\n",
    "\n",
    "# Appending news sub-category corresponding to above impression news in above created DataFrame\n",
    "impressions_train['impr_subcat'] = impressions_train['impr'].replace([int(i) for i in news],[int(news[i]['sub_cat']) for i in news])\n",
    "\n",
    "# Droping impr columns as news data is added for it.\n",
    "impressions_train = impressions_train.drop(['impr'],axis=1)\n",
    "\n",
    "impressions_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 376471/376471 [01:14<00:00, 5078.98it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imp_id</th>\n",
       "      <th>label</th>\n",
       "      <th>impr_cat</th>\n",
       "      <th>impr_subcat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>96774</td>\n",
       "      <td>96774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>42250</td>\n",
       "      <td>42250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>63649</td>\n",
       "      <td>63649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>15784</td>\n",
       "      <td>15784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>31476</td>\n",
       "      <td>31476</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   imp_id label impr_cat impr_subcat\n",
       "0       1     0    96774       96774\n",
       "1       1     0    42250       42250\n",
       "2       1     0    63649       63649\n",
       "3       1     0    15784       15784\n",
       "4       1     0    31476       31476"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For validation set\n",
    "\n",
    "imp_id = []\n",
    "imp_list = []\n",
    "imp_label = []\n",
    "for i in tqdm(impr_valid,total = len(impr_valid)):\n",
    "    imp, label = np.transpose([[news_index[imp.split('-')[0]],imp.split('-')[1]] for imp in impr_valid[i]])\n",
    "    pos = (label == '1').sum()\n",
    "    neg = 0\n",
    "    for j in range(min(len(imp),max_impr)):\n",
    "        if label[j] == '0' and np_ratio > -1:\n",
    "            if neg <= pos*np_ratio :\n",
    "                imp_id.append(i)\n",
    "                imp_list.append(imp[j])\n",
    "                imp_label.append(label[j])\n",
    "                neg+=1\n",
    "        else:\n",
    "            imp_id.append(i)\n",
    "            imp_list.append(imp[j])\n",
    "            imp_label.append(label[j])\n",
    "\n",
    "impr_valid = None \n",
    "\n",
    "impressions_valid = cudf.DataFrame({'imp_id': imp_id,'impr': imp_list,'label': imp_label})\n",
    "impressions_valid['impr_cat'] = impressions_valid['impr'].replace([int(i) for i in news],[int(news[i]['cat']) for i in news])\n",
    "impressions_valid['impr_subcat'] = impressions_valid['impr'].replace([int(i) for i in news],[int(news[i]['sub_cat']) for i in news])\n",
    "impressions_valid = impressions_valid.drop(['impr'],axis=1)\n",
    "\n",
    "impressions_valid.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process 5: Merge behaviour and news datasets \n",
    "\n",
    "Collating all the required columns from both behaviours and news dataset would make the feature engineering process much more faster. \n",
    "\n",
    "We will merge the history columns (from behaviors dataframe) with the above created impression data and save it as a parquet file. <br>\n",
    "We will also re-initialize RMM to allow us to perform memory intensive merge operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For training set\n",
    "rmm.reinitialize(managed_memory=True)\n",
    "\n",
    "final_data = impressions_train.merge(behaviors_train,left_on = ['imp_id'],right_on = ['impression_id']).drop(['imp_id'],axis=1)\n",
    "final_data = cudf.concat([final_data.drop(['time'],axis=1).astype('int'),final_data['time']],axis=1)\n",
    "final_data.to_parquet(os.path.join(data_input_path, 'train.parquet'), compression = None)\n",
    "\n",
    "#client.run(_rmm_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For validation set\n",
    "rmm.reinitialize(managed_memory=True)\n",
    "\n",
    "final_data = impressions_valid.merge(behaviors_valid,left_on = ['imp_id'],right_on = ['impression_id']).drop(['imp_id'],axis=1)\n",
    "final_data = cudf.concat([final_data.drop(['time'],axis=1).astype('int'),final_data['time']],axis=1)\n",
    "final_data.to_parquet(os.path.join(data_input_path, 'valid.parquet'),compression = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we have our initial pre-processed data - **train.parquet** and **valid.parquet** - that would be used for feature engineering and further processing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Feature Engineering - time-based features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started with NVTabular, we'll first use it for creating simple time based features that would be extracted from the timestamp column in the behaviours data. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaring features of train set that we created\n",
    "\n",
    "cat_features = [\n",
    " 'hist_cat_0',\n",
    " 'hist_subcat_0',\n",
    " 'hist_cat_1',\n",
    " 'hist_subcat_1',\n",
    " 'hist_cat_2',\n",
    " 'hist_subcat_2',\n",
    " 'hist_cat_3',\n",
    " 'hist_subcat_3',\n",
    " 'hist_cat_4',\n",
    " 'hist_subcat_4',\n",
    " 'hist_cat_5',\n",
    " 'hist_subcat_5',\n",
    " 'hist_cat_6',\n",
    " 'hist_subcat_6',\n",
    " 'hist_cat_7',\n",
    " 'hist_subcat_7',\n",
    " 'hist_cat_8',\n",
    " 'hist_subcat_8',\n",
    " 'hist_cat_9',\n",
    " 'hist_subcat_9',\n",
    " 'impr_cat',\n",
    " 'impr_subcat',\n",
    " 'impression_id',\n",
    " 'uid']\n",
    "\n",
    "cont_features = ['hist_count']\n",
    "\n",
    "labels = ['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating time based features by extracting the relevant elements using cuDF\n",
    "\n",
    "datetime = nvt.ColumnGroup(['time']) >> (lambda col: cudf.to_datetime(col,format=\"%m/%d/%Y %I:%M:%S %p\"))\n",
    "\n",
    "hour = datetime >> (lambda col: col.dt.hour) >> nvt.ops.Rename(postfix = '_hour')\n",
    "minute = datetime >> (lambda col: col.dt.minute) >> nvt.ops.Rename(postfix = '_minute')\n",
    "seconds = datetime >> (lambda col: col.dt.second) >> nvt.ops.Rename(postfix = '_second')\n",
    "\n",
    "weekday = datetime >> (lambda col: col.dt.weekday) >> nvt.ops.Rename(postfix = '_wd')\n",
    "day = datetime >> (lambda col: cudf.to_datetime(col, unit='s').dt.day) >> nvt.ops.Rename(postfix = '_day')\n",
    "\n",
    "week = day >> (lambda col: (col/7).floor().astype('int')) >> nvt.ops.Rename(postfix = '_week')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create embedding tables and segregate the pre-processing functions (normalization, fill missing values etc.) among categorical and continous features, we define and pipeline them using NVTabular's operator overloading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = cat_features + hour + minute + seconds + weekday + day + week + datetime >> nvt.ops.Categorify(out_path = data_output_path)\n",
    "cont_features = cont_features >> nvt.ops.FillMissing() >> nvt.ops.NormalizeMinMax()\n",
    "labels = ['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the complete workflow pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ExecutableNotFound",
     "evalue": "failed to execute ['dot', '-Kdot', '-Tsvg'], make sure the Graphviz executables are on your systems' PATH",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/graphviz/backend.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(cmd, input, capture_output, check, encoding, quiet, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m         \u001b[0mproc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstartupinfo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_startupinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[1;32m    853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 854\u001b[0;31m             self._execute_child(args, executable, preexec_fn, close_fds,\n\u001b[0m\u001b[1;32m    855\u001b[0m                                 \u001b[0mpass_fds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\u001b[0m\n\u001b[1;32m   1701\u001b[0m                         \u001b[0merr_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1702\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1703\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dot'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mExecutableNotFound\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/graphviz/files.py\u001b[0m in \u001b[0;36m_repr_svg_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_repr_svg_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'svg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquiet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/graphviz/files.py\u001b[0m in \u001b[0;36mpipe\u001b[0;34m(self, format, renderer, formatter, quiet)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m         out = backend.pipe(self._engine, format, data,\n\u001b[0m\u001b[1;32m    170\u001b[0m                            \u001b[0mrenderer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformatter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m                            quiet=quiet)\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/graphviz/backend.py\u001b[0m in \u001b[0;36mpipe\u001b[0;34m(engine, format, data, renderer, formatter, quiet)\u001b[0m\n\u001b[1;32m    246\u001b[0m     \"\"\"\n\u001b[1;32m    247\u001b[0m     \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcommand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m     \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcapture_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquiet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquiet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/graphviz/backend.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(cmd, input, capture_output, check, encoding, quiet, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrno\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0merrno\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mENOENT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mExecutableNotFound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mExecutableNotFound\u001b[0m: failed to execute ['dot', '-Kdot', '-Tsvg'], make sure the Graphviz executables are on your systems' PATH"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7f3f64e80910>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = cat_features + cont_features\n",
    "output.graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this graph, we would create a workflow object that calculates statistics and performs the relevant transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc = nvt.Workflow(cat_features + cont_features + labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a nvt.Dataset from parquet file that was created in step 4.\n",
    "\n",
    "data_train = nvt.Dataset(os.path.join(data_input_path, \"train.parquet\"), engine=\"parquet\",part_size=\"256MB\")\n",
    "data_valid = nvt.Dataset(os.path.join(data_input_path, \"valid.parquet\"), engine=\"parquet\",part_size=\"256MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are going to train the DNNs using HugeCTR, we need to conform to the following dtypes:\n",
    "\n",
    "- categorical feature columns in int64\n",
    "- continuous feature columns in float32\n",
    "- label columns in float32\n",
    "    \n",
    "We will make a dictionary containing names of columns as key and the required datatype as value. This dictionary will be used by NVTabular for type casting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_dtypes={}\n",
    "\n",
    "for col in cat_features.columns:\n",
    "    dict_dtypes[col] = np.int64\n",
    "\n",
    "for col in cont_features.columns:\n",
    "    dict_dtypes[col] = np.float32\n",
    "\n",
    "for col in labels:\n",
    "    dict_dtypes[col] = np.float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fit the workflow on the training set to record the statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 49s, sys: 23.6 s, total: 2min 13s\n",
      "Wall time: 2min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "proc.fit(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we apply the transformation to the dataset and persist it to disk as parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 46s, sys: 50 s, total: 2min 36s\n",
      "Wall time: 2min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# For training set\n",
    "proc.transform(data_train).to_parquet(output_path= output_train_path,\n",
    "                                shuffle=nvt.io.Shuffle.PER_PARTITION,\n",
    "                                dtypes=dict_dtypes,\n",
    "                                out_files_per_proc=10,\n",
    "                                cats = cat_features.columns,\n",
    "                                conts = cont_features.columns,\n",
    "                                labels = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.2 s, sys: 7.03 s, total: 23.2 s\n",
      "Wall time: 22.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# For validation set\n",
    "proc.transform(data_valid).to_parquet(output_path= output_valid_path,\n",
    "                                shuffle=nvt.io.Shuffle.PER_PARTITION,\n",
    "                                dtypes=dict_dtypes,\n",
    "                                out_files_per_proc=10,\n",
    "                                cats = cat_features.columns,\n",
    "                                conts = cont_features.columns,\n",
    "                                labels = labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the NVTabular processed parquet files and look at our first NVTabular pre-processed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_hour</th>\n",
       "      <th>hist_cat_0</th>\n",
       "      <th>hist_subcat_0</th>\n",
       "      <th>hist_cat_1</th>\n",
       "      <th>hist_subcat_1</th>\n",
       "      <th>hist_cat_2</th>\n",
       "      <th>hist_subcat_2</th>\n",
       "      <th>hist_cat_3</th>\n",
       "      <th>hist_subcat_3</th>\n",
       "      <th>hist_cat_4</th>\n",
       "      <th>...</th>\n",
       "      <th>impression_id</th>\n",
       "      <th>uid</th>\n",
       "      <th>time_minute</th>\n",
       "      <th>time_second</th>\n",
       "      <th>time_wd</th>\n",
       "      <th>time_day</th>\n",
       "      <th>time_day_week</th>\n",
       "      <th>time</th>\n",
       "      <th>hist_count</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>108</td>\n",
       "      <td>3</td>\n",
       "      <td>56</td>\n",
       "      <td>9</td>\n",
       "      <td>108</td>\n",
       "      <td>10</td>\n",
       "      <td>144</td>\n",
       "      <td>17</td>\n",
       "      <td>...</td>\n",
       "      <td>2146</td>\n",
       "      <td>352684</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>67921</td>\n",
       "      <td>0.016230</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>143</td>\n",
       "      <td>12</td>\n",
       "      <td>159</td>\n",
       "      <td>14</td>\n",
       "      <td>87</td>\n",
       "      <td>12</td>\n",
       "      <td>157</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>3165</td>\n",
       "      <td>255108</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>31343</td>\n",
       "      <td>0.038702</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>63</td>\n",
       "      <td>4</td>\n",
       "      <td>184</td>\n",
       "      <td>15</td>\n",
       "      <td>211</td>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>2009</td>\n",
       "      <td>401055</td>\n",
       "      <td>52</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>45137</td>\n",
       "      <td>0.076155</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>158</td>\n",
       "      <td>12</td>\n",
       "      <td>173</td>\n",
       "      <td>16</td>\n",
       "      <td>222</td>\n",
       "      <td>12</td>\n",
       "      <td>171</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>1635</td>\n",
       "      <td>232239</td>\n",
       "      <td>46</td>\n",
       "      <td>48</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.119850</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>172</td>\n",
       "      <td>13</td>\n",
       "      <td>87</td>\n",
       "      <td>14</td>\n",
       "      <td>87</td>\n",
       "      <td>13</td>\n",
       "      <td>30</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>2636</td>\n",
       "      <td>122099</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>49783</td>\n",
       "      <td>0.014981</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   time_hour  hist_cat_0  hist_subcat_0  hist_cat_1  hist_subcat_1  \\\n",
       "0         10           8            108           3             56   \n",
       "1         13          10            143          12            159   \n",
       "2          3           4             63           4            184   \n",
       "3          3          12            158          12            173   \n",
       "4         10          12            172          13             87   \n",
       "\n",
       "   hist_cat_2  hist_subcat_2  hist_cat_3  hist_subcat_3  hist_cat_4  ...  \\\n",
       "0           9            108          10            144          17  ...   \n",
       "1          14             87          12            157           4  ...   \n",
       "2          15            211           4             70          12  ...   \n",
       "3          16            222          12            171           8  ...   \n",
       "4          14             87          13             30           6  ...   \n",
       "\n",
       "   impression_id     uid  time_minute  time_second  time_wd  time_day  \\\n",
       "0           2146  352684            6            9        3         5   \n",
       "1           3165  255108            2            5        1         3   \n",
       "2           2009  401055           52           16        2         4   \n",
       "3           1635  232239           46           48        4         1   \n",
       "4           2636  122099            3            2        2         4   \n",
       "\n",
       "   time_day_week   time  hist_count  label  \n",
       "0              1  67921    0.016230    0.0  \n",
       "1              1  31343    0.038702    0.0  \n",
       "2              1  45137    0.076155    0.0  \n",
       "3              1      0    0.119850    0.0  \n",
       "4              1  49783    0.014981    0.0  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = dask_cudf.read_parquet(os.path.join(output_train_path, '*.parquet'))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After transformation and persisting the data on the disk, the following files will be created:\n",
    "   1. parquet\n",
    "       - The number of parquet files depends on `out_files_per_proc` in `proc_train.transform()` \n",
    "   2. _file_list.txt\n",
    "       - The 1st line contains the number of parquet files\n",
    "       - The subsequent lines are the paths to each parquet file.\n",
    "   3. _metadata.json\n",
    "       - This file is used by HugeCTR in parsing the processed parquet files.\n",
    "       - 'file_stats' contains the name of the parquet files and their corresponding number of rows.\n",
    "       - 'cats' is a list of categorical features/columns in the dataset and their index.\n",
    "       - 'conts' is a list of continous/dense columns in the dataset and their index.\n",
    "       - 'labels' is a list of labels in the dataset and their index.\n",
    "       - This file shouldn't be edited manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the contents of _metadata.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file_stats': [{'file_name': '0.ee88d3947f75411ebecea519a8d84e1e.parquet',\n",
       "   'num_rows': 7714011},\n",
       "  {'file_name': '1.7b819cdbe9b74b8d8e8d70a82c5592bd.parquet',\n",
       "   'num_rows': 7716568},\n",
       "  {'file_name': '2.96ff5b25178442e1a0afe5a043bddb4e.parquet',\n",
       "   'num_rows': 7719289},\n",
       "  {'file_name': '3.239a792c3e4a452cbd670fe0dcf7f7d9.parquet',\n",
       "   'num_rows': 7717958},\n",
       "  {'file_name': '4.48ae392a00604b37b3c1a30396c6e7ed.parquet',\n",
       "   'num_rows': 7713252},\n",
       "  {'file_name': '5.540012b6dd0941e2a5717557fc389fbc.parquet',\n",
       "   'num_rows': 7714906},\n",
       "  {'file_name': '6.85b26a9252c44eae8276210f5dfffe74.parquet',\n",
       "   'num_rows': 7719046},\n",
       "  {'file_name': '7.b32bd793c054446fb135709f3e18ef72.parquet',\n",
       "   'num_rows': 7723206},\n",
       "  {'file_name': '8.6f58be3da99a49038adfbc79d4bfddb9.parquet',\n",
       "   'num_rows': 7719582},\n",
       "  {'file_name': '9.8749db3725574f7d8d710885d411b8e6.parquet',\n",
       "   'num_rows': 7717609}],\n",
       " 'cats': [{'col_name': 'time_hour', 'index': 0},\n",
       "  {'col_name': 'hist_cat_0', 'index': 1},\n",
       "  {'col_name': 'hist_subcat_0', 'index': 2},\n",
       "  {'col_name': 'hist_cat_1', 'index': 3},\n",
       "  {'col_name': 'hist_subcat_1', 'index': 4},\n",
       "  {'col_name': 'hist_cat_2', 'index': 5},\n",
       "  {'col_name': 'hist_subcat_2', 'index': 6},\n",
       "  {'col_name': 'hist_cat_3', 'index': 7},\n",
       "  {'col_name': 'hist_subcat_3', 'index': 8},\n",
       "  {'col_name': 'hist_cat_4', 'index': 9},\n",
       "  {'col_name': 'hist_subcat_4', 'index': 10},\n",
       "  {'col_name': 'hist_cat_5', 'index': 11},\n",
       "  {'col_name': 'hist_subcat_5', 'index': 12},\n",
       "  {'col_name': 'hist_cat_6', 'index': 13},\n",
       "  {'col_name': 'hist_subcat_6', 'index': 14},\n",
       "  {'col_name': 'hist_cat_7', 'index': 15},\n",
       "  {'col_name': 'hist_subcat_7', 'index': 16},\n",
       "  {'col_name': 'hist_cat_8', 'index': 17},\n",
       "  {'col_name': 'hist_subcat_8', 'index': 18},\n",
       "  {'col_name': 'hist_cat_9', 'index': 19},\n",
       "  {'col_name': 'hist_subcat_9', 'index': 20},\n",
       "  {'col_name': 'impr_cat', 'index': 21},\n",
       "  {'col_name': 'impr_subcat', 'index': 22},\n",
       "  {'col_name': 'impression_id', 'index': 23},\n",
       "  {'col_name': 'uid', 'index': 24},\n",
       "  {'col_name': 'time_minute', 'index': 25},\n",
       "  {'col_name': 'time_second', 'index': 26},\n",
       "  {'col_name': 'time_wd', 'index': 27},\n",
       "  {'col_name': 'time_day', 'index': 28},\n",
       "  {'col_name': 'time_day_week', 'index': 29},\n",
       "  {'col_name': 'time', 'index': 30}],\n",
       " 'conts': [{'col_name': 'hist_count', 'index': 31}],\n",
       " 'labels': [{'col_name': 'label', 'index': 32}]}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(os.path.join(output_train_path, '_metadata.json'),'r') as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to get the embedding size for the categorical variables. This will be an important input for defining the embedding table size to be used by HugeCTR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hist_cat_0': (18, 16),\n",
       " 'hist_cat_1': (18, 16),\n",
       " 'hist_cat_2': (19, 16),\n",
       " 'hist_cat_3': (18, 16),\n",
       " 'hist_cat_4': (18, 16),\n",
       " 'hist_cat_5': (18, 16),\n",
       " 'hist_cat_6': (18, 16),\n",
       " 'hist_cat_7': (18, 16),\n",
       " 'hist_cat_8': (17, 16),\n",
       " 'hist_cat_9': (17, 16),\n",
       " 'hist_subcat_0': (235, 34),\n",
       " 'hist_subcat_1': (239, 34),\n",
       " 'hist_subcat_2': (236, 34),\n",
       " 'hist_subcat_3': (235, 34),\n",
       " 'hist_subcat_4': (229, 34),\n",
       " 'hist_subcat_5': (224, 33),\n",
       " 'hist_subcat_6': (225, 33),\n",
       " 'hist_subcat_7': (219, 33),\n",
       " 'hist_subcat_8': (213, 32),\n",
       " 'hist_subcat_9': (199, 31),\n",
       " 'impr_cat': (26708, 482),\n",
       " 'impr_subcat': (26708, 482),\n",
       " 'impression_id': (2232749, 512),\n",
       " 'time': (90397, 512),\n",
       " 'time_day': (7, 16),\n",
       " 'time_day_week': (3, 16),\n",
       " 'time_hour': (16, 16),\n",
       " 'time_minute': (61, 16),\n",
       " 'time_second': (61, 16),\n",
       " 'time_wd': (6, 16),\n",
       " 'uid': (711223, 512)}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nvtabular.ops import get_embedding_sizes\n",
    "embeddings_simple_time = get_embedding_sizes(proc)\n",
    "embeddings_simple_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[16,\n",
       " 18,\n",
       " 235,\n",
       " 18,\n",
       " 239,\n",
       " 19,\n",
       " 236,\n",
       " 18,\n",
       " 235,\n",
       " 18,\n",
       " 229,\n",
       " 18,\n",
       " 224,\n",
       " 18,\n",
       " 225,\n",
       " 18,\n",
       " 219,\n",
       " 17,\n",
       " 213,\n",
       " 17,\n",
       " 199,\n",
       " 26708,\n",
       " 26708,\n",
       " 2232749,\n",
       " 711223,\n",
       " 61,\n",
       " 61,\n",
       " 6,\n",
       " 7,\n",
       " 3,\n",
       " 90397]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reformatting the above output for ease of copy paste in HugeCTRs config.json\n",
    "\n",
    "embedding_size_str_simple_time = [embeddings_simple_time[x][0] for x in cat_features.columns]\n",
    "embedding_size_str_simple_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check the name of the categorical and continuous features that we've defined. This should match with the cats and conts dictionaries in the _metadata.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['time_hour',\n",
       " 'hist_cat_0',\n",
       " 'hist_subcat_0',\n",
       " 'hist_cat_1',\n",
       " 'hist_subcat_1',\n",
       " 'hist_cat_2',\n",
       " 'hist_subcat_2',\n",
       " 'hist_cat_3',\n",
       " 'hist_subcat_3',\n",
       " 'hist_cat_4',\n",
       " 'hist_subcat_4',\n",
       " 'hist_cat_5',\n",
       " 'hist_subcat_5',\n",
       " 'hist_cat_6',\n",
       " 'hist_subcat_6',\n",
       " 'hist_cat_7',\n",
       " 'hist_subcat_7',\n",
       " 'hist_cat_8',\n",
       " 'hist_subcat_8',\n",
       " 'hist_cat_9',\n",
       " 'hist_subcat_9',\n",
       " 'impr_cat',\n",
       " 'impr_subcat',\n",
       " 'impression_id',\n",
       " 'uid',\n",
       " 'time_minute',\n",
       " 'time_second',\n",
       " 'time_wd',\n",
       " 'time_day',\n",
       " 'time_day_week',\n",
       " 'time']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hist_count']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cont_features.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving on to training a DNN, let's try few complex feature engineering techniques using NVTabular. We would later train DNNs on both these feature engineered dataset and compare their performances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Feature Engineering - count and target encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now perform count and target encoding on the processed dataset generated in step 4. Let's start by defining directories for the input dataset and the output processed dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define our worker/output directories\n",
    "dask_workdir = os.path.join(BASE_DIR, \"workdir\")\n",
    "\n",
    "# Mapping our processed_nvt output directories as input directories for new workflow.\n",
    "data_input_path = os.path.join(BASE_DIR, \"dataset\")\n",
    "\n",
    "# Defining new directories for output\n",
    "data_output_path = os.path.join(BASE_DIR, \"processed_ce-te\")\n",
    "output_train_path = os.path.join(data_output_path, \"train\")\n",
    "output_valid_path = os.path.join(data_output_path, \"valid\")\n",
    "\n",
    "# Creating and cleaning our worker/output directories\n",
    "try:\n",
    "    # Ensure BASE_DIR exists\n",
    "    if not os.path.isdir(BASE_DIR):\n",
    "        os.mkdir(BASE_DIR)\n",
    "        \n",
    "    # Make sure we have a clean worker space for Dask\n",
    "    if os.path.isdir(dask_workdir):\n",
    "        shutil.rmtree(dask_workdir)\n",
    "    os.mkdir(dask_workdir)\n",
    "\n",
    "    # Make sure we have a clean output path for our new dataset\n",
    "    if os.path.isdir(data_output_path):\n",
    "        shutil.rmtree(data_output_path)\n",
    "        \n",
    "    os.mkdir(data_output_path)\n",
    "    os.mkdir(output_train_path)\n",
    "    os.mkdir(output_valid_path)\n",
    "\n",
    "except OSError:\n",
    "    print (\"Creation of the directories failed\")\n",
    "else:\n",
    "    print (\"Successfully created the directories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you would observe, we have created a new directory by the name `processed_ce-te`. The complete directory structure now is:\n",
    "\n",
    "basedir <br>\n",
    "&emsp; |--- workdir    \n",
    "&emsp; |--- dataset <br>\n",
    "&emsp; &emsp; |--- train <br>\n",
    "&emsp; &emsp; |--- valid  <br>\n",
    "&emsp; |--- processed_nvt <br>\n",
    "&emsp;  &emsp; |--- train <br>\n",
    "&emsp;  &emsp; |--- valid  <br>\n",
    "&emsp; |--- processed_ce-te <br>\n",
    "&emsp;  &emsp; |--- train <br>\n",
    "&emsp;  &emsp; |--- valid  <br>\n",
    "&emsp; |--- configs <br>\n",
    "&emsp; |--- weights <br>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, defining the categorical and continous features based on processed data generated in step-4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = ['hist_cat_0',\n",
    " 'hist_subcat_0',\n",
    " 'hist_cat_1',\n",
    " 'hist_subcat_1',\n",
    " 'hist_cat_2',\n",
    " 'hist_subcat_2',\n",
    " 'hist_cat_3',\n",
    " 'hist_subcat_3',\n",
    " 'hist_cat_4',\n",
    " 'hist_subcat_4',\n",
    " 'hist_cat_5',\n",
    " 'hist_subcat_5',\n",
    " 'hist_cat_6',\n",
    " 'hist_subcat_6',\n",
    " 'hist_cat_7',\n",
    " 'hist_subcat_7',\n",
    " 'hist_cat_8',\n",
    " 'hist_subcat_8',\n",
    " 'hist_cat_9',\n",
    " 'hist_subcat_9',\n",
    " 'impr_cat',\n",
    " 'impr_subcat',\n",
    " 'impression_id',\n",
    " 'uid',]\n",
    "\n",
    "cont_features = ['hist_count']\n",
    "\n",
    "labels = ['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Count Encoding** calculates the frequency of one or more categorical features. For the purpose of this tutorial, we will count how often the user had clicked on news with the same category/sub-category in a given impression.\n",
    "\n",
    "To calculate the occurence of the same news category/sub-category in history, we will iterate over the group of rows with the same impression id. We will also consider the category/sub-category of the impression news.<br>\n",
    "Let's start by defining supportive functions for counting the category and subcategory from history columns. This supportive function will be used by `apply_rows()` in LambdaOp `create_count_features`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also limit the number of history columns to be considered for count encoding. For now, let's use all the history columns that we have in the dataset i.e. all 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_hist = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_cat_count(\n",
    "         hist_cat_0,\n",
    "         hist_cat_1,\n",
    "         hist_cat_2,\n",
    "         hist_cat_3,\n",
    "         hist_cat_4,\n",
    "         hist_cat_5,\n",
    "         hist_cat_6,\n",
    "         hist_cat_7,\n",
    "         hist_cat_8,\n",
    "         hist_cat_9,\n",
    "         impr_cat,\n",
    "         impr_cat_count,\n",
    "         k):\n",
    "    \n",
    "    # Following loop iterates over each row of columns hist_cat_0->9 and impr_cat\n",
    "    for i, temp in enumerate(zip(hist_cat_0,\n",
    "                                 hist_cat_1,\n",
    "                                 hist_cat_2,\n",
    "                                 hist_cat_3,\n",
    "                                 hist_cat_4,\n",
    "                                 hist_cat_5,\n",
    "                                 hist_cat_6,\n",
    "                                 hist_cat_7,\n",
    "                                 hist_cat_8,\n",
    "                                 hist_cat_9,\n",
    "                                 impr_cat,\n",
    "                                )):\n",
    "        \n",
    "        # Iterate over each column and check if history category matches with impression category.\n",
    "        for j in temp[:-1]:\n",
    "            if j == temp[-1]:\n",
    "                k += 1\n",
    "        \n",
    "        # Update the count in the corresponding row of output column (impr_cat_count)\n",
    "        impr_cat_count[i] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_subcat_count(\n",
    "         hist_subcat_0,\n",
    "         hist_subcat_1,\n",
    "         hist_subcat_2,\n",
    "         hist_subcat_3,\n",
    "         hist_subcat_4,\n",
    "         hist_subcat_5,\n",
    "         hist_subcat_6,\n",
    "         hist_subcat_7,\n",
    "         hist_subcat_8,\n",
    "         hist_subcat_9,\n",
    "         impr_subcat,\n",
    "         impr_subcat_count,\n",
    "         k):\n",
    "    \n",
    "    # Following loop iterates over each row of columns hist_subcat_0->9 and impr_cat\n",
    "    for i, temp in enumerate(zip(\n",
    "                                 hist_subcat_0,\n",
    "                                 hist_subcat_1,\n",
    "                                 hist_subcat_2,\n",
    "                                 hist_subcat_3,\n",
    "                                 hist_subcat_4,\n",
    "                                 hist_subcat_5,\n",
    "                                 hist_subcat_6,\n",
    "                                 hist_subcat_7,\n",
    "                                 hist_subcat_8,\n",
    "                                 hist_subcat_9,\n",
    "                                 impr_subcat,\n",
    "                                )):\n",
    "\n",
    "        # Iterate over each column and check if history sub-category matches with impression sub-category.\n",
    "        for j in temp[:-1]:\n",
    "            if j == temp[-1]:\n",
    "                k += 1      \n",
    "                \n",
    "        # Update the count(occurence) in corresponding row of output column (impr_cat_count)        \n",
    "        impr_subcat_count[i] = k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add the count encoding for 'categories' and 'sub_categories' to each row for their corresponding news_id, we will write a LambdaOp by simply inhereting from NVTabular's `Operator` class and defining the `transform` and `output_column_names` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class create_count_features(Operator):\n",
    "    def transform(self, columns, gdf):\n",
    "        if columns[-1] == 'impr_cat':\n",
    "            gdf = gdf.apply_rows(add_cat_count,incols = ['hist_cat_{}'.format(i) for i in range(max_hist)]+['impr_cat'],outcols = {'impr_cat_count': np.int64},kwargs={'k': 0})\n",
    "            return(gdf.drop(columns,axis=1))\n",
    "        if columns[-1] == 'impr_subcat':\n",
    "            gdf = gdf.apply_rows(add_subcat_count,incols = ['hist_subcat_{}'.format(i) for i in range(max_hist)]+['impr_subcat'],outcols = {'impr_subcat_count': np.int64},kwargs={'k': 0})\n",
    "            return(gdf.drop(columns,axis=1))\n",
    "\n",
    "    def output_column_names(self, columns):\n",
    "        col = []\n",
    "        if columns[-1] == 'impr_cat':\n",
    "            col.append('impr_cat_count')\n",
    "        if columns[-1] == 'impr_subcat':\n",
    "            col.append('impr_subcat_count')\n",
    "        return col\n",
    "\n",
    "    def dependencies(self):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Target encoding** is used to average the target value by some category/group. This technique is used to find numeric mean relationship between the categorical features and target.\n",
    "\n",
    "We have observed that the hist_cat columns are the most suitable for target encoding. Rather than using just 1 history category column, we also found that a group of history columns encode better probabilities with the target variable. \n",
    "\n",
    "For this tutorial, we are going use 5 history category columns, in a moving window fashion, along with the impression category column to calculate the target encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "te_columns = [['hist_cat_'+str(j) for j in range(i-5+1, i+1)] + ['impr_cat'] for i in range(4, max_hist)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_encode = (\n",
    "    te_columns >>\n",
    "    nvt.ops.TargetEncoding(\n",
    "        ['label'],\n",
    "        out_path = BASE_DIR,\n",
    "        kfold=5,\n",
    "        p_smooth=20,\n",
    "        out_dtype=\"float32\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also create the time based features in the same way as we did in step-5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime = nvt.ColumnGroup(['time']) >> (lambda col: cudf.to_datetime(col,format=\"%m/%d/%Y %I:%M:%S %p\"))\n",
    "\n",
    "hour = datetime >> (lambda col: col.dt.hour) >> nvt.ops.Rename(postfix = '_hour')\n",
    "minute = datetime >> (lambda col: col.dt.minute) >> nvt.ops.Rename(postfix = '_minute')\n",
    "seconds = datetime >> (lambda col: col.dt.second) >> nvt.ops.Rename(postfix = '_second')\n",
    "\n",
    "weekday = datetime >> (lambda col: col.dt.weekday) >> nvt.ops.Rename(postfix = '_wd')\n",
    "day = datetime >> (lambda col: cudf.to_datetime(col, unit='s').dt.day) >> nvt.ops.Rename(postfix = '_day')\n",
    "\n",
    "week = day >> (lambda col: (col/7).floor().astype('int')) >> nvt.ops.Rename(postfix = '_week')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_count_encode = ['hist_cat_{}'.format(i) for i in range(max_hist)] + ['impr_cat'] >> create_count_features()\n",
    "\n",
    "subcat_count_encode = ['hist_subcat_{}'.format(i) for i in range(max_hist)] + ['impr_subcat'] >> create_count_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = cat_features + datetime + hour + minute + seconds + weekday + day + week >> nvt.ops.Categorify(out_path = data_output_path)\n",
    "cont_features = cont_features + cat_count_encode + subcat_count_encode >> nvt.ops.FillMissing() >> nvt.ops.NormalizeMinMax()\n",
    "cont_features += target_encode >> nvt.ops.Rename(postfix = '_TE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the complete workflow pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ExecutableNotFound",
     "evalue": "failed to execute ['dot', '-Kdot', '-Tsvg'], make sure the Graphviz executables are on your systems' PATH",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/graphviz/backend.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(cmd, input, capture_output, check, encoding, quiet, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m         \u001b[0mproc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstartupinfo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_startupinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[1;32m    853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 854\u001b[0;31m             self._execute_child(args, executable, preexec_fn, close_fds,\n\u001b[0m\u001b[1;32m    855\u001b[0m                                 \u001b[0mpass_fds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\u001b[0m\n\u001b[1;32m   1701\u001b[0m                         \u001b[0merr_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1702\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1703\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dot'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mExecutableNotFound\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/graphviz/files.py\u001b[0m in \u001b[0;36m_repr_svg_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_repr_svg_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'svg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquiet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/graphviz/files.py\u001b[0m in \u001b[0;36mpipe\u001b[0;34m(self, format, renderer, formatter, quiet)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m         out = backend.pipe(self._engine, format, data,\n\u001b[0m\u001b[1;32m    170\u001b[0m                            \u001b[0mrenderer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformatter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m                            quiet=quiet)\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/graphviz/backend.py\u001b[0m in \u001b[0;36mpipe\u001b[0;34m(engine, format, data, renderer, formatter, quiet)\u001b[0m\n\u001b[1;32m    246\u001b[0m     \"\"\"\n\u001b[1;32m    247\u001b[0m     \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcommand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m     \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcapture_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquiet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquiet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/graphviz/backend.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(cmd, input, capture_output, check, encoding, quiet, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrno\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0merrno\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mENOENT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mExecutableNotFound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mExecutableNotFound\u001b[0m: failed to execute ['dot', '-Kdot', '-Tsvg'], make sure the Graphviz executables are on your systems' PATH"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7f3de4f240a0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = cat_features + cont_features\n",
    "output.graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc = nvt.Workflow(cat_features + cont_features + labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize a nvt.Dataset object from parquet dataset that was created in step 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = nvt.Dataset(os.path.join(data_input_path, \"train.parquet\"), engine=\"parquet\",part_size=\"256MB\")\n",
    "data_valid = nvt.Dataset(os.path.join(data_input_path, \"valid.parquet\"), engine=\"parquet\",part_size=\"256MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_dtypes={}\n",
    "\n",
    "for col in cat_features.columns:\n",
    "    dict_dtypes[col] = np.int64\n",
    "\n",
    "for col in cont_features.columns:\n",
    "    dict_dtypes[col] = np.float32\n",
    "\n",
    "for col in labels:\n",
    "    dict_dtypes[col] = np.float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fit the workflow on our training dataset to record the statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 49s, sys: 1min 29s, total: 6min 19s\n",
      "Wall time: 6min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "proc.fit(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39min 18s, sys: 34min 31s, total: 1h 13min 49s\n",
      "Wall time: 1h 12min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# For training set\n",
    "proc.transform(data_train).to_parquet(output_path=output_train_path,\n",
    "                                shuffle=nvt.io.Shuffle.PER_PARTITION,\n",
    "                                dtypes=dict_dtypes,\n",
    "                                out_files_per_proc=10,\n",
    "                                cats = cat_features.columns,\n",
    "                                conts = cont_features.columns,\n",
    "                                labels = labels)\n",
    "\n",
    "rmm.reinitialize(managed_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 35s, sys: 5min 45s, total: 12min 20s\n",
      "Wall time: 12min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# For validation set\n",
    "proc.transform(data_valid).to_parquet(output_path=output_valid_path,\n",
    "                                shuffle=nvt.io.Shuffle.PER_PARTITION,\n",
    "                                dtypes=dict_dtypes,\n",
    "                                out_files_per_proc=10,\n",
    "                                cats = cat_features.columns,\n",
    "                                conts = cont_features.columns,\n",
    "                                labels = labels)\n",
    "\n",
    "rmm.reinitialize(managed_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick look at the contents of _metadata.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file_stats': [{'file_name': '0.515824acdb824a4dad10e1443a742f4f.parquet',\n",
       "   'num_rows': 7717106},\n",
       "  {'file_name': '1.90aa8f40137340a6815f0af4a41724b0.parquet',\n",
       "   'num_rows': 7719406},\n",
       "  {'file_name': '2.b81c1cd3a59a46f196b758c89e940763.parquet',\n",
       "   'num_rows': 7717841},\n",
       "  {'file_name': '3.e8ee8aaacd2548398c2346aed4b45d28.parquet',\n",
       "   'num_rows': 7720591},\n",
       "  {'file_name': '4.e37c20946d274693895b3fd6592051ab.parquet',\n",
       "   'num_rows': 7715286},\n",
       "  {'file_name': '5.0ed0ef8ce7754beaa876d7dc98316a9b.parquet',\n",
       "   'num_rows': 7716759},\n",
       "  {'file_name': '6.b8d293674b2e4d529e3967a4f212acbb.parquet',\n",
       "   'num_rows': 7715976},\n",
       "  {'file_name': '7.af52633b932d49d18d50dbeeb0fb749e.parquet',\n",
       "   'num_rows': 7716664},\n",
       "  {'file_name': '8.bf7424d0fe5148c193e49a2c32f065a2.parquet',\n",
       "   'num_rows': 7724349},\n",
       "  {'file_name': '9.68efef93eb024e6f9678b952fcef33d2.parquet',\n",
       "   'num_rows': 7711449}],\n",
       " 'cats': [{'col_name': 'time', 'index': 0},\n",
       "  {'col_name': 'hist_cat_0', 'index': 1},\n",
       "  {'col_name': 'hist_subcat_0', 'index': 2},\n",
       "  {'col_name': 'hist_cat_1', 'index': 3},\n",
       "  {'col_name': 'hist_subcat_1', 'index': 4},\n",
       "  {'col_name': 'hist_cat_2', 'index': 5},\n",
       "  {'col_name': 'hist_subcat_2', 'index': 6},\n",
       "  {'col_name': 'hist_cat_3', 'index': 7},\n",
       "  {'col_name': 'hist_subcat_3', 'index': 8},\n",
       "  {'col_name': 'hist_cat_4', 'index': 9},\n",
       "  {'col_name': 'hist_subcat_4', 'index': 10},\n",
       "  {'col_name': 'hist_cat_5', 'index': 11},\n",
       "  {'col_name': 'hist_subcat_5', 'index': 12},\n",
       "  {'col_name': 'hist_cat_6', 'index': 13},\n",
       "  {'col_name': 'hist_subcat_6', 'index': 14},\n",
       "  {'col_name': 'hist_cat_7', 'index': 15},\n",
       "  {'col_name': 'hist_subcat_7', 'index': 16},\n",
       "  {'col_name': 'hist_cat_8', 'index': 17},\n",
       "  {'col_name': 'hist_subcat_8', 'index': 18},\n",
       "  {'col_name': 'hist_cat_9', 'index': 19},\n",
       "  {'col_name': 'hist_subcat_9', 'index': 20},\n",
       "  {'col_name': 'impr_cat', 'index': 21},\n",
       "  {'col_name': 'impr_subcat', 'index': 22},\n",
       "  {'col_name': 'impression_id', 'index': 23},\n",
       "  {'col_name': 'uid', 'index': 24},\n",
       "  {'col_name': 'time_hour', 'index': 25},\n",
       "  {'col_name': 'time_minute', 'index': 26},\n",
       "  {'col_name': 'time_second', 'index': 27},\n",
       "  {'col_name': 'time_wd', 'index': 28},\n",
       "  {'col_name': 'time_day', 'index': 29},\n",
       "  {'col_name': 'time_day_week', 'index': 30}],\n",
       " 'conts': [{'col_name': 'impr_cat_count', 'index': 31},\n",
       "  {'col_name': 'hist_count', 'index': 32},\n",
       "  {'col_name': 'impr_subcat_count', 'index': 33},\n",
       "  {'col_name': 'TE_hist_cat_0_hist_cat_1_hist_cat_2_hist_cat_3_hist_cat_4_impr_cat_label_TE',\n",
       "   'index': 34},\n",
       "  {'col_name': 'TE_hist_cat_1_hist_cat_2_hist_cat_3_hist_cat_4_hist_cat_5_impr_cat_label_TE',\n",
       "   'index': 35},\n",
       "  {'col_name': 'TE_hist_cat_2_hist_cat_3_hist_cat_4_hist_cat_5_hist_cat_6_impr_cat_label_TE',\n",
       "   'index': 36},\n",
       "  {'col_name': 'TE_hist_cat_3_hist_cat_4_hist_cat_5_hist_cat_6_hist_cat_7_impr_cat_label_TE',\n",
       "   'index': 37},\n",
       "  {'col_name': 'TE_hist_cat_4_hist_cat_5_hist_cat_6_hist_cat_7_hist_cat_8_impr_cat_label_TE',\n",
       "   'index': 38},\n",
       "  {'col_name': 'TE_hist_cat_5_hist_cat_6_hist_cat_7_hist_cat_8_hist_cat_9_impr_cat_label_TE',\n",
       "   'index': 39}],\n",
       " 'labels': [{'col_name': 'label', 'index': 40}]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(os.path.join(output_train_path, '_metadata.json'),'r') as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hist_cat_0': (18, 16),\n",
       " 'hist_cat_1': (18, 16),\n",
       " 'hist_cat_2': (19, 16),\n",
       " 'hist_cat_3': (18, 16),\n",
       " 'hist_cat_4': (18, 16),\n",
       " 'hist_cat_5': (18, 16),\n",
       " 'hist_cat_6': (18, 16),\n",
       " 'hist_cat_7': (18, 16),\n",
       " 'hist_cat_8': (17, 16),\n",
       " 'hist_cat_9': (17, 16),\n",
       " 'hist_subcat_0': (235, 34),\n",
       " 'hist_subcat_1': (239, 34),\n",
       " 'hist_subcat_2': (236, 34),\n",
       " 'hist_subcat_3': (235, 34),\n",
       " 'hist_subcat_4': (229, 34),\n",
       " 'hist_subcat_5': (224, 33),\n",
       " 'hist_subcat_6': (225, 33),\n",
       " 'hist_subcat_7': (219, 33),\n",
       " 'hist_subcat_8': (213, 32),\n",
       " 'hist_subcat_9': (199, 31),\n",
       " 'impr_cat': (26708, 482),\n",
       " 'impr_subcat': (26708, 482),\n",
       " 'impression_id': (2232749, 512),\n",
       " 'time': (90397, 512),\n",
       " 'time_day': (7, 16),\n",
       " 'time_day_week': (3, 16),\n",
       " 'time_hour': (16, 16),\n",
       " 'time_minute': (61, 16),\n",
       " 'time_second': (61, 16),\n",
       " 'time_wd': (6, 16),\n",
       " 'uid': (711223, 512)}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nvtabular.ops import get_embedding_sizes\n",
    "embeddings_count_encode =  get_embedding_sizes(proc)\n",
    "embeddings_count_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[90397,\n",
       " 18,\n",
       " 235,\n",
       " 18,\n",
       " 239,\n",
       " 19,\n",
       " 236,\n",
       " 18,\n",
       " 235,\n",
       " 18,\n",
       " 229,\n",
       " 18,\n",
       " 224,\n",
       " 18,\n",
       " 225,\n",
       " 18,\n",
       " 219,\n",
       " 17,\n",
       " 213,\n",
       " 17,\n",
       " 199,\n",
       " 26708,\n",
       " 26708,\n",
       " 2232749,\n",
       " 711223,\n",
       " 16,\n",
       " 61,\n",
       " 61,\n",
       " 6,\n",
       " 7,\n",
       " 3]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reformatting the above output for ease of copy paste in HugeCTRs config.json\n",
    "\n",
    "embedding_size_str_count_encode = [embeddings_count_encode[x][0] for x in cat_features.columns]\n",
    "embedding_size_str_count_encode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have 2 versions of our dataset ready, one with time based features and other with count + target encoded features, we can start training a few DNNs using HugeCTR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll shutdown our Dask client from earlier to free up some memory so that we can share it with HugeCTR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.shutdown()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Train DNN with HugeCTR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we would be training Deep Learning Recommendation Model (DLRM) using HugeCTR's high level python API. We would also be using the inference python API for evaluation on the validation set.\n",
    "\n",
    "We would be training 2 models, one each for the 2 datasets that we've processed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train configuration for simple time based features dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HugeCTR now offers a high-level, Keras-like python API suite for defining the model, layers, optimizer and executing training<br>\n",
    "As a first step, we will develop a train python file for our feature engineered dataset and DLRM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths to save the config and the weights\n",
    "\n",
    "train_file_path = os.path.join(config_output_path,'train_dlrm_fp32_simple-time_1gpu.py')\n",
    "weights_output_path = os.path.join(weights_path,'dlrm_fp32_simple-time_1gpu/')\n",
    "\n",
    "# Directory inside weights folder for saving weights of this training\n",
    "if os.path.isdir(weights_output_path):\n",
    "    shutil.rmtree(weights_output_path)\n",
    "os.mkdir(weights_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For using the HugeCTR's high level python API, we will follow [this documentation](https://github.com/NVIDIA/HugeCTR/blob/master/docs/python_interface.md) which is available on the github repository, and apply it for our dataset and model.\n",
    "\n",
    "The parameters that we should modify are:  \n",
    "\n",
    "- solver:\n",
    "        - max_iter: Num. of samples / batch size\n",
    "        - gpu: List of GPU IDs to use for training\n",
    "        - batchsize: Num. of samples to process in the batch training mode\n",
    "        - eval_interval: Num. of iterations after which evaluation should trigger on the validation set\n",
    "\n",
    "\n",
    "- optimizer:\n",
    "        - type: Adam \n",
    "        - learning_rate: 1e-4 (smaller value to begin with)\n",
    "\n",
    "\n",
    "- layers\n",
    "        - format: Parquet (since our dataset is in parquet)\n",
    "        - source and eval_source: Path to _file_list.txt for the train and eval dataset produced by NVTabular\n",
    "        - slot_num: For LocalizedSlot, set it to the number of categorical features\n",
    "        - max_feature_num_per_sample: For LocalizedSlot, this can be the same as slot_num\n",
    "        - slot_size_array: Cardinality of the categorical features (in the same order as column names in 'cats' dictionary of _metadata.json)\n",
    "        - embedding_vec_size: Dimension of the embedding vectors for the categorical features\n",
    "        - label_dim: Labels dimension\n",
    "        - dense_dim: Number of dense/continous features\n",
    "        - sparse: Dimensions of categorical features\n",
    "        - DLRM layer fc3: Output dimension of fc3 should be the same as embedding_vec_size\n",
    "         \n",
    "We've developed one such training python file below with the appropriate path to the dataset and default batch size for a 32GB GPU.<br>\n",
    "\n",
    "Let's make use of the data path and other variables we've defined in the steps above and re-define the ones which may have changed throughout the pre-processing step.\n",
    "The model graph can be saved into a JSON file by calling model.graph_to_json, which will be used for inference afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./basedir/configs/train_dlrm_fp32_count-target-encode_1gpu.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $train_file_path\n",
    "\n",
    "import os\n",
    "import hugectr\n",
    "from mpi4py import MPI\n",
    "\n",
    "# Define \"fast\" root directory for this example\n",
    "BASE_DIR = os.environ.get(\"BASE_DIR\", \"./basedir\")\n",
    "\n",
    "# Directory to load NVTabular's processed dataset\n",
    "data_input_path = os.path.join(BASE_DIR, \"processed_nvt\")\n",
    "input_train_path = os.path.join(data_input_path, \"train\")\n",
    "input_valid_path = os.path.join(data_input_path, \"valid\")\n",
    "\n",
    "# Directory to store HugeCTR's train weights\n",
    "config_output_path = os.path.join(BASE_DIR, \"configs\")\n",
    "weights_path = os.path.join(BASE_DIR, \"weights\")\n",
    "weights_output_path = os.path.join(weights_path,'dlrm_fp32_simple-time_1gpu/')\n",
    "\n",
    "# GPUs used for training\n",
    "NUM_GPUS = [0]\n",
    "\n",
    "# Model related parameter\n",
    "embedding_vec_size = 4                                             \n",
    "batchsize = 2048                                                   # Batch size used for training\n",
    "batchsize_eval = 2048                                              # Batch size used for evaluation\n",
    "max_eval_batchsize = 3768                                          # Iterations required to go through the complete validation set with the set batchsize_eval\n",
    "\n",
    "# Training related parameters\n",
    "num_iter = 30001                                                   # Iterations to train the model for\n",
    "eval_trigger = 10000                                               # Start evaluation after these iterations\n",
    "snapshot_trigger = 10000                                           # Save model checkpoints after these iterations\n",
    "\n",
    "# Input for slot size array\n",
    "embedding_size_str_simple_time = [16,18,235,18,239,19,236,18,235,18,229,18,224,18,225,18,219,17,213,17,199,26708,26708,2232749,\n",
    "                                    711223,61,61,6,7,3,90397]\n",
    "\n",
    "embeddings_simple_time ={'hist_cat_0': (18, 16),\n",
    " 'hist_cat_1': (18, 16),\n",
    " 'hist_cat_2': (19, 16),\n",
    " 'hist_cat_3': (18, 16),\n",
    " 'hist_cat_4': (18, 16),\n",
    " 'hist_cat_5': (18, 16),\n",
    " 'hist_cat_6': (18, 16),\n",
    " 'hist_cat_7': (18, 16),\n",
    " 'hist_cat_8': (17, 16),\n",
    " 'hist_cat_9': (17, 16),\n",
    " 'hist_subcat_0': (235, 34),\n",
    " 'hist_subcat_1': (239, 34),\n",
    " 'hist_subcat_2': (236, 34),\n",
    " 'hist_subcat_3': (235, 34),\n",
    " 'hist_subcat_4': (229, 34),\n",
    " 'hist_subcat_5': (224, 33),\n",
    " 'hist_subcat_6': (225, 33),\n",
    " 'hist_subcat_7': (219, 33),\n",
    " 'hist_subcat_8': (213, 32),\n",
    " 'hist_subcat_9': (199, 31),\n",
    " 'impr_cat': (26708, 482),\n",
    " 'impr_subcat': (26708, 482),\n",
    " 'impression_id': (2232749, 512),\n",
    " 'time': (90397, 512),\n",
    " 'time_day': (7, 16),\n",
    " 'time_day_week': (3, 16),\n",
    " 'time_hour': (16, 16),\n",
    " 'time_minute': (61, 16),\n",
    " 'time_second': (61, 16),\n",
    " 'time_wd': (6, 16),\n",
    " 'uid': (711223, 512)}\n",
    "\n",
    "## Creating the model\n",
    "\n",
    "solver = hugectr.CreateSolver(\n",
    "                              vvgpu = [NUM_GPUS],                       # GPU Indices to be used for training\n",
    "                              max_eval_batches = max_eval_batchsize,    # Max no. of eval batches on which eval will be done\n",
    "                              batchsize_eval = batchsize_eval,          # Minibatch size for eval\n",
    "                              batchsize = batchsize,                    # Minibatch size for training\n",
    "                              \n",
    "                              # learning rate parameters\n",
    "                              lr = 0.001,\n",
    "                              warmup_steps = 10000,\n",
    "                              decay_start = 20000,\n",
    "                              decay_steps = 200000,\n",
    "                              decay_power = 1,\n",
    "                              end_lr = 1e-06,\n",
    "                              \n",
    "                              # traning setting\n",
    "                              i64_input_key = True,                     # As we are using Parquet from NVTabular, I64 should be true\n",
    "                              repeat_dataset = True,                    # Repeat the dataset for training loop, True for Non Epoch Based Training\n",
    "                              use_mixed_precision = False,              # Flag to indicate use of Mixed precision training\n",
    "                              use_cuda_graph = False,                   # cuda graph for forward and back proppogation\n",
    "                              use_algorithm_search = False,             # algo search within the fc-layers\n",
    "                              )\n",
    "\n",
    "# create datareader object\n",
    "reader = hugectr.DataReaderParams(\n",
    "    \n",
    "    data_reader_type = hugectr.DataReaderType_t.Parquet,                # Dataset format selection: Parquet\n",
    "    source = [input_train_path+\"/_file_list.txt\"],                     # path to file list of processed NVT Train Dataset, can be used to input various file lists\n",
    "    eval_source = input_valid_path+\"/_file_list.txt\",                  # path to file list of processed NVT Valid Dataset\n",
    "    check_type = hugectr.Check_t.Non,                                   # data error detection turned off\n",
    "    slot_size_array = embedding_size_str_simple_time,                  # embedding slot array size list which is obtained from NVT operations\n",
    "    \n",
    "    )\n",
    "\n",
    "# create optimiser\n",
    "optimizer = hugectr.CreateOptimizer(\n",
    "        optimizer_type = hugectr.Optimizer_t.Adam, # Select optimizer type : Adam\n",
    "        update_type = hugectr.Update_t.Local,      # Select update type : Local\n",
    "        epsilon = 1e-07,                           # Adam parameter\n",
    "        beta1 = 0.9,                               # Adam parameter\n",
    "        beta2 = 0.999,                             # Adam parameter\n",
    "        atomic_update = False,                     # Atomic update for SGD, we are using adam,so set to false\n",
    ")\n",
    "\n",
    "# create a hugectr model object\n",
    "model = hugectr.Model(solver, reader, optimizer)\n",
    "\n",
    "# adding layers to the model\n",
    "\n",
    "model.add(hugectr.Input(\n",
    "                        # label parameters\n",
    "                        label_name = \"label\",                                 # Name of the Label column in the dataset\n",
    "                        label_dim = 1,                                        # label dimension, 1 for binary label based dataset. Can be customised according to users need\n",
    "                        # continous feature parameters\n",
    "                        dense_dim = 1,                                        # total number of dense (continous) features\n",
    "                        dense_name = \"dense\",                                 # Name of the dense (continuous) features\n",
    "                        # Sparse parameters for categorial inputs\n",
    "                        data_reader_sparse_param_array = [hugectr.DataReaderSparseParam(\n",
    "                            hugectr.DataReaderSparse_t.Localized,             # selecting localised or distributed reading of data\n",
    "                            max_feature_num = len(embeddings_simple_time) ,   # total number of features in the dataset\n",
    "                            max_nnz = 1,                                      # set 1 for one hot label dataset\n",
    "                            slot_num = len(embeddings_simple_time),           # total number of slots used for this sparse input in the dataset\n",
    "                        )],\n",
    "                        sparse_names = [\"data1\"]                              #list of names of the sparse input tensors to be referenced by following layers\n",
    "                        ))\n",
    "\n",
    "# adding sparse layer\n",
    "model.add(hugectr.SparseEmbedding(\n",
    "                            embedding_type = hugectr.Embedding_t.LocalizedSlotSparseEmbeddingHash,\n",
    "                            max_vocabulary_size_per_gpu = 10000000,           # maximum vocabulary size or cardinality across all the input features (can be calculated from embedding size list)\n",
    "                            embedding_vec_size = embedding_vec_size,          # model parameter\n",
    "                            combiner = 0,                                     # intra-slot reduction operation (0=sum, 1=average)\n",
    "                            sparse_embedding_name = \"sparse_embedding1\",      #name of the sparse embedding tensor to be referenced by following layers\n",
    "                            bottom_name = \"data1\",\n",
    "                            optimizer = optimizer))\n",
    "\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"dense\"],\n",
    "                            top_names = [\"fc1\"],\n",
    "                            num_output=512))\n",
    "\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc1\"],\n",
    "                            top_names = [\"relu1\"]))\n",
    "\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"relu1\"],\n",
    "                            top_names = [\"fc2\"],\n",
    "                            num_output=256))\n",
    "\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc2\"],\n",
    "                            top_names = [\"relu2\"]))\n",
    "\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"relu2\"],\n",
    "                            top_names = [\"fc3\"],\n",
    "                            num_output=embedding_vec_size))\n",
    "\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc3\"],\n",
    "                            top_names = [\"relu3\"]))\n",
    "\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Interaction,\n",
    "                            bottom_names = [\"relu3\",\"sparse_embedding1\"],\n",
    "                            top_names = [\"interaction1\"]))\n",
    "\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"interaction1\"],\n",
    "                            top_names = [\"fc4\"],\n",
    "                            num_output=1024))\n",
    "\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc4\"],\n",
    "                            top_names = [\"relu4\"]))\n",
    "\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"relu4\"],\n",
    "                            top_names = [\"fc5\"],\n",
    "                            num_output=1024))\n",
    "\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc5\"],\n",
    "                            top_names = [\"relu5\"]))\n",
    "\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"relu5\"],\n",
    "                            top_names = [\"fc6\"],\n",
    "                            num_output=512))\n",
    "\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc6\"],\n",
    "                            top_names = [\"relu6\"]))\n",
    "\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"relu6\"],\n",
    "                            top_names = [\"fc7\"],\n",
    "                            num_output=256))\n",
    "\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc7\"],\n",
    "                            top_names = [\"relu7\"]))\n",
    "\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"relu7\"],\n",
    "                            top_names = [\"fc8\"],\n",
    "                            num_output=1))\n",
    "\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.BinaryCrossEntropyLoss,\n",
    "                            bottom_names = [\"fc8\", \"label\"],\n",
    "                            top_names = [\"loss\"]))\n",
    "\n",
    "model.compile()\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.graph_to_json(graph_config_file = os.path.join(config_output_path,\"dlrm_fp32_simple-time_1gpu.json\") )\n",
    "\n",
    "model.fit(\n",
    "          max_iter = num_iter ,                     # maximum number of iterations\n",
    "          display = 1000,                           # display stats after no. of iterations\n",
    "          eval_interval = eval_trigger,             # interval for performing evaluation\n",
    "          snapshot = snapshot_trigger,              # interval after which model snapshots will be taken\n",
    "          snapshot_prefix = weights_output_path     # path for saving weights\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to train a DLRM model on simple time feature dataset with HugeCTR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================Model Init=====================================================\n",
      "[23d07h11m16s][HUGECTR][INFO]: Global seed is 3862588489\n",
      "Device 0: Tesla V100-SXM2-16GB\n",
      "Device 1: Tesla V100-SXM2-16GB\n",
      "[23d07h11m18s][HUGECTR][INFO]: num of DataReader workers: 2\n",
      "[23d07h11m18s][HUGECTR][INFO]: num_internal_buffers 1\n",
      "[23d07h11m18s][HUGECTR][INFO]: num_internal_buffers 1\n",
      "[23d07h11m18s][HUGECTR][INFO]: Vocabulary size: 3090372\n",
      "[23d07h11m18s][HUGECTR][INFO]: max_vocabulary_size_per_gpu_=10000000\n",
      "[23d07h11m18s][HUGECTR][INFO]: All2All Warmup Start\n",
      "[23d07h11m18s][HUGECTR][INFO]: All2All Warmup End\n",
      "===================================================Model Compile===================================================\n",
      "[23d07h11m18s][HUGECTR][INFO]: gpu0 start to init embedding\n",
      "[23d07h11m18s][HUGECTR][INFO]: gpu1 start to init embedding\n",
      "[23d07h11m18s][HUGECTR][INFO]: gpu0 init embedding done\n",
      "[23d07h11m18s][HUGECTR][INFO]: gpu1 init embedding done\n",
      "===================================================Model Summary===================================================\n",
      "Label                                   Dense                         Sparse                        \n",
      "label                                   dense                          data1                         \n",
      "(None, 1)                               (None, 1)                               \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Layer Type                              Input Name                    Output Name                   Output Shape                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "LocalizedSlotSparseEmbeddingHash        data1                         sparse_embedding1             (None, 31, 4)                 \n",
      "InnerProduct                            dense                         fc1                           (None, 512)                   \n",
      "ReLU                                    fc1                           relu1                         (None, 512)                   \n",
      "InnerProduct                            relu1                         fc2                           (None, 256)                   \n",
      "ReLU                                    fc2                           relu2                         (None, 256)                   \n",
      "InnerProduct                            relu2                         fc3                           (None, 4)                     \n",
      "ReLU                                    fc3                           relu3                         (None, 4)                     \n",
      "Interaction                             relu3,sparse_embedding1       interaction1                  (None, 501)                   \n",
      "InnerProduct                            interaction1                  fc4                           (None, 1024)                  \n",
      "ReLU                                    fc4                           relu4                         (None, 1024)                  \n",
      "InnerProduct                            relu4                         fc5                           (None, 1024)                  \n",
      "ReLU                                    fc5                           relu5                         (None, 1024)                  \n",
      "InnerProduct                            relu5                         fc6                           (None, 512)                   \n",
      "ReLU                                    fc6                           relu6                         (None, 512)                   \n",
      "InnerProduct                            relu6                         fc7                           (None, 256)                   \n",
      "ReLU                                    fc7                           relu7                         (None, 256)                   \n",
      "InnerProduct                            relu7                         fc8                           (None, 1)                     \n",
      "BinaryCrossEntropyLoss                  fc8,label                     loss                                                        \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "[23d70h11m19s][HUGECTR][INFO]: Save the model graph to ./basedir/configs/dlrm_fp32_simple-time_1gpu.json, successful\n",
      "=====================================================Model Fit=====================================================\n",
      "[23d70h11m19s][HUGECTR][INFO]: Use non-epoch mode with number of iterations: 30001\n",
      "[23d70h11m19s][HUGECTR][INFO]: Training batchsize: 2048, evaluation batchsize: 2048\n",
      "[23d70h11m19s][HUGECTR][INFO]: Evaluation interval: 10000, snapshot interval: 10000\n",
      "[23d70h11m19s][HUGECTR][INFO]: Sparse embedding trainable: 1, dense network trainable: 1\n",
      "[23d70h11m19s][HUGECTR][INFO]: Use mixed precision: 0, scaler: 1.000000, use cuda graph: 0\n",
      "[23d70h11m19s][HUGECTR][INFO]: lr: 0.001000, warmup_steps: 10000, decay_start: 20000, decay_steps: 200000, decay_power: 1.000000, end_lr: 0.000001\n",
      "[23d70h11m19s][HUGECTR][INFO]: Training source file: ./basedir/processed_nvt/train/_file_list.txt\n",
      "[23d70h11m19s][HUGECTR][INFO]: Evaluation source file: ./basedir/processed_nvt/valid/_file_list.txt\n",
      "[23d70h11m22s][HUGECTR][INFO]: Iter: 1000 Time(1000 iters): 3.148147s Loss: 0.169052 lr:0.000100\n",
      "[23d70h11m25s][HUGECTR][INFO]: Iter: 2000 Time(1000 iters): 2.842924s Loss: 0.163499 lr:0.000200\n",
      "[23d70h11m28s][HUGECTR][INFO]: Iter: 3000 Time(1000 iters): 2.849511s Loss: 0.148275 lr:0.000300\n",
      "[23d70h11m30s][HUGECTR][INFO]: Iter: 4000 Time(1000 iters): 2.858828s Loss: 0.166277 lr:0.000400\n",
      "[23d70h11m33s][HUGECTR][INFO]: Iter: 5000 Time(1000 iters): 2.849095s Loss: 0.167650 lr:0.000500\n",
      "[23d70h11m36s][HUGECTR][INFO]: Iter: 6000 Time(1000 iters): 2.838695s Loss: 0.180689 lr:0.000600\n",
      "[23d70h11m39s][HUGECTR][INFO]: Iter: 7000 Time(1000 iters): 2.840885s Loss: 0.187179 lr:0.000700\n",
      "[23d70h11m42s][HUGECTR][INFO]: Iter: 8000 Time(1000 iters): 2.852413s Loss: 0.171740 lr:0.000800\n",
      "[23d70h11m45s][HUGECTR][INFO]: Iter: 9000 Time(1000 iters): 2.838460s Loss: 0.160823 lr:0.000900\n",
      "[23d70h11m47s][HUGECTR][INFO]: Iter: 10000 Time(1000 iters): 2.821659s Loss: 0.151937 lr:0.001000\n",
      "[23d70h11m54s][HUGECTR][INFO]: Evaluation, AUC: 0.640359\n",
      "[23d70h11m54s][HUGECTR][INFO]: Eval Time for 3768 iters: 6.086037s\n",
      "[HCDEBUG][ERROR] Runtime error: Error: file not open for writing /raid/ashish/hugectr/HugeCTR/src/embeddings/localized_slot_sparse_embedding_hash.cu:659 \n",
      "\n",
      "[23d70h11m54s][HUGECTR][INFO]: Dumping dense weights to file, successful\n",
      "[23d70h11m54s][HUGECTR][INFO]: Dumping dense optimizer states to file, successful\n",
      "[23d70h11m54s][HUGECTR][INFO]: Dumping untrainable weights to file, successful\n",
      "[23d70h11m56s][HUGECTR][INFO]: Iter: 11000 Time(1000 iters): 8.943657s Loss: 0.139087 lr:0.001000\n",
      "[23d70h11m59s][HUGECTR][INFO]: Iter: 12000 Time(1000 iters): 2.808265s Loss: 0.172807 lr:0.001000\n",
      "[23d70h12m20s][HUGECTR][INFO]: Iter: 13000 Time(1000 iters): 2.810484s Loss: 0.175422 lr:0.001000\n",
      "[23d70h12m50s][HUGECTR][INFO]: Iter: 14000 Time(1000 iters): 2.827208s Loss: 0.157402 lr:0.001000\n",
      "[23d70h12m80s][HUGECTR][INFO]: Iter: 15000 Time(1000 iters): 2.847038s Loss: 0.157846 lr:0.001000\n",
      "[23d70h12m10s][HUGECTR][INFO]: Iter: 16000 Time(1000 iters): 2.824519s Loss: 0.139494 lr:0.001000\n",
      "[23d70h12m13s][HUGECTR][INFO]: Iter: 17000 Time(1000 iters): 2.820706s Loss: 0.163341 lr:0.001000\n",
      "[23d70h12m16s][HUGECTR][INFO]: Iter: 18000 Time(1000 iters): 2.819033s Loss: 0.143452 lr:0.001000\n",
      "[23d70h12m19s][HUGECTR][INFO]: Iter: 19000 Time(1000 iters): 2.830976s Loss: 0.190055 lr:0.001000\n",
      "[23d70h12m22s][HUGECTR][INFO]: Iter: 20000 Time(1000 iters): 2.829234s Loss: 0.162570 lr:0.001000\n",
      "[23d70h12m28s][HUGECTR][INFO]: Evaluation, AUC: 0.659378\n",
      "[23d70h12m28s][HUGECTR][INFO]: Eval Time for 3768 iters: 6.111610s\n",
      "[HCDEBUG][ERROR] Runtime error: Error: file not open for writing /raid/ashish/hugectr/HugeCTR/src/embeddings/localized_slot_sparse_embedding_hash.cu:659 \n",
      "\n",
      "[23d70h12m28s][HUGECTR][INFO]: Dumping dense weights to file, successful\n",
      "[23d70h12m28s][HUGECTR][INFO]: Dumping dense optimizer states to file, successful\n",
      "[23d70h12m28s][HUGECTR][INFO]: Dumping untrainable weights to file, successful\n",
      "[23d70h12m31s][HUGECTR][INFO]: Iter: 21000 Time(1000 iters): 8.974913s Loss: 0.151020 lr:0.000995\n",
      "[23d70h12m34s][HUGECTR][INFO]: Iter: 22000 Time(1000 iters): 2.828258s Loss: 0.149087 lr:0.000990\n",
      "[23d70h12m36s][HUGECTR][INFO]: Iter: 23000 Time(1000 iters): 2.839758s Loss: 0.167663 lr:0.000985\n",
      "[23d70h12m39s][HUGECTR][INFO]: Iter: 24000 Time(1000 iters): 2.825893s Loss: 0.140056 lr:0.000980\n",
      "[23d70h12m42s][HUGECTR][INFO]: Iter: 25000 Time(1000 iters): 2.826714s Loss: 0.143033 lr:0.000975\n",
      "[23d70h12m45s][HUGECTR][INFO]: Iter: 26000 Time(1000 iters): 2.827719s Loss: 0.168889 lr:0.000970\n",
      "[23d70h12m48s][HUGECTR][INFO]: Iter: 27000 Time(1000 iters): 2.833992s Loss: 0.150375 lr:0.000965\n",
      "[23d70h12m51s][HUGECTR][INFO]: Iter: 28000 Time(1000 iters): 2.826019s Loss: 0.127328 lr:0.000960\n",
      "[23d70h12m53s][HUGECTR][INFO]: Iter: 29000 Time(1000 iters): 2.827171s Loss: 0.147030 lr:0.000955\n",
      "[23d70h12m56s][HUGECTR][INFO]: Iter: 30000 Time(1000 iters): 2.834218s Loss: 0.171532 lr:0.000950\n",
      "[23d70h13m20s][HUGECTR][INFO]: Evaluation, AUC: 0.664657\n",
      "[23d70h13m20s][HUGECTR][INFO]: Eval Time for 3768 iters: 6.248412s\n",
      "[HCDEBUG][ERROR] Runtime error: Error: file not open for writing /raid/ashish/hugectr/HugeCTR/src/embeddings/localized_slot_sparse_embedding_hash.cu:659 \n",
      "\n",
      "[23d70h13m20s][HUGECTR][INFO]: Dumping dense weights to file, successful\n",
      "[23d70h13m30s][HUGECTR][INFO]: Dumping dense optimizer states to file, successful\n",
      "[23d70h13m30s][HUGECTR][INFO]: Dumping untrainable weights to file, successful\n"
     ]
    }
   ],
   "source": [
    "!python3 $train_file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train configuration for count and target encoded features dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the same methodology as done above, we will make a DLRM train python file for this version of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths to save the training file and the weights\n",
    "\n",
    "train_file_path = os.path.join(config_output_path, 'train_dlrm_fp32_count-target-encode_1gpu.py')\n",
    "weights_output_path = os.path.join(weights_path,'dlrm_fp32_count-target-encode_1gpu/')\n",
    "\n",
    "# Creating Directory inside weights folder for saving weights of this training\n",
    "if os.path.isdir(weights_output_path):\n",
    "    shutil.rmtree(weights_output_path)\n",
    "os.mkdir(weights_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a python file for count and target encoded dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./basedir/configs/train_dlrm_fp32_count-target-encode_1gpu.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $train_file_path\n",
    "\n",
    "import os\n",
    "import hugectr\n",
    "from mpi4py import MPI\n",
    "\n",
    "# Define \"fast\" root directory for this example\n",
    "BASE_DIR = os.environ.get(\"BASE_DIR\", \"./basedir\")\n",
    "\n",
    "# Directory for NVTabular's count and target encoded processed dataset\n",
    "data_output_path = os.path.join(BASE_DIR, \"processed_ce-te\")\n",
    "output_train_path = os.path.join(data_output_path, \"train\")\n",
    "output_valid_path = os.path.join(data_output_path, \"valid\")\n",
    "\n",
    "# Directory to store HugeCTR's train weights\n",
    "config_output_path = os.path.join(BASE_DIR, \"configs\")\n",
    "weights_path = os.path.join(BASE_DIR, \"weights\")\n",
    "weights_output_path = os.path.join(weights_path,'dlrm_fp32_count-target-encode_1gpu/')\n",
    "\n",
    "# GPUs used for training\n",
    "NUM_GPUS = [0]\n",
    "\n",
    "# Model related parameter\n",
    "embedding_vec_size = 4                                          \n",
    "batchsize = 2048                                                   # Batch size used for training\n",
    "batchsize_eval = 2048                                              # Batch size used for evaluation\n",
    "max_eval_batchsize = 3768                                          # Iterations required to go through the complete validation set with the set batchsize_eval\n",
    "\n",
    "# Training related parameters\n",
    "num_iter = 30001                                                   # Iterations to train the model for\n",
    "eval_trigger = 10000                                               # Start evaluation after these iterations\n",
    "snapshot_trigger = 10000                                           # Save model checkpoints after these iterations\n",
    "\n",
    "# Input for slot size array\n",
    "embedding_size_str_count_encode = [90397,18,235,18,239,19,236,18,235,18,229,18,224,18,225,18,\n",
    "                                  219,17,213,17,199,26708,26708,2232749,711223,16,61,61,6,7,3]\n",
    "\n",
    "embeddings_count_encode = {'hist_cat_0': (18, 16),\n",
    " 'hist_cat_1': (18, 16),\n",
    " 'hist_cat_2': (19, 16),\n",
    " 'hist_cat_3': (18, 16),\n",
    " 'hist_cat_4': (18, 16),\n",
    " 'hist_cat_5': (18, 16),\n",
    " 'hist_cat_6': (18, 16),\n",
    " 'hist_cat_7': (18, 16),\n",
    " 'hist_cat_8': (17, 16),\n",
    " 'hist_cat_9': (17, 16),\n",
    " 'hist_subcat_0': (235, 34),\n",
    " 'hist_subcat_1': (239, 34),\n",
    " 'hist_subcat_2': (236, 34),\n",
    " 'hist_subcat_3': (235, 34),\n",
    " 'hist_subcat_4': (229, 34),\n",
    " 'hist_subcat_5': (224, 33),\n",
    " 'hist_subcat_6': (225, 33),\n",
    " 'hist_subcat_7': (219, 33),\n",
    " 'hist_subcat_8': (213, 32),\n",
    " 'hist_subcat_9': (199, 31),\n",
    " 'impr_cat': (26708, 482),\n",
    " 'impr_subcat': (26708, 482),\n",
    " 'impression_id': (2232749, 512),\n",
    " 'time': (90397, 512),\n",
    " 'time_day': (7, 16),\n",
    " 'time_day_week': (3, 16),\n",
    " 'time_hour': (16, 16),\n",
    " 'time_minute': (61, 16),\n",
    " 'time_second': (61, 16),\n",
    " 'time_wd': (6, 16),\n",
    " 'uid': (711223, 512)}\n",
    "\n",
    "solver = hugectr.CreateSolver(\n",
    "                              vvgpu = [NUM_GPUS],                       # GPU Indices to be used for training\n",
    "                              max_eval_batches = max_eval_batchsize,    # Max no. of eval batches on which eval will be done\n",
    "                              batchsize_eval = batchsize_eval,          # Minibatch size for eval\n",
    "                              batchsize = batchsize,                    # Minibatch size for training\n",
    "                              \n",
    "                              #learning rate parameters\n",
    "                              lr = 0.001,\n",
    "                              warmup_steps = 10000,\n",
    "                              decay_start = 20000,\n",
    "                              decay_steps = 200000,\n",
    "                              decay_power = 1,\n",
    "                              end_lr = 1e-06,\n",
    "                              \n",
    "                              # traning setting\n",
    "                              i64_input_key = True,                     # As we are using Parquet from NVTabular, I64 should be true\n",
    "                              repeat_dataset = True,                    # Repeat the dataset for training loop, True for Non Epoch Based Training\n",
    "                              use_mixed_precision = False,              # Flag to indicate use of Mixed precision training\n",
    "                              use_cuda_graph = False,                   # cuda graph for forward and back proppogation\n",
    "                              use_algorithm_search = False,             # algo search within the fc-layers\n",
    "                              )\n",
    "\n",
    "# create datareader object\n",
    "reader = hugectr.DataReaderParams(\n",
    "    \n",
    "    data_reader_type = hugectr.DataReaderType_t.Parquet,                # Dataset format selection: Parquet\n",
    "    source = [output_train_path+\"/_file_list.txt\"],                     # path to file list of processed NVT Train Dataset, can be used to input various file lists\n",
    "    eval_source = output_valid_path+\"/_file_list.txt\",                  # path to file list of processed NVT Valid Dataset\n",
    "    check_type = hugectr.Check_t.Non,                                   # data error detection turned off\n",
    "    slot_size_array = embedding_size_str_count_encode,                   # embedding slot array size list which is obtained from NVT operations\n",
    "    \n",
    "    )\n",
    "\n",
    "# create optimiser\n",
    "optimizer = hugectr.CreateOptimizer(\n",
    "        optimizer_type = hugectr.Optimizer_t.Adam, # Select optimizer type : Adam\n",
    "        update_type = hugectr.Update_t.Local,      # Select update type : Local\n",
    "        epsilon = 1e-07,                           # Adam parameter\n",
    "        beta1 = 0.9,                               # Adam parameter\n",
    "        beta2 = 0.999,                             # Adam parameter\n",
    "        atomic_update = False,                     # Atomic update for SGD, we are using adam,so set to false\n",
    ")\n",
    "\n",
    "# create a hugectr model object\n",
    "model = hugectr.Model(solver, reader, optimizer)\n",
    "\n",
    "# adding layers to the model\n",
    "\n",
    "model.add(hugectr.Input(\n",
    "                        # label parameters\n",
    "                        label_name = \"label\",                                 # Name of the Label column in the dataset\n",
    "                        label_dim = 1,                                        # label dimension, 1 for binary label based dataset. Can be customised according to users need\n",
    "                        # continous feature parameters\n",
    "                        dense_dim = 1,                                        # total number of dense (continous) features\n",
    "                        dense_name = \"dense\",                                 # Name of the dense (continuous) features\n",
    "                        # Sparse parameters for categorial inputs\n",
    "                        data_reader_sparse_param_array = [hugectr.DataReaderSparseParam(\n",
    "                            hugectr.DataReaderSparse_t.Localized,             # selecting localised or distributed reading of data\n",
    "                            max_feature_num = len(embeddings_count_encode) ,   # total number of features in the dataset\n",
    "                            max_nnz = 1,                                      # set 1 for one hot label dataset\n",
    "                            slot_num = len(embeddings_count_encode),           # total number of slots used for this sparse input in the dataset\n",
    "                        )],\n",
    "                        sparse_names = [\"data1\"]                              #list of names of the sparse input tensors to be referenced by following layers\n",
    "                        ))\n",
    "\n",
    "# adding sparse layer\n",
    "model.add(hugectr.SparseEmbedding(embedding_type = hugectr.Embedding_t.LocalizedSlotSparseEmbeddingHash,\n",
    "                            max_vocabulary_size_per_gpu = 10000000, # maximum vocabulary size or cardinality across all the input features (can be calculated from embedding size list)\n",
    "                            embedding_vec_size = embedding_vec_size, # model parameter\n",
    "                            combiner = 0, # intra-slot reduction operation (0=sum, 1=average)\n",
    "                            sparse_embedding_name = \"sparse_embedding1\", #name of the sparse embedding tensor to be referenced by following layers\n",
    "                            bottom_name = \"data1\",\n",
    "                            optimizer = optimizer))\n",
    "\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"dense\"],\n",
    "                            top_names = [\"fc1\"],\n",
    "                            num_output=512))\n",
    "\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc1\"],\n",
    "                            top_names = [\"relu1\"]))\n",
    "\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"relu1\"],\n",
    "                            top_names = [\"fc2\"],\n",
    "                            num_output=256))\n",
    "\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc2\"],\n",
    "                            top_names = [\"relu2\"]))\n",
    "\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"relu2\"],\n",
    "                            top_names = [\"fc3\"],\n",
    "                            num_output=embedding_vec_size))\n",
    "\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc3\"],\n",
    "                            top_names = [\"relu3\"]))\n",
    "\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Interaction,\n",
    "                            bottom_names = [\"relu3\",\"sparse_embedding1\"],\n",
    "                            top_names = [\"interaction1\"]))\n",
    "\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"interaction1\"],\n",
    "                            top_names = [\"fc4\"],\n",
    "                            num_output=1024))\n",
    "\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc4\"],\n",
    "                            top_names = [\"relu4\"]))\n",
    "\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"relu4\"],\n",
    "                            top_names = [\"fc5\"],\n",
    "                            num_output=1024))\n",
    "\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc5\"],\n",
    "                            top_names = [\"relu5\"]))\n",
    "\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"relu5\"],\n",
    "                            top_names = [\"fc6\"],\n",
    "                            num_output=512))\n",
    "\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc6\"],\n",
    "                            top_names = [\"relu6\"]))\n",
    "\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"relu6\"],\n",
    "                            top_names = [\"fc7\"],\n",
    "                            num_output=256))\n",
    "\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc7\"],\n",
    "                            top_names = [\"relu7\"]))\n",
    "\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"relu7\"],\n",
    "                            top_names = [\"fc8\"],\n",
    "                            num_output=1))\n",
    "\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.BinaryCrossEntropyLoss,\n",
    "                            bottom_names = [\"fc8\", \"label\"],\n",
    "                            top_names = [\"loss\"]))\n",
    "\n",
    "model.compile()\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.graph_to_json(graph_config_file = os.path.join(config_output_path,\"dlrm_fp32_count-target-encode_1gpu.json\"))\n",
    "\n",
    "model.fit(\n",
    "          max_iter = num_iter ,                     # maximum number of iterations\n",
    "          display = 1000,                           # display stats after no. of iterations\n",
    "          eval_interval = eval_trigger,             # interval for performing evaluation\n",
    "          snapshot = snapshot_trigger,              # interval after which model snapshots will be taken\n",
    "          snapshot_prefix = weights_output_path     # path for saving weights\n",
    "          )    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to train a DLRM model with HugeCTR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================Model Init=====================================================\n",
      "[23d07h13m04s][HUGECTR][INFO]: Global seed is 3766403167\n",
      "Device 0: Tesla V100-SXM2-16GB\n",
      "Device 1: Tesla V100-SXM2-16GB\n",
      "[23d07h13m06s][HUGECTR][INFO]: num of DataReader workers: 2\n",
      "[23d07h13m06s][HUGECTR][INFO]: num_internal_buffers 1\n",
      "[23d07h13m06s][HUGECTR][INFO]: num_internal_buffers 1\n",
      "[23d07h13m06s][HUGECTR][INFO]: Vocabulary size: 3090372\n",
      "[23d07h13m06s][HUGECTR][INFO]: max_vocabulary_size_per_gpu_=10000000\n",
      "[23d07h13m06s][HUGECTR][INFO]: All2All Warmup Start\n",
      "[23d07h13m06s][HUGECTR][INFO]: All2All Warmup End\n",
      "===================================================Model Compile===================================================\n",
      "[23d07h13m06s][HUGECTR][INFO]: gpu0 start to init embedding\n",
      "[23d07h13m06s][HUGECTR][INFO]: gpu1 start to init embedding\n",
      "[23d07h13m06s][HUGECTR][INFO]: gpu0 init embedding done\n",
      "[23d07h13m06s][HUGECTR][INFO]: gpu1 init embedding done\n",
      "===================================================Model Summary===================================================\n",
      "Label                                   Dense                         Sparse                        \n",
      "label                                   dense                          data1                         \n",
      "(None, 1)                               (None, 1)                               \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Layer Type                              Input Name                    Output Name                   Output Shape                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "LocalizedSlotSparseEmbeddingHash        data1                         sparse_embedding1             (None, 31, 4)                 \n",
      "InnerProduct                            dense                         fc1                           (None, 512)                   \n",
      "ReLU                                    fc1                           relu1                         (None, 512)                   \n",
      "InnerProduct                            relu1                         fc2                           (None, 256)                   \n",
      "ReLU                                    fc2                           relu2                         (None, 256)                   \n",
      "InnerProduct                            relu2                         fc3                           (None, 4)                     \n",
      "ReLU                                    fc3                           relu3                         (None, 4)                     \n",
      "Interaction                             relu3,sparse_embedding1       interaction1                  (None, 501)                   \n",
      "InnerProduct                            interaction1                  fc4                           (None, 1024)                  \n",
      "ReLU                                    fc4                           relu4                         (None, 1024)                  \n",
      "InnerProduct                            relu4                         fc5                           (None, 1024)                  \n",
      "ReLU                                    fc5                           relu5                         (None, 1024)                  \n",
      "InnerProduct                            relu5                         fc6                           (None, 512)                   \n",
      "ReLU                                    fc6                           relu6                         (None, 512)                   \n",
      "InnerProduct                            relu6                         fc7                           (None, 256)                   \n",
      "ReLU                                    fc7                           relu7                         (None, 256)                   \n",
      "InnerProduct                            relu7                         fc8                           (None, 1)                     \n",
      "BinaryCrossEntropyLoss                  fc8,label                     loss                                                        \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "[23d70h13m70s][HUGECTR][INFO]: Save the model graph to ./basedir/configs/dlrm_fp32_count-target-encode_1gpu.json, successful\n",
      "=====================================================Model Fit=====================================================\n",
      "[23d70h13m70s][HUGECTR][INFO]: Use non-epoch mode with number of iterations: 30001\n",
      "[23d70h13m70s][HUGECTR][INFO]: Training batchsize: 2048, evaluation batchsize: 2048\n",
      "[23d70h13m70s][HUGECTR][INFO]: Evaluation interval: 10000, snapshot interval: 10000\n",
      "[23d70h13m70s][HUGECTR][INFO]: Sparse embedding trainable: 1, dense network trainable: 1\n",
      "[23d70h13m70s][HUGECTR][INFO]: Use mixed precision: 0, scaler: 1.000000, use cuda graph: 0\n",
      "[23d70h13m70s][HUGECTR][INFO]: lr: 0.001000, warmup_steps: 10000, decay_start: 20000, decay_steps: 200000, decay_power: 1.000000, end_lr: 0.000001\n",
      "[23d70h13m70s][HUGECTR][INFO]: Training source file: ./basedir/processed_ce-te/train/_file_list.txt\n",
      "[23d70h13m70s][HUGECTR][INFO]: Evaluation source file: ./basedir/processed_ce-te/valid/_file_list.txt\n",
      "[23d70h13m10s][HUGECTR][INFO]: Iter: 1000 Time(1000 iters): 3.166246s Loss: 0.152442 lr:0.000100\n",
      "[23d70h13m13s][HUGECTR][INFO]: Iter: 2000 Time(1000 iters): 2.876258s Loss: 0.141435 lr:0.000200\n",
      "[23d70h13m16s][HUGECTR][INFO]: Iter: 3000 Time(1000 iters): 2.885032s Loss: 0.148973 lr:0.000300\n",
      "[23d70h13m19s][HUGECTR][INFO]: Iter: 4000 Time(1000 iters): 2.867446s Loss: 0.146038 lr:0.000400\n",
      "[23d70h13m22s][HUGECTR][INFO]: Iter: 5000 Time(1000 iters): 2.868622s Loss: 0.162829 lr:0.000500\n",
      "[23d70h13m24s][HUGECTR][INFO]: Iter: 6000 Time(1000 iters): 2.879623s Loss: 0.147671 lr:0.000600\n",
      "[23d70h13m27s][HUGECTR][INFO]: Iter: 7000 Time(1000 iters): 2.866108s Loss: 0.153517 lr:0.000700\n",
      "[23d70h13m30s][HUGECTR][INFO]: Iter: 8000 Time(1000 iters): 2.892620s Loss: 0.147979 lr:0.000800\n",
      "[23d70h13m33s][HUGECTR][INFO]: Iter: 9000 Time(1000 iters): 2.865599s Loss: 0.140153 lr:0.000900\n",
      "[23d70h13m36s][HUGECTR][INFO]: Iter: 10000 Time(1000 iters): 2.865384s Loss: 0.147290 lr:0.001000\n",
      "[23d70h13m42s][HUGECTR][INFO]: Evaluation, AUC: 0.637862\n",
      "[23d70h13m42s][HUGECTR][INFO]: Eval Time for 3768 iters: 6.278898s\n",
      "[HCDEBUG][ERROR] Runtime error: Error: file not open for writing /raid/ashish/hugectr/HugeCTR/src/embeddings/localized_slot_sparse_embedding_hash.cu:659 \n",
      "\n",
      "[23d70h13m42s][HUGECTR][INFO]: Dumping dense weights to file, successful\n",
      "[23d70h13m42s][HUGECTR][INFO]: Dumping dense optimizer states to file, successful\n",
      "[23d70h13m42s][HUGECTR][INFO]: Dumping untrainable weights to file, successful\n",
      "[23d70h13m45s][HUGECTR][INFO]: Iter: 11000 Time(1000 iters): 9.179425s Loss: 0.175223 lr:0.001000\n",
      "[23d70h13m48s][HUGECTR][INFO]: Iter: 12000 Time(1000 iters): 2.861754s Loss: 0.163202 lr:0.001000\n",
      "[23d70h13m51s][HUGECTR][INFO]: Iter: 13000 Time(1000 iters): 2.862150s Loss: 0.143603 lr:0.001000\n",
      "[23d70h13m54s][HUGECTR][INFO]: Iter: 14000 Time(1000 iters): 2.862138s Loss: 0.143617 lr:0.001000\n",
      "[23d70h13m56s][HUGECTR][INFO]: Iter: 15000 Time(1000 iters): 2.855054s Loss: 0.166280 lr:0.001000\n",
      "[23d70h13m59s][HUGECTR][INFO]: Iter: 16000 Time(1000 iters): 2.862290s Loss: 0.139950 lr:0.001000\n",
      "[23d70h14m20s][HUGECTR][INFO]: Iter: 17000 Time(1000 iters): 2.871672s Loss: 0.141921 lr:0.001000\n",
      "[23d70h14m50s][HUGECTR][INFO]: Iter: 18000 Time(1000 iters): 2.867121s Loss: 0.136212 lr:0.001000\n",
      "[23d70h14m80s][HUGECTR][INFO]: Iter: 19000 Time(1000 iters): 2.880103s Loss: 0.168026 lr:0.001000\n",
      "[23d70h14m11s][HUGECTR][INFO]: Iter: 20000 Time(1000 iters): 2.855813s Loss: 0.159970 lr:0.001000\n",
      "[23d70h14m17s][HUGECTR][INFO]: Evaluation, AUC: 0.658306\n",
      "[23d70h14m17s][HUGECTR][INFO]: Eval Time for 3768 iters: 6.272022s\n",
      "[HCDEBUG][ERROR] Runtime error: Error: file not open for writing /raid/ashish/hugectr/HugeCTR/src/embeddings/localized_slot_sparse_embedding_hash.cu:659 \n",
      "\n",
      "[23d70h14m17s][HUGECTR][INFO]: Dumping dense weights to file, successful\n",
      "[23d70h14m17s][HUGECTR][INFO]: Dumping dense optimizer states to file, successful\n",
      "[23d70h14m17s][HUGECTR][INFO]: Dumping untrainable weights to file, successful\n",
      "[23d70h14m20s][HUGECTR][INFO]: Iter: 21000 Time(1000 iters): 9.166304s Loss: 0.151504 lr:0.000995\n",
      "[23d70h14m23s][HUGECTR][INFO]: Iter: 22000 Time(1000 iters): 2.869393s Loss: 0.151518 lr:0.000990\n",
      "[23d70h14m26s][HUGECTR][INFO]: Iter: 23000 Time(1000 iters): 2.874710s Loss: 0.151519 lr:0.000985\n",
      "[23d70h14m29s][HUGECTR][INFO]: Iter: 24000 Time(1000 iters): 2.858837s Loss: 0.145712 lr:0.000980\n",
      "[23d70h14m31s][HUGECTR][INFO]: Iter: 25000 Time(1000 iters): 2.874874s Loss: 0.139983 lr:0.000975\n",
      "[23d70h14m34s][HUGECTR][INFO]: Iter: 26000 Time(1000 iters): 2.855446s Loss: 0.158094 lr:0.000970\n",
      "[23d70h14m37s][HUGECTR][INFO]: Iter: 27000 Time(1000 iters): 2.854188s Loss: 0.134265 lr:0.000965\n",
      "[23d70h14m40s][HUGECTR][INFO]: Iter: 28000 Time(1000 iters): 2.858711s Loss: 0.161178 lr:0.000960\n",
      "[23d70h14m43s][HUGECTR][INFO]: Iter: 29000 Time(1000 iters): 2.863104s Loss: 0.139409 lr:0.000955\n",
      "[23d70h14m46s][HUGECTR][INFO]: Iter: 30000 Time(1000 iters): 2.871633s Loss: 0.141221 lr:0.000950\n",
      "[23d70h14m52s][HUGECTR][INFO]: Evaluation, AUC: 0.665502\n",
      "[23d70h14m52s][HUGECTR][INFO]: Eval Time for 3768 iters: 6.396110s\n",
      "[HCDEBUG][ERROR] Runtime error: Error: file not open for writing /raid/ashish/hugectr/HugeCTR/src/embeddings/localized_slot_sparse_embedding_hash.cu:659 \n",
      "\n",
      "[23d70h14m52s][HUGECTR][INFO]: Dumping dense weights to file, successful\n",
      "[23d70h14m52s][HUGECTR][INFO]: Dumping dense optimizer states to file, successful\n",
      "[23d70h14m52s][HUGECTR][INFO]: Dumping untrainable weights to file, successful\n"
     ]
    }
   ],
   "source": [
    "!python3 $train_file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Inference on 1st validation set with HugeCTR\n",
    "\n",
    "After training 2 DLRM models, let's evaluate them on the validation set using HugeCTR's python inference API. The evaluation metric is AUC.<br>\n",
    "We will utilize the saved model graph in JSON format for inference, then prepare the validation data into CSR format and finally use the inference APIs to get the predictions.\n",
    "\n",
    "Let's start with the first trained model i.e. DLRM trained on simple time based features. In the next step, we would repeat the same process for the second trained model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the inference session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from hugectr.inference import InferenceParams, CreateInferenceSession\n",
    "from mpi4py import MPI\n",
    "\n",
    "config_inference_file_path = os.path.join(config_output_path,'dlrm_fp32_simple-time_1gpu_inference.json')\n",
    "weights_output_path = os.path.join(weights_path,'dlrm_fp32_simple-time_1gpu/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create inference session\n",
    "inference_params = InferenceParams(model_name = \"dlrm\",\n",
    "                              max_batchsize = 2048,\n",
    "                              hit_rate_threshold = 0.6,\n",
    "                              dense_model_file = weights_output_path+\"/_dense_30000.model\",\n",
    "                              sparse_model_files = [weights_output_path+\"/0_sparse_30000.model\"],\n",
    "                              device_id = 0,\n",
    "                              use_gpu_embedding_cache = True,\n",
    "                              cache_size_percentage = 0.2,\n",
    "                              i64_input_key = True)\n",
    "inference_session = CreateInferenceSession(config_inference_file_path, inference_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare validation set for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "output_valid_path = os.path.join(BASE_DIR, \"processed_nvt/valid\")\n",
    "\n",
    "nvtdata_test = pd.read_parquet(output_valid_path)\n",
    "nvtdata_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_feats = ['hist_count']\n",
    "\n",
    "cat_feats = ['time_hour',\n",
    " 'hist_cat_0',\n",
    " 'hist_subcat_0',\n",
    " 'hist_cat_1',\n",
    " 'hist_subcat_1',\n",
    " 'hist_cat_2',\n",
    " 'hist_subcat_2',\n",
    " 'hist_cat_3',\n",
    " 'hist_subcat_3',\n",
    " 'hist_cat_4',\n",
    " 'hist_subcat_4',\n",
    " 'hist_cat_5',\n",
    " 'hist_subcat_5',\n",
    " 'hist_cat_6',\n",
    " 'hist_subcat_6',\n",
    " 'hist_cat_7',\n",
    " 'hist_subcat_7',\n",
    " 'hist_cat_8',\n",
    " 'hist_subcat_8',\n",
    " 'hist_cat_9',\n",
    " 'hist_subcat_9',\n",
    " 'impr_cat',\n",
    " 'impr_subcat',\n",
    " 'impression_id',\n",
    " 'uid',\n",
    " 'time_minute',\n",
    " 'time_second',\n",
    " 'time_wd',\n",
    " 'time_day',\n",
    " 'time_day_week',\n",
    " 'time']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For inference, HugeCTR expects the data to conform to CSR format which mandates the categorical variables to occupy different integer ranges.<br>\n",
    "As an example, if there are 10 users and 10 items then HugeCTR expects the users to be encoded in the 1-10 range, while the items to be encoded in the 11-20 range. NVTabular encodes both users and items in the 1-10 ranges.\n",
    "\n",
    "For this reason, we need to shift the keys of the categorical variable produced by NVTabular to comply with HugeCTR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "shift = np.insert(np.cumsum(embedding_size_str_simple_time), 0, 0)[:-1]\n",
    "cat_data = nvtdata_test[cat_feats].values + shift\n",
    "dense_data = nvtdata_test[con_feats].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to perform batched inference\n",
    "def infer_batch(inference_session, dense_data_batch, cat_data_batch):\n",
    "    dense_features = list(dense_data_batch.flatten())\n",
    "    embedding_columns = list(cat_data_batch.flatten())\n",
    "    row_ptrs= list(range(0,len(embedding_columns)+1))\n",
    "    output = inference_session.predict(dense_features, embedding_columns, row_ptrs, True)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to carry out inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2048\n",
    "num_batches = (len(dense_data) // batch_size) + 1\n",
    "batch_idx = np.array_split(np.arange(len(dense_data)), num_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "\n",
    "for batch_id in tqdm(batch_idx):\n",
    "    dense_data_batch = dense_data[batch_id]\n",
    "    cat_data_batch = cat_data[batch_id]\n",
    "    results = infer_batch(inference_session, dense_data_batch, cat_data_batch)\n",
    "    labels.extend(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract ground truth to calculate AUC\n",
    "ground_truth = nvtdata_test['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "roc_auc_score(ground_truth, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Inference on 2nd validation set with HugeCTR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the same procedure as in the last step, let's compute the AUC on the count plus target encoded feature engineered validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_inference_file_path = os.path.join(config_output_path, 'dlrm_fp32_count-target-encode_1gpu_inference.json')\n",
    "weights_output_path = os.path.join(weights_path,'dlrm_fp32_count-target-encode_1gpu/')\n",
    "\n",
    "# create inference session\n",
    "inference_params = InferenceParams(model_name = \"dlrm\",\n",
    "                              max_batchsize = 2048,\n",
    "                              hit_rate_threshold = 0.6,\n",
    "                              dense_model_file = weights_output_path+\"/_dense_30000.model\",\n",
    "                              sparse_model_files = [weights_output_path+\"/0_sparse_30000.model\"],\n",
    "                              device_id = 0,\n",
    "                              use_gpu_embedding_cache = True,\n",
    "                              cache_size_percentage = 0.2,\n",
    "                              i64_input_key = True)\n",
    "inference_session = CreateInferenceSession(config_inference_file_path, inference_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "output_valid_path = os.path.join(BASE_DIR, \"processed_ce-te/valid\")\n",
    "\n",
    "nvtdata_test = pd.read_parquet(output_valid_path)\n",
    "nvtdata_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_feats = [\n",
    " 'TE_hist_cat_0_hist_cat_1_hist_cat_2_hist_cat_3_hist_cat_4_impr_cat_label_TE',\n",
    " 'TE_hist_cat_1_hist_cat_2_hist_cat_3_hist_cat_4_hist_cat_5_impr_cat_label_TE',\n",
    " 'TE_hist_cat_2_hist_cat_3_hist_cat_4_hist_cat_5_hist_cat_6_impr_cat_label_TE',\n",
    " 'TE_hist_cat_3_hist_cat_4_hist_cat_5_hist_cat_6_hist_cat_7_impr_cat_label_TE',\n",
    " 'TE_hist_cat_4_hist_cat_5_hist_cat_6_hist_cat_7_hist_cat_8_impr_cat_label_TE',\n",
    " 'TE_hist_cat_5_hist_cat_6_hist_cat_7_hist_cat_8_hist_cat_9_impr_cat_label_TE',\n",
    " 'hist_count',\n",
    " 'impr_cat_count',\n",
    " 'impr_subcat_count']\n",
    "\n",
    "cat_feats = ['time',\n",
    " 'hist_cat_0',\n",
    " 'hist_subcat_0',\n",
    " 'hist_cat_1',\n",
    " 'hist_subcat_1',\n",
    " 'hist_cat_2',\n",
    " 'hist_subcat_2',\n",
    " 'hist_cat_3',\n",
    " 'hist_subcat_3',\n",
    " 'hist_cat_4',\n",
    " 'hist_subcat_4',\n",
    " 'hist_cat_5',\n",
    " 'hist_subcat_5',\n",
    " 'hist_cat_6',\n",
    " 'hist_subcat_6',\n",
    " 'hist_cat_7',\n",
    " 'hist_subcat_7',\n",
    " 'hist_cat_8',\n",
    " 'hist_subcat_8',\n",
    " 'hist_cat_9',\n",
    " 'hist_subcat_9',\n",
    " 'impr_cat',\n",
    " 'impr_subcat',\n",
    " 'impression_id',\n",
    " 'uid',\n",
    " 'time_hour',\n",
    " 'time_minute',\n",
    " 'time_second',\n",
    " 'time_wd',\n",
    " 'time_day',\n",
    " 'time_day_week']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shift = np.insert(np.cumsum(embedding_size_str_count_encode), 0, 0)[:-1]\n",
    "cat_data = nvtdata_test[cat_feats].values + shift\n",
    "dense_data = nvtdata_test[con_feats].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to perform batched inference\n",
    "def infer_batch(inference_session, dense_data_batch, cat_data_batch):\n",
    "    dense_features = list(dense_data_batch.flatten())\n",
    "    embedding_columns = list(cat_data_batch.flatten())\n",
    "    row_ptrs= list(range(0,len(embedding_columns)+1))\n",
    "    output = inference_session.predict(dense_features, embedding_columns, row_ptrs, True)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to carry out inference on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2048\n",
    "num_batches = (len(dense_data) // batch_size) + 1\n",
    "batch_idx = np.array_split(np.arange(len(dense_data)), num_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "\n",
    "for batch_id in tqdm(batch_idx):\n",
    "    dense_data_batch = dense_data[batch_id]\n",
    "    cat_data_batch = cat_data[batch_id]\n",
    "    results = infer_batch(inference_session, dense_data_batch, cat_data_batch)\n",
    "    labels.extend(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract ground truth to calculate AUC\n",
    "ground_truth = nvtdata_test['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.client - ERROR - Failed to reconnect to scheduler after 10.00 seconds, closing client\n",
      "_GatheringFuture exception was never retrieved\n",
      "future: <_GatheringFuture finished exception=CancelledError()>\n",
      "asyncio.exceptions.CancelledError\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "roc_auc_score(ground_truth, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial notebook, we have walked through the process of data cleaning, pre-processing, feature engineering to model training and inferencing, all using the Merlin framework. We hope that this notebook would be helpful for building Recommendation Systems on your datasets as well.\n",
    "\n",
    "Feel free to experiment with the various hyper-parameters on the feature engineering and model training side and share your results!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
