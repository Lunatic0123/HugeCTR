{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# HugeCTR Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In HugeCTR version 3.0, we have supported inference and added it to the Python interface. This notebook explains how to make inference with trained HugeCTR models in Python.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Build the HugeCTR Python Interface](#1)\n",
    "1. [DCN Inference Demo](#2)\n",
    "1. [API Signatures](#3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "## 1. Build the HugeCTR Python Interface\n",
    "\n",
    "To build the HugeCTR Python interface: \n",
    "\n",
    "1. Enter the HugeCTR docker container and run the following commands:\n",
    "   ```bash\n",
    "   $ cd hugectr\n",
    "   $ mkdir -p build && cd build\n",
    "   $ cmake -DCMAKE_BUILD_TYPE=Release -DSM=70 .. # Target is NVIDIA V100\n",
    "   $ make -j\n",
    "   ```\n",
    "\n",
    "   A dynamic link to the `hugectr.so` library is generated in the `hugectr/build/lib/` folder as shown here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hugectr.so\t\tlibgmock.a\t libgtest.a\t  libhuge_ctr_shared.so\r\n",
      "libembedding_plugin.so\tlibgmock_main.a  libgtest_main.a  libhuge_ctr_static.a\r\n"
     ]
    }
   ],
   "source": [
    "!ls /hugectr/build/lib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Copy `hugectr.so` to the folder where you want to use the Python interface. \n",
    "   You can also install it to /usr/local/hugectr/lib and set the environment variable export to `PYTHONPATH=/usr/local/hugectr/lib:$PYTHONPATH` if you want to use the Python interface within the docker container environment.\n",
    "\n",
    "3. Import HugeCTR, train your model and make inference using with Python as shown here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp /hugectr/build/lib/hugectr.so ./\n",
    "import hugectr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "## 2. DCN Inference Demo\n",
    "\n",
    "### 2.1 Download and Preprocess Data\n",
    "1. Download the Kaggle Criteo dataset using the following command:\n",
    "   ```shell\n",
    "   $ wget https://s3-eu-west-1.amazonaws.com/kaggle-display-advertising-challenge-dataset/dac.tar.gz\n",
    "   ```\n",
    "\n",
    "   For additional information, see [http://labs.criteo.com/2014/02/kaggle-display-advertising-challenge-dataset/](http://labs.criteo.com/2014/02/kaggle-display-advertising-challenge-dataset/).\n",
    "   \n",
    "2. Extract the dataset using the following command:\n",
    "   ```shell\n",
    "   $ tar zxvf dac.tar.gz\n",
    "   ```\n",
    "\n",
    "3. Preprocess the data using  the following commands:\n",
    "   ```shell\n",
    "   $ mkdir dcn_data\n",
    "   $ shuf train.txt > train.shuf.txt\n",
    "   $ python3 /hugectr/tools/criteo_script/preprocess.py --src_csv_path=train.shuf.txt --dst_csv_path=dcn_data/train.out.txt --normalize_dense=1 --feature_cross=0\n",
    "   ```\n",
    "\n",
    "4. Split the dataset using the following commands:\n",
    "   ```shell\n",
    "   head -n 36672493 dcn_data/train.out.txt > dcn_data/train && \\\\\n",
    "   tail -n 9168124 dcn_data/train.out.txt > dcn_data/valtest && \\\\\n",
    "   head -n 4584062 dcn_data/valtest > dcn_data/val && \\\\\n",
    "   tail -n 4584062 dcn_data/valtest > dcn_data/test\n",
    "   ```\n",
    "5. Convert the dataset to the HugeCTR Norm dataset format by generating a `file_list.*.txt` and `file_list.*.keyset` as well as all training data (`*.data`) so that the features of the designated source can be employed during training such as model prefetch during training using the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile criteo2hugectr.sh\n",
    "mkdir -p dcn_data_hugectr/criteo && \\\n",
    "mkdir -p dcn_data_hugectr/criteo_test && \\\n",
    "cd dcn_data_hugectr && \\\n",
    "cp /hugectr/build/bin/criteo2hugectr ./ &&\n",
    "./criteo2hugectr ../dcn_data/train criteo/sparse_embedding file_list.txt\n",
    "./criteo2hugectr ../dcn_data/val criteo_test/sparse_embedding file_list_test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash criteo2hugectr.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 DCN Model Training\n",
    "\n",
    "We can train fom scratch and store the trained dense model and embedding table in model files by doing the following: \n",
    "\n",
    "1. Create a JSON file for the DCN model. \n",
    "   **NOTE**: Please note that the solver clause no longer needs to be added to the JSON file when using the Python interface. Instead, you can configure the parameters using `hugectr.solver_parser_helper()` directly in the Python interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dcn_train.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile dcn_train.json\n",
    "{\n",
    "  \"optimizer\": {\n",
    "    \"type\": \"Adam\",\n",
    "    \"update_type\": \"Global\",\n",
    "    \"adam_hparam\": {\n",
    "      \"learning_rate\": 0.001,\n",
    "      \"beta1\": 0.9,\n",
    "      \"beta2\": 0.999,\n",
    "      \"epsilon\": 0.0000001\n",
    "    }\n",
    "  },\n",
    "  \"layers\": [\n",
    "    {\n",
    "      \"name\": \"data\",\n",
    "      \"type\": \"Data\",\n",
    "      \"source\": \"./file_list.txt\",\n",
    "      \"eval_source\": \"./file_list_test.txt\",\n",
    "      \"check\": \"Sum\",\n",
    "      \"label\": {\n",
    "        \"top\": \"label\",\n",
    "        \"label_dim\": 1\n",
    "      },\n",
    "      \"dense\": {\n",
    "        \"top\": \"dense\",\n",
    "        \"dense_dim\": 13\n",
    "      },\n",
    "      \"sparse\": [\n",
    "        {\n",
    "          \"top\": \"data1\",\n",
    "          \"type\": \"DistributedSlot\",\n",
    "          \"max_feature_num_per_sample\": 30,\n",
    "          \"slot_num\": 26\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"sparse_embedding1\",\n",
    "      \"type\": \"DistributedSlotSparseEmbeddingHash\",\n",
    "      \"bottom\": \"data1\",\n",
    "      \"top\": \"sparse_embedding1\",\n",
    "      \"sparse_embedding_hparam\": {\n",
    "        \"max_vocabulary_size_per_gpu\": 1737709,\n",
    "        \"embedding_vec_size\": 16,\n",
    "        \"combiner\": 0\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"reshape1\",\n",
    "      \"type\": \"Reshape\",\n",
    "      \"bottom\": \"sparse_embedding1\",\n",
    "      \"top\": \"reshape1\",\n",
    "      \"leading_dim\": 416\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"concat1\",\n",
    "      \"type\": \"Concat\",\n",
    "      \"bottom\": [\n",
    "        \"reshape1\",\n",
    "        \"dense\"\n",
    "      ],\n",
    "      \"top\": \"concat1\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"slice1\",\n",
    "      \"type\": \"Slice\",\n",
    "      \"bottom\": \"concat1\",\n",
    "      \"ranges\": [\n",
    "        [\n",
    "          0,\n",
    "          429\n",
    "        ],\n",
    "        [\n",
    "          0,\n",
    "          429\n",
    "        ]\n",
    "      ],\n",
    "      \"top\": [\n",
    "        \"slice11\",\n",
    "        \"slice12\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"multicross1\",\n",
    "      \"type\": \"MultiCross\",\n",
    "      \"bottom\": \"slice11\",\n",
    "      \"top\": \"multicross1\",\n",
    "      \"mc_param\": {\n",
    "        \"num_layers\": 6\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"fc1\",\n",
    "      \"type\": \"InnerProduct\",\n",
    "      \"bottom\": \"slice12\",\n",
    "      \"top\": \"fc1\",\n",
    "      \"fc_param\": {\n",
    "        \"num_output\": 1024\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"relu1\",\n",
    "      \"type\": \"ReLU\",\n",
    "      \"bottom\": \"fc1\",\n",
    "      \"top\": \"relu1\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"dropout1\",\n",
    "      \"type\": \"Dropout\",\n",
    "      \"rate\": 0.5,\n",
    "      \"bottom\": \"relu1\",\n",
    "      \"top\": \"dropout1\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"fc2\",\n",
    "      \"type\": \"InnerProduct\",\n",
    "      \"bottom\": \"dropout1\",\n",
    "      \"top\": \"fc2\",\n",
    "      \"fc_param\": {\n",
    "        \"num_output\": 1024\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"relu2\",\n",
    "      \"type\": \"ReLU\",\n",
    "      \"bottom\": \"fc2\",\n",
    "      \"top\": \"relu2\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"dropout2\",\n",
    "      \"type\": \"Dropout\",\n",
    "      \"rate\": 0.5,\n",
    "      \"bottom\": \"relu2\",\n",
    "      \"top\": \"dropout2\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"concat2\",\n",
    "      \"type\": \"Concat\",\n",
    "      \"bottom\": [\n",
    "        \"dropout2\",\n",
    "        \"multicross1\"\n",
    "      ],\n",
    "      \"top\": \"concat2\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"fc4\",\n",
    "      \"type\": \"InnerProduct\",\n",
    "      \"bottom\": \"concat2\",\n",
    "      \"top\": \"fc4\",\n",
    "      \"fc_param\": {\n",
    "        \"num_output\": 1\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"loss\",\n",
    "      \"type\": \"BinaryCrossEntropyLoss\",\n",
    "      \"bottom\": [\n",
    "        \"fc4\",\n",
    "        \"label\"\n",
    "      ],\n",
    "      \"top\": \"loss\"\n",
    "    }\n",
    "  ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Write the Python script for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dcn_train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile dcn_train.py\n",
    "from hugectr import Session, solver_parser_helper\n",
    "import sys\n",
    "\n",
    "def dcn_train(json_file):\n",
    "  solver_config = solver_parser_helper(seed = 0,\n",
    "                                     batchsize = 2048,\n",
    "                                     batchsize_eval =2048,\n",
    "                                     model_file = \"\",\n",
    "                                     embedding_files = [],\n",
    "                                     vvgpu = [[0]],\n",
    "                                     use_mixed_precision = False,\n",
    "                                     scaler = 1.0,\n",
    "                                     i64_input_key = False,\n",
    "                                     use_algorithm_search = True,\n",
    "                                     use_cuda_graph = True,\n",
    "                                     repeat_dataset = True\n",
    "                                    )\n",
    "  sess = Session(solver_config, json_file)\n",
    "  sess.start_data_reading()\n",
    "  for i in range(10000):\n",
    "    sess.train()\n",
    "    if (i%200 == 0):\n",
    "      loss = sess.get_current_loss()\n",
    "      print(\"[HUGECTR][INFO] iter: {}; loss: {}\".format(i, loss))\n",
    "    if (i%1000 == 0 and i != 0):\n",
    "      sess.check_overflow()\n",
    "      sess.copy_weights_for_evaluation()\n",
    "      data_reader_eval = sess.get_data_reader_eval()\n",
    "      for _ in range(solver_config.max_eval_batches):\n",
    "        sess.eval()\n",
    "      metrics = sess.get_eval_metrics()\n",
    "      print(\"[HUGECTR][INFO] iter: {}, {}\".format(i, metrics))\n",
    "  sess.download_params_to_files(\"./\", i+1)\n",
    "  return\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  json_file = sys.argv[1]\n",
    "  dcn_train(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing dcn_train.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile dcn_train.sh\n",
    "cd dcn_data_hugectr && \\\n",
    "python3 ../dcn_train.py ../dcn_train.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15d08h52m28s][HUGECTR][INFO]: Global seed is 3908372682\n",
      "[15d08h52m29s][HUGECTR][INFO]: Peer-to-peer access cannot be fully enabled.\n",
      "Device 0: GeForce RTX 2080 Ti\n",
      "[15d08h52m29s][HUGECTR][INFO]: cache_eval_data is not specified using default: 0\n",
      "[15d08h52m29s][HUGECTR][INFO]: num_workers is not specified using default: 12\n",
      "[15d08h52m29s][HUGECTR][INFO]: num of DataReader workers: 12\n",
      "[15d08h52m29s][HUGECTR][INFO]: max_nnz is not specified using default: 30\n",
      "[15d08h52m29s][HUGECTR][INFO]: num_internal_buffers 1\n",
      "[15d08h52m29s][HUGECTR][INFO]: num_internal_buffers 1\n",
      "[15d08h52m29s][HUGECTR][INFO]: max_vocabulary_size_per_gpu_=1737709\n",
      "[15d08h52m30s][HUGECTR][INFO]: gpu0 start to init embedding\n",
      "[15d08h52m30s][HUGECTR][INFO]: gpu0 init embedding done\n",
      "[HUGECTR][INFO] iter: 0; loss: 0.5902401804924011\n",
      "[HUGECTR][INFO] iter: 200; loss: 0.4940361976623535\n",
      "[HUGECTR][INFO] iter: 400; loss: 0.46117234230041504\n",
      "[HUGECTR][INFO] iter: 600; loss: 0.46512502431869507\n",
      "[HUGECTR][INFO] iter: 800; loss: 0.4839572310447693\n",
      "[HUGECTR][INFO] iter: 1000; loss: 0.4705187976360321\n",
      "[HUGECTR][INFO] iter: 1000, [('AUC', 0.7965685725212097)]\n",
      "[HUGECTR][INFO] iter: 1200; loss: 0.46971777081489563\n",
      "[HUGECTR][INFO] iter: 1400; loss: 0.4750780165195465\n",
      "[HUGECTR][INFO] iter: 1600; loss: 0.45408159494400024\n",
      "[HUGECTR][INFO] iter: 1800; loss: 0.48052942752838135\n",
      "[HUGECTR][INFO] iter: 2000; loss: 0.45116859674453735\n",
      "[HUGECTR][INFO] iter: 2000, [('AUC', 0.8016412854194641)]\n",
      "[HUGECTR][INFO] iter: 2200; loss: 0.455051451921463\n",
      "[HUGECTR][INFO] iter: 2400; loss: 0.48516514897346497\n",
      "[HUGECTR][INFO] iter: 2600; loss: 0.4505443871021271\n",
      "[HUGECTR][INFO] iter: 2800; loss: 0.4679661989212036\n",
      "[HUGECTR][INFO] iter: 3000; loss: 0.45308732986450195\n",
      "[HUGECTR][INFO] iter: 3000, [('AUC', 0.801027238368988)]\n",
      "[HUGECTR][INFO] iter: 3200; loss: 0.4419821500778198\n",
      "[HUGECTR][INFO] iter: 3400; loss: 0.47689560055732727\n",
      "[HUGECTR][INFO] iter: 3600; loss: 0.452593594789505\n",
      "[HUGECTR][INFO] iter: 3800; loss: 0.45451000332832336\n",
      "[HUGECTR][INFO] iter: 4000; loss: 0.47622862458229065\n",
      "[HUGECTR][INFO] iter: 4000, [('AUC', 0.8058502674102783)]\n",
      "[HUGECTR][INFO] iter: 4200; loss: 0.4689927399158478\n",
      "[HUGECTR][INFO] iter: 4400; loss: 0.4484305679798126\n",
      "[HUGECTR][INFO] iter: 4600; loss: 0.4585236608982086\n",
      "[HUGECTR][INFO] iter: 4800; loss: 0.4488183856010437\n",
      "[HUGECTR][INFO] iter: 5000; loss: 0.44987255334854126\n",
      "[HUGECTR][INFO] iter: 5000, [('AUC', 0.8054690361022949)]\n",
      "[HUGECTR][INFO] iter: 5200; loss: 0.4623567461967468\n",
      "[HUGECTR][INFO] iter: 5400; loss: 0.45883575081825256\n",
      "[HUGECTR][INFO] iter: 5600; loss: 0.4537130296230316\n",
      "[HUGECTR][INFO] iter: 5800; loss: 0.42941921949386597\n",
      "[HUGECTR][INFO] iter: 6000; loss: 0.4558860659599304\n",
      "[HUGECTR][INFO] iter: 6000, [('AUC', 0.8041548728942871)]\n",
      "[HUGECTR][INFO] iter: 6200; loss: 0.4479009211063385\n",
      "[HUGECTR][INFO] iter: 6400; loss: 0.44403597712516785\n",
      "[HUGECTR][INFO] iter: 6600; loss: 0.4476199448108673\n",
      "[HUGECTR][INFO] iter: 6800; loss: 0.45518559217453003\n",
      "[HUGECTR][INFO] iter: 7000; loss: 0.4287780821323395\n",
      "[HUGECTR][INFO] iter: 7000, [('AUC', 0.8051096200942993)]\n",
      "[HUGECTR][INFO] iter: 7200; loss: 0.44521287083625793\n",
      "[HUGECTR][INFO] iter: 7400; loss: 0.48106324672698975\n",
      "[HUGECTR][INFO] iter: 7600; loss: 0.45251166820526123\n",
      "[HUGECTR][INFO] iter: 7800; loss: 0.4300231337547302\n",
      "[HUGECTR][INFO] iter: 8000; loss: 0.47906413674354553\n",
      "[HUGECTR][INFO] iter: 8000, [('AUC', 0.8070412874221802)]\n",
      "[HUGECTR][INFO] iter: 8200; loss: 0.46400538086891174\n",
      "[HUGECTR][INFO] iter: 8400; loss: 0.4356161653995514\n",
      "[HUGECTR][INFO] iter: 8600; loss: 0.4590938091278076\n",
      "[HUGECTR][INFO] iter: 8800; loss: 0.449056476354599\n",
      "[HUGECTR][INFO] iter: 9000; loss: 0.4581243395805359\n",
      "[HUGECTR][INFO] iter: 9000, [('AUC', 0.8066021800041199)]\n",
      "[HUGECTR][INFO] iter: 9200; loss: 0.460602730512619\n",
      "[HUGECTR][INFO] iter: 9400; loss: 0.4471574127674103\n",
      "[HUGECTR][INFO] iter: 9600; loss: 0.46517327427864075\n",
      "[HUGECTR][INFO] iter: 9800; loss: 0.459434449672699\n",
      "[15d08h54m06s][HUGECTR][INFO]: Rank0: Dump hash table from GPU0\n",
      "[15d08h54m06s][HUGECTR][INFO]: Rank0: Write hash table <key,value> pairs to file\n",
      "[15d08h54m06s][HUGECTR][INFO]: Done\n"
     ]
    }
   ],
   "source": [
    "!bash dcn_train.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 DCN Model Inference\n",
    "\n",
    "1. Create a JSON file for DCN model inference\n",
    "\n",
    "Check the stored model files that will be used in the inference, and create the JSON file for inference. We should remove the solver and optimizer clauses and add the inference clause in the JSON file. The paths of the stored dense model and sparse model(s) should be specified at \"dense_model_file\" and \"sparse_model_file\" within the inference clause. We need to make some modifications to \"data\" in the layers clause. Besides, we need to change the last layer from BinaryCrossEntropyLoss to Sigmoid. The rest of \"layers\" should be exactly the same as that in the training JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dcn_data_hugectr/0_sparse_10000.model  dcn_data_hugectr/_dense_10000.model\r\n"
     ]
    }
   ],
   "source": [
    "!ls dcn_data_hugectr/*.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing dcn_inference.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile dcn_inference.json\n",
    "{\n",
    "  \"inference\": {\n",
    "    \"max_batchsize\": 4096,\n",
    "    \"dense_model_file\": \"dcn_data_hugectr/_dense_10000.model\",\n",
    "    \"sparse_model_file\": \"dcn_data_hugectr/0_sparse_10000.model\"\n",
    "  },\n",
    "  \"layers\": [\n",
    "    {\n",
    "      \"name\": \"data\",\n",
    "      \"type\": \"Data\",\n",
    "      \"check\": \"Sum\",\n",
    "      \"label\": {\n",
    "        \"label_dim\": 1\n",
    "      },\n",
    "      \"dense\": {\n",
    "        \"top\": \"dense\",\n",
    "        \"dense_dim\": 13\n",
    "      },\n",
    "      \"sparse\": [\n",
    "        {\n",
    "          \"top\": \"data1\",\n",
    "          \"type\": \"DistributedSlot\",\n",
    "          \"max_feature_num_per_sample\": 30,\n",
    "          \"slot_num\": 26\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"sparse_embedding1\",\n",
    "      \"type\": \"DistributedSlotSparseEmbeddingHash\",\n",
    "      \"bottom\": \"data1\",\n",
    "      \"top\": \"sparse_embedding1\",\n",
    "      \"sparse_embedding_hparam\": {\n",
    "        \"max_vocabulary_size_per_gpu\": 1737709,\n",
    "        \"embedding_vec_size\": 16,\n",
    "        \"combiner\": 0\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"reshape1\",\n",
    "      \"type\": \"Reshape\",\n",
    "      \"bottom\": \"sparse_embedding1\",\n",
    "      \"top\": \"reshape1\",\n",
    "      \"leading_dim\": 416\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"concat1\",\n",
    "      \"type\": \"Concat\",\n",
    "      \"bottom\": [\n",
    "        \"reshape1\",\n",
    "        \"dense\"\n",
    "      ],\n",
    "      \"top\": \"concat1\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"slice1\",\n",
    "      \"type\": \"Slice\",\n",
    "      \"bottom\": \"concat1\",\n",
    "      \"ranges\": [\n",
    "        [\n",
    "          0,\n",
    "          429\n",
    "        ],\n",
    "        [\n",
    "          0,\n",
    "          429\n",
    "        ]\n",
    "      ],\n",
    "      \"top\": [\n",
    "        \"slice11\",\n",
    "        \"slice12\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"multicross1\",\n",
    "      \"type\": \"MultiCross\",\n",
    "      \"bottom\": \"slice11\",\n",
    "      \"top\": \"multicross1\",\n",
    "      \"mc_param\": {\n",
    "        \"num_layers\": 6\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"fc1\",\n",
    "      \"type\": \"InnerProduct\",\n",
    "      \"bottom\": \"slice12\",\n",
    "      \"top\": \"fc1\",\n",
    "      \"fc_param\": {\n",
    "        \"num_output\": 1024\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"relu1\",\n",
    "      \"type\": \"ReLU\",\n",
    "      \"bottom\": \"fc1\",\n",
    "      \"top\": \"relu1\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"dropout1\",\n",
    "      \"type\": \"Dropout\",\n",
    "      \"rate\": 0.5,\n",
    "      \"bottom\": \"relu1\",\n",
    "      \"top\": \"dropout1\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"fc2\",\n",
    "      \"type\": \"InnerProduct\",\n",
    "      \"bottom\": \"dropout1\",\n",
    "      \"top\": \"fc2\",\n",
    "      \"fc_param\": {\n",
    "        \"num_output\": 1024\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"relu2\",\n",
    "      \"type\": \"ReLU\",\n",
    "      \"bottom\": \"fc2\",\n",
    "      \"top\": \"relu2\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"dropout2\",\n",
    "      \"type\": \"Dropout\",\n",
    "      \"rate\": 0.5,\n",
    "      \"bottom\": \"relu2\",\n",
    "      \"top\": \"dropout2\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"concat2\",\n",
    "      \"type\": \"Concat\",\n",
    "      \"bottom\": [\n",
    "        \"dropout2\",\n",
    "        \"multicross1\"\n",
    "      ],\n",
    "      \"top\": \"concat2\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"fc4\",\n",
    "      \"type\": \"InnerProduct\",\n",
    "      \"bottom\": \"concat2\",\n",
    "      \"top\": \"fc4\",\n",
    "      \"fc_param\": {\n",
    "        \"num_output\": 1\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"sigmoid\",\n",
    "      \"type\": \"Sigmoid\",\n",
    "      \"bottom\": \"fc4\",\n",
    "      \"top\": \"sigmoid\"\n",
    "    }\n",
    "  ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Convert the criteo data to inference format\n",
    "\n",
    "The HugeCTR inference is enabled by predict() method of InferenceSession, which requires dense features, embedding columns and row pointers of slots as the input and gives the prediction result as the output. We need to convert the criteo data to inference format first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 ../tools/criteo_predict/criteo2predict.py --src_csv_path=dcn_data/test --src_config=../tools/criteo_predict/dcn_data.json --dst_path=./dcn_csr.txt --batch_size=4096"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 3. Write the Python script for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing dcn_inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile dcn_inference.py\n",
    "import sys\n",
    "from hugectr.inference import CreateParameterServer, CreateEmbeddingCache, InferenceSession\n",
    "\n",
    "def dcn_inference(config_file, model_name, data_path):\n",
    "  # read data from file\n",
    "  data_file = open(data_path)\n",
    "  labels = [int(item) for item in data_file.readline().split(' ')]\n",
    "  dense_features = [float(item) for item in data_file.readline().split(' ')]\n",
    "  embedding_columns = [int(item) for item in data_file.readline().split(' ')]\n",
    "  row_ptrs = [int(item) for item in data_file.readline().split(' ')]\n",
    "  # create parameter server, embedding cache and inference session\n",
    "  parameter_server = CreateParameterServer([config_file], [model_name], False)\n",
    "  embedding_cache = CreateEmbeddingCache(parameter_server, 0, True, 0.2, config_file, model_name, False)\n",
    "  inference_session = InferenceSession(config_file, 0, embedding_cache)\n",
    "  # make prediction and calculate accuracy\n",
    "  output = inference_session.predict(dense_features, embedding_columns, row_ptrs)\n",
    "  accuracy = calculate_accuracy(labels, output)\n",
    "  print(\"[HUGECTR][INFO] prediction number samples: {}, accuracy: {}\".format(len(labels), accuracy))\n",
    "\n",
    "def calculate_accuracy(labels, output):\n",
    "  num_samples = len(labels)\n",
    "  flags = [1 if ((labels[i] == 0 and output[i] <= 0.5) or (labels[i] == 1 and output[i] > 0.5)) else 0 for i in range(num_samples)]\n",
    "  correct_samples = sum(flags)\n",
    "  return float(correct_samples)/float(num_samples)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "  config_file = sys.argv[1]\n",
    "  model_name = sys.argv[2]\n",
    "  data_path = sys.argv[3]\n",
    "  dcn_inference(config_file, model_name, data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15d08h54m47s][HUGECTR][INFO]: default_emb_vec_value is not specified using default: 0.000000\n",
      "[15d08h54m48s][HUGECTR][INFO]: Global seed is 211386932\n",
      "[15d08h54m49s][HUGECTR][INFO]: Peer-to-peer access cannot be fully enabled.\n",
      "[15d08h54m49s][HUGECTR][INFO]: algorithm_search is not specified using default: 1\n",
      "[15d08h54m49s][HUGECTR][INFO]: cuda_graph is not specified using default: 1\n",
      "[15d08h54m49s][HUGECTR][INFO]: start create embedding for inference\n",
      "[15d08h54m49s][HUGECTR][INFO]: sparse_input name data1\n",
      "[15d08h54m49s][HUGECTR][INFO]: create embedding for inference success\n",
      "[HUGECTR][INFO] prediction number samples: 4096, accuracy: 0.80419921875\n"
     ]
    }
   ],
   "source": [
    "!python3 dcn_inference.py dcn_inference.json DCN dcn_csr.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "## 3. API Signatures\n",
    "\n",
    "Here is the list of all the API signatures within the HugeCTR Python interface related to the inference feature. As you can see from the above example, we have included `CreateParameterServer`, `CreateEmbeddingCache`, and `InferenceSession`.\n",
    "\n",
    "**CreateParameterServer**\n",
    "```bash\n",
    "CreateParameterServer(...) method of builtins.PyCapsule instance\n",
    "    CreateParameterServer(model_config_path: List[str], model_name: List[str], i64_input_key: bool) -> hugectr.inference.ParameterServerBase\n",
    "```\n",
    "\n",
    "**CreateEmbeddingCache**\n",
    "```bash\n",
    "CreateEmbeddingCache(...) method of builtins.PyCapsule instance\n",
    "    CreateEmbeddingCache(parameter_server: hugectr.inference.ParameterServerBase, cuda_dev_id: int, use_gpu_embedding_cache: bool, cache_size_percentage: float, model_config_path: str, model_name: str, i64_input_key: bool) -> hugectr.inference.EmbeddingCacheInterface\n",
    "```\n",
    "\n",
    "**InferenceSession**\n",
    "```bash\n",
    "class InferenceSession(pybind11_builtins.pybind11_object)\n",
    " |  Method resolution order:\n",
    " |      InferenceSession\n",
    " |      pybind11_builtins.pybind11_object\n",
    " |      builtins.object\n",
    " |\n",
    " |  Methods defined here:\n",
    " |\n",
    " |  __init__(...)\n",
    " |      __init__(self: hugectr.inference.InferenceSession, config_file: str, device_id: int, embedding_cache: hugectr.inference.EmbeddingCacheInterface) -> None\n",
    " |\n",
    " |  predict(...)\n",
    " |      predict(*args, **kwargs)\n",
    " |      Overloaded function.\n",
    " |\n",
    " |      1. predict(self: hugectr.inference.InferenceSession, dense_feature: List[float], embeddingcolumns: List[int], row_ptrs: List[int]) -> List[float]\n",
    " |\n",
    " |      2. predict(self: hugectr.inference.InferenceSession, dense_feature: List[float], embeddingcolumns: List[int], row_ptrs: List[int], i64_input_key: bool) -> List[float]\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
