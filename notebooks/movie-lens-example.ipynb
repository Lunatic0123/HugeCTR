{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2020 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# HugeCTR demo on Movie lens data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "HugeCTR is a recommender specific framework which is capable of distributed training across multiple GPUs and nodes for Click-Through-Rate (CTR) estimation. It is a component of NVIDIA [Merlin](https://developer.nvidia.com/nvidia-merlin#getstarted), which is a framework accelerating the entire pipeline from data ingestion and training to deploying GPU-accelerated recommender systems.\n",
    "\n",
    "### Learning objectives\n",
    "\n",
    "This notebook demonstrates the steps for training a deep learning recommender model (DLRM) on the movie lens 20M [dataset](https://grouplens.org/datasets/movielens/20m/). We will walk you through the process of data preprocessing, train a DLRM model with HugeCTR, then using the movie embedding to answer item similarity queries.\n",
    "\n",
    "## Content\n",
    "1. [Pre-requisite](#1)\n",
    "1. [Data download and preprocessing](#2)\n",
    "1. [HugeCTR DLRM training](#3)\n",
    "1. [Answer item similarity with DLRM embedding](#4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "## 1. Pre-requisite\n",
    "\n",
    "\n",
    "### 1.1 Docker containers\n",
    "Please make sure that you have started the notebook inside the running NGC docker container: `nvcr.io/nvidia/merlin/merlin-training:0.5`. The HugeCTR Python interface have been installed to the system path `/usr/local/hugectr/lib/`. Besides, this system path is added to the environment variable `PYTHONPATH`, which means that you can use the HugeCTR Python interface within the docker container environment.\n",
    "\n",
    "### 1.2 Hardware\n",
    "This notebook requires a Pascal, Volta, Turing, Ampere or newer GPUs, such as P100, V100, T4 or A100. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr 27 05:03:43 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 455.34       Driver Version: 455.34       CUDA Version: 11.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  A100-PCIE-40GB      On   | 00000000:01:00.0 Off |                    0 |\n",
      "| N/A   28C    P0    29W / 250W |      0MiB / 40536MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  A100-PCIE-40GB      On   | 00000000:24:00.0 Off |                    0 |\n",
      "| N/A   28C    P0    30W / 250W |      0MiB / 40536MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  A100-PCIE-40GB      On   | 00000000:41:00.0 Off |                    0 |\n",
      "| N/A   28C    P0    31W / 250W |      0MiB / 40536MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  A100-PCIE-40GB      On   | 00000000:61:00.0 Off |                    0 |\n",
      "| N/A   28C    P0    31W / 250W |      0MiB / 40536MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  A100-PCIE-40GB      On   | 00000000:81:00.0 Off |                    0 |\n",
      "| N/A   27C    P0    30W / 250W |      0MiB / 40536MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  A100-PCIE-40GB      On   | 00000000:A1:00.0 Off |                    0 |\n",
      "| N/A   28C    P0    31W / 250W |      0MiB / 40536MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  A100-PCIE-40GB      On   | 00000000:C1:00.0 Off |                    0 |\n",
      "| N/A   27C    P0    31W / 250W |      0MiB / 40536MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  A100-PCIE-40GB      On   | 00000000:E1:00.0 Off |                    0 |\n",
      "| N/A   28C    P0    31W / 250W |      0MiB / 40536MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "## 2. Data download and preprocessing\n",
    "\n",
    "We first install a few extra utilities for data preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and installing 'tqdm' package.\n",
      "Downloading and installing 'unzip' command\n",
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /opt/conda/envs/merlin\n",
      "\n",
      "  added / updated specs:\n",
      "    - unzip\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    unzip-6.0                  |       h7f98852_2         143 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         143 KB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  unzip              conda-forge/linux-64::unzip-6.0-h7f98852_2\n",
      "\n",
      "\n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n"
     ]
    }
   ],
   "source": [
    "print(\"Downloading and installing 'tqdm' package.\")\n",
    "!pip3 -q install torch tqdm\n",
    "\n",
    "print(\"Downloading and installing 'unzip' command\")\n",
    "!conda install -y -q -c conda-forge unzip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we download and unzip the movie lens 20M [dataset](https://grouplens.org/datasets/movielens/20m/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and extracting 'Movie Lens 20M' dataset.\n",
      "Archive:  data/ml-20m.zip\n",
      "hugeCTR  ml-20m  ml-20m.zip  test.parquet  train.parquet  valid.parquet\n"
     ]
    }
   ],
   "source": [
    "print(\"Downloading and extracting 'Movie Lens 20M' dataset.\")\n",
    "!wget -nc http://files.grouplens.org/datasets/movielens/ml-20m.zip -P data -q --show-progress\n",
    "!unzip -n data/ml-20m.zip -d data\n",
    "!ls ./data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Movie lens data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_RATINGS = 20\n",
    "USER_COLUMN = 'userId'\n",
    "ITEM_COLUMN = 'movieId'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we read the data into a Pandas dataframe, and encode userID and itemID with integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering out users with less than 20 ratings\n",
      "Mapping original user and item IDs to new sequential IDs\n",
      "Number of users: 138493\n",
      "Number of items: 26744\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./data/ml-20m/ratings.csv')\n",
    "print(\"Filtering out users with less than {} ratings\".format(MIN_RATINGS))\n",
    "grouped = df.groupby(USER_COLUMN)\n",
    "df = grouped.filter(lambda x: len(x) >= MIN_RATINGS)\n",
    "\n",
    "print(\"Mapping original user and item IDs to new sequential IDs\")\n",
    "df[USER_COLUMN], unique_users = pd.factorize(df[USER_COLUMN])\n",
    "df[ITEM_COLUMN], unique_items = pd.factorize(df[ITEM_COLUMN])\n",
    "\n",
    "nb_users = len(unique_users)\n",
    "nb_items = len(unique_items)\n",
    "\n",
    "print(\"Number of users: %d\\nNumber of items: %d\"%(len(unique_users), len(unique_items)))\n",
    "\n",
    "# Save the mapping to do the inference later on\n",
    "import pickle\n",
    "with open('./mappings.pickle', 'wb') as handle:\n",
    "    pickle.dump({\"users\": unique_users, \"items\": unique_items}, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we split the data into a train and test set, the last movie each user has recently seen will be used for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to sort before popping to get the last item\n",
    "df.sort_values(by='timestamp', inplace=True)\n",
    "    \n",
    "# clean up data\n",
    "del df['rating'], df['timestamp']\n",
    "df = df.drop_duplicates() # assuming it keeps order\n",
    "\n",
    "# now we have filtered and sorted by time data, we can split test data out\n",
    "grouped_sorted = df.groupby(USER_COLUMN, group_keys=False)\n",
    "test_data = grouped_sorted.tail(1).sort_values(by=USER_COLUMN)\n",
    "# need to pop for each group\n",
    "train_data = grouped_sorted.apply(lambda x: x.iloc[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0</td>\n",
       "      <td>86</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    userId  movieId  target\n",
       "20       0       20       1\n",
       "19       0       19       1\n",
       "86       0       86       1\n",
       "61       0       61       1\n",
       "23       0       23       1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['target']=1\n",
    "test_data['target']=1\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the movie lens data contains only positive examples, let us first define an utility function to generate negative samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _TestNegSampler:\n",
    "    def __init__(self, train_ratings, nb_users, nb_items, nb_neg):\n",
    "        self.nb_neg = nb_neg\n",
    "        self.nb_users = nb_users \n",
    "        self.nb_items = nb_items \n",
    "\n",
    "        # compute unique ids for quickly created hash set and fast lookup\n",
    "        ids = (train_ratings[:, 0] * self.nb_items) + train_ratings[:, 1]\n",
    "        self.set = set(ids)\n",
    "\n",
    "    def generate(self, batch_size=128*1024):\n",
    "        users = torch.arange(0, self.nb_users).reshape([1, -1]).repeat([self.nb_neg, 1]).transpose(0, 1).reshape(-1)\n",
    "\n",
    "        items = [-1] * len(users)\n",
    "\n",
    "        random_items = torch.LongTensor(batch_size).random_(0, self.nb_items).tolist()\n",
    "        print('Generating validation negatives...')\n",
    "        for idx, u in enumerate(tqdm.tqdm(users.tolist())):\n",
    "            if not random_items:\n",
    "                random_items = torch.LongTensor(batch_size).random_(0, self.nb_items).tolist()\n",
    "            j = random_items.pop()\n",
    "            while u * self.nb_items + j in self.set:\n",
    "                if not random_items:\n",
    "                    random_items = torch.LongTensor(batch_size).random_(0, self.nb_items).tolist()\n",
    "                j = random_items.pop()\n",
    "\n",
    "            items[idx] = j\n",
    "        items = torch.LongTensor(items)\n",
    "        return items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we generate the negative samples for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating validation negatives...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 69246500/69246500 [01:05<00:00, 1059133.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating validation negatives...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13849300/13849300 [00:13<00:00, 1048928.58it/s]\n"
     ]
    }
   ],
   "source": [
    "sampler = _TestNegSampler(df.values, nb_users, nb_items, 500)  # using 500 negative samples\n",
    "train_negs = sampler.generate()\n",
    "train_negs = train_negs.reshape(-1, 500)\n",
    "\n",
    "sampler = _TestNegSampler(df.values, nb_users, nb_items, 100)  # using 100 negative samples\n",
    "test_negs = sampler.generate()\n",
    "test_negs = test_negs.reshape(-1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 138493/138493 [07:06<00:00, 324.79it/s]\n",
      "100%|██████████| 138493/138493 [01:24<00:00, 1635.21it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# generating negative samples for training\n",
    "train_data_neg = np.zeros((train_negs.shape[0]*train_negs.shape[1],3), dtype=int)\n",
    "idx = 0\n",
    "for i in tqdm.tqdm(range(train_negs.shape[0])):\n",
    "    for j in range(train_negs.shape[1]):\n",
    "        train_data_neg[idx, 0] = i # user ID\n",
    "        train_data_neg[idx, 1] = train_negs[i, j] # negative item ID\n",
    "        idx += 1\n",
    "    \n",
    "# generating negative samples for testing\n",
    "test_data_neg = np.zeros((test_negs.shape[0]*test_negs.shape[1],3), dtype=int)\n",
    "idx = 0\n",
    "for i in tqdm.tqdm(range(test_negs.shape[0])):\n",
    "    for j in range(test_negs.shape[1]):\n",
    "        test_data_neg[idx, 0] = i\n",
    "        test_data_neg[idx, 1] = test_negs[i, j]\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_np= np.concatenate([train_data_neg, train_data.values])\n",
    "np.random.shuffle(train_data_np)\n",
    "\n",
    "test_data_np= np.concatenate([test_data_neg, test_data.values])\n",
    "np.random.shuffle(test_data_np)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HugeCTR expect user ID and item ID to be different, so we use 0 -> nb_users for user IDs and\n",
    "# nb_users -> nb_users+nb_items for item IDs.\n",
    "train_data_np[:,1] += nb_users \n",
    "test_data_np[:,1] += nb_users "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "165236"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(train_data_np[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write HugeCTR data files\n",
    "\n",
    "Next, we will write the data to disk using HugeCTR [Norm](../docs/configuration_file_setup.md#norm) dataset format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ctypes import c_longlong as ll\n",
    "from ctypes import c_uint\n",
    "from ctypes import c_float\n",
    "from ctypes import c_int\n",
    "\n",
    "def write_hugeCTR_data(huge_ctr_data, filename='huge_ctr_data.dat'):\n",
    "    print(\"Writing %d samples\"%huge_ctr_data.shape[0])\n",
    "    with open(filename, 'wb') as f:\n",
    "        #write header\n",
    "        f.write(ll(0)) # 0: no error check; 1: check_num\n",
    "        f.write(ll(huge_ctr_data.shape[0])) # the number of samples in this data file\n",
    "        f.write(ll(1)) # dimension of label\n",
    "        f.write(ll(1)) # dimension of dense feature\n",
    "        f.write(ll(2)) # long long slot_num\n",
    "        for _ in range(3): f.write(ll(0)) # reserved for future use\n",
    "\n",
    "        for i in tqdm.tqdm(range(huge_ctr_data.shape[0])):\n",
    "            f.write(c_float(huge_ctr_data[i,2])) # float label[label_dim];\n",
    "            f.write(c_float(0)) # dummy dense feature\n",
    "            f.write(c_int(1)) # slot 1 nnz: user ID\n",
    "            f.write(c_uint(huge_ctr_data[i,0]))\n",
    "            f.write(c_int(1)) # slot 2 nnz: item ID\n",
    "            f.write(c_uint(huge_ctr_data[i,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_filelist(filelist_name, num_files, filename_prefix):\n",
    "    with open(filelist_name, 'wt') as f:\n",
    "        f.write('{0}\\n'.format(num_files));\n",
    "        for i in range(num_files):\n",
    "            f.write('{0}_{1}.dat\\n'.format(filename_prefix, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 8910827 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8910827/8910827 [00:35<00:00, 249365.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 8910827 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8910827/8910827 [00:35<00:00, 250680.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 8910827 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8910827/8910827 [00:35<00:00, 251559.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 8910827 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8910827/8910827 [00:35<00:00, 253939.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 8910827 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8910827/8910827 [00:34<00:00, 255285.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 8910827 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8910827/8910827 [00:34<00:00, 254706.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 8910827 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8910827/8910827 [00:35<00:00, 251623.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 8910827 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8910827/8910827 [00:36<00:00, 247415.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 8910827 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8910827/8910827 [00:35<00:00, 247776.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 8910827 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8910827/8910827 [00:36<00:00, 247428.62it/s]\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./data/hugeCTR\n",
    "!mkdir ./data/hugeCTR\n",
    "\n",
    "for i, data_arr in enumerate(np.array_split(train_data_np,10)):\n",
    "    write_hugeCTR_data(data_arr, filename='./data/hugeCTR/train_huge_ctr_data_%d.dat'%i)\n",
    "\n",
    "generate_filelist('./data/hugeCTR/train_filelist.txt', 10, './data/hugeCTR/train_huge_ctr_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 1398780 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1398780/1398780 [00:05<00:00, 241195.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 1398780 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1398780/1398780 [00:05<00:00, 245859.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 1398780 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1398780/1398780 [00:05<00:00, 250170.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 1398779 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1398779/1398779 [00:05<00:00, 253676.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 1398779 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1398779/1398779 [00:05<00:00, 250075.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 1398779 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1398779/1398779 [00:05<00:00, 250036.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 1398779 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1398779/1398779 [00:05<00:00, 244384.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 1398779 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1398779/1398779 [00:05<00:00, 249214.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 1398779 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1398779/1398779 [00:05<00:00, 250584.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 1398779 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1398779/1398779 [00:05<00:00, 249278.58it/s]"
     ]
    }
   ],
   "source": [
    "for i, data_arr in enumerate(np.array_split(test_data_np,10)):\n",
    "    write_hugeCTR_data(data_arr, filename='./data/hugeCTR/test_huge_ctr_data_%d.dat'%i)\n",
    "    \n",
    "generate_filelist('./data/hugeCTR/test_filelist.txt', 10, './data/hugeCTR/test_huge_ctr_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "## 3. HugeCTR DLRM training\n",
    "\n",
    "In this section, we will train a DLRM network on the augmented movie lens data. First, we write the training Python script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hugectr_dlrm_movielens.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hugectr_dlrm_movielens.py\n",
    "import hugectr\n",
    "from mpi4py import MPI\n",
    "solver = hugectr.CreateSolver(max_eval_batches = 1000,\n",
    "                              batchsize_eval = 65536,\n",
    "                              batchsize = 65536,\n",
    "                              lr = 0.1,\n",
    "                              warmup_steps = 1000,\n",
    "                              decay_start = 10000,\n",
    "                              decay_steps = 40000,\n",
    "                              decay_power = 2.0,\n",
    "                              end_lr = 1e-5,\n",
    "                              vvgpu = [[0]],\n",
    "                              repeat_dataset = True,\n",
    "                              use_mixed_precision = True,\n",
    "                              scaler = 1024)\n",
    "reader = hugectr.DataReaderParams(data_reader_type = hugectr.DataReaderType_t.Norm,\n",
    "                                  source = [\"./data/hugeCTR/train_filelist.txt\"],\n",
    "                                  eval_source = \"./data/hugeCTR/test_filelist.txt\",\n",
    "                                  check_type = hugectr.Check_t.Non,\n",
    "                                  slot_size_array = [138493 , 26744])\n",
    "optimizer = hugectr.CreateOptimizer(optimizer_type = hugectr.Optimizer_t.SGD,\n",
    "                                    update_type = hugectr.Update_t.Local,\n",
    "                                    atomic_update = True)\n",
    "model = hugectr.Model(solver, reader, optimizer)\n",
    "model.add(hugectr.Input(label_dim = 1, label_name = \"label\",\n",
    "                        dense_dim = 1, dense_name = \"dense\",\n",
    "                        data_reader_sparse_param_array = \n",
    "                        [hugectr.DataReaderSparseParam(hugectr.DataReaderSparse_t.Localized, 2, 1, 2)],\n",
    "                        sparse_names = [\"data1\"]))\n",
    "model.add(hugectr.SparseEmbedding(embedding_type = hugectr.Embedding_t.LocalizedSlotSparseEmbeddingHash, \n",
    "                            slot_size_array = [138493 , 26744],\n",
    "                            max_vocabulary_size_per_gpu = 165237,\n",
    "                            embedding_vec_size = 64,\n",
    "                            combiner = 0,\n",
    "                            sparse_embedding_name = \"sparse_embedding1\",\n",
    "                            bottom_name = \"data1\",\n",
    "                            optimizer = optimizer))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.FusedInnerProduct,\n",
    "                            bottom_names = [\"dense\"],\n",
    "                            top_names = [\"fc1\"],\n",
    "                            num_output=64))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.FusedInnerProduct,\n",
    "                            bottom_names = [\"fc1\"],\n",
    "                            top_names = [\"fc2\"],\n",
    "                            num_output=128))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.FusedInnerProduct,\n",
    "                            bottom_names = [\"fc2\"],\n",
    "                            top_names = [\"fc3\"],\n",
    "                            num_output=64))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Interaction,\n",
    "                            bottom_names = [\"fc3\",\"sparse_embedding1\"],\n",
    "                            top_names = [\"interaction1\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.FusedInnerProduct,\n",
    "                            bottom_names = [\"interaction1\"],\n",
    "                            top_names = [\"fc4\"],\n",
    "                            num_output=1024))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.FusedInnerProduct,\n",
    "                            bottom_names = [\"fc4\"],\n",
    "                            top_names = [\"fc5\"],\n",
    "                            num_output=1024))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.FusedInnerProduct,\n",
    "                            bottom_names = [\"fc5\"],\n",
    "                            top_names = [\"fc6\"],\n",
    "                            num_output=512))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.FusedInnerProduct,\n",
    "                            bottom_names = [\"fc6\"],\n",
    "                            top_names = [\"fc7\"],\n",
    "                            num_output=256))                                                  \n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"fc7\"],\n",
    "                            top_names = [\"fc8\"],\n",
    "                            num_output=1))                                                                                           \n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.BinaryCrossEntropyLoss,\n",
    "                            bottom_names = [\"fc8\", \"label\"],\n",
    "                            top_names = [\"loss\"]))\n",
    "model.compile()\n",
    "model.summary()\n",
    "model.fit(max_iter = 50000, display = 1000, eval_interval = 3000, snapshot = 3000, snapshot_prefix = \"./hugeCTR_saved_model_DLRM/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ./hugeCTR_saved_model_DLRM/\n",
    "!mkdir ./hugeCTR_saved_model_DLRM/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================Model Init=====================================================\n",
      "[27d05h37m43s][HUGECTR][INFO]: Global seed is 1972631303\n",
      "[27d05h37m44s][HUGECTR][INFO]: Peer-to-peer access cannot be fully enabled.\n",
      "Device 0: A100-PCIE-40GB\n",
      "[27d05h37m44s][HUGECTR][INFO]: num of DataReader workers: 12\n",
      "[27d05h37m44s][HUGECTR][INFO]: num_internal_buffers 1\n",
      "[27d05h37m45s][HUGECTR][INFO]: num_internal_buffers 1\n",
      "[27d05h37m45s][HUGECTR][INFO]: max_vocabulary_size_per_gpu_=165237\n",
      "[27d05h37m45s][HUGECTR][INFO]: All2All Warmup Start\n",
      "[27d05h37m45s][HUGECTR][INFO]: All2All Warmup End\n",
      "===================================================Model Compile===================================================\n",
      "[27d05h37m58s][HUGECTR][INFO]: gpu0 start to init embedding of slot0 , slot_size=138493, key_offset=0, value_index_offset=0\n",
      "[27d05h37m58s][HUGECTR][INFO]: gpu0 start to init embedding of slot1 , slot_size=26744, key_offset=138493, value_index_offset=138493\n",
      "[27d05h37m58s][HUGECTR][INFO]: gpu0 init embedding done\n",
      "===================================================Model Summary===================================================\n",
      "Label                                   Dense                         Sparse                        \n",
      "label                                   dense                          data1                         \n",
      "(None, 1)                               (None, 1)                               \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Layer Type                              Input Name                    Output Name                   Output Shape                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "LocalizedSlotSparseEmbeddingHash        data1                         sparse_embedding1             (None, 2, 64)                 \n",
      "FusedInnerProduct                       dense                         fc1                           (None, 64)                    \n",
      "FusedInnerProduct                       fc1                           fc2                           (None, 128)                   \n",
      "FusedInnerProduct                       fc2                           fc3                           (None, 64)                    \n",
      "Interaction                             fc3,sparse_embedding1         interaction1                  (None, 68)                    \n",
      "FusedInnerProduct                       interaction1                  fc4                           (None, 1024)                  \n",
      "FusedInnerProduct                       fc4                           fc5                           (None, 1024)                  \n",
      "FusedInnerProduct                       fc5                           fc6                           (None, 512)                   \n",
      "FusedInnerProduct                       fc6                           fc7                           (None, 256)                   \n",
      "InnerProduct                            fc7                           fc8                           (None, 1)                     \n",
      "BinaryCrossEntropyLoss                  fc8,label                     loss                                                        \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "=====================================================Model Fit=====================================================\n",
      "[27d50h37m58s][HUGECTR][INFO]: Use non-epoch mode with number of iterations: 50000\n",
      "[27d50h37m58s][HUGECTR][INFO]: Training batchsize: 65536, evaluation batchsize: 65536\n",
      "[27d50h37m58s][HUGECTR][INFO]: Evaluation interval: 3000, snapshot interval: 3000\n",
      "[27d50h37m58s][HUGECTR][INFO]: Sparse embedding trainable: 1, dense network trainable: 1\n",
      "[27d50h37m58s][HUGECTR][INFO]: Use mixed precision: 1, scaler: 1024.000000, use cuda graph: 1\n",
      "[27d50h37m58s][HUGECTR][INFO]: lr: 0.100000, warmup_steps: 1000, decay_start: 10000, decay_steps: 40000, decay_power: 2.000000, end_lr: 0.000010\n",
      "[27d50h37m58s][HUGECTR][INFO]: Training source file: ./data/hugeCTR/train_filelist.txt\n",
      "[27d50h37m58s][HUGECTR][INFO]: Evaluation source file: ./data/hugeCTR/test_filelist.txt\n",
      "[27d50h38m10s][HUGECTR][INFO]: Iter: 1000 Time(1000 iters): 11.936485s Loss: 0.530086 lr:0.100000\n",
      "[27d50h38m21s][HUGECTR][INFO]: Iter: 2000 Time(1000 iters): 10.662183s Loss: 0.532817 lr:0.100000\n",
      "[27d50h38m28s][HUGECTR][INFO]: Iter: 3000 Time(1000 iters): 7.627635s Loss: 0.525166 lr:0.100000\n",
      "[27d50h38m34s][HUGECTR][INFO]: Evaluation, AUC: 0.762257\n",
      "[27d50h38m34s][HUGECTR][INFO]: Eval Time for 1000 iters: 5.384788s\n",
      "[27d50h38m34s][HUGECTR][INFO]: Rank0: Dump hash table from GPU0\n",
      "[27d50h38m34s][HUGECTR][INFO]: Rank0: Write hash table <key,slot_id,value> pairs to file\n",
      "[27d50h38m34s][HUGECTR][INFO]: Done\n",
      "[27d50h38m34s][HUGECTR][INFO]: Dumping sparse weights to files, successful\n",
      "[27d50h38m34s][HUGECTR][INFO]: Dumping sparse optimzer states to files, successful\n",
      "[27d50h38m34s][HUGECTR][INFO]: Dumping dense weights to file, successful\n",
      "[27d50h38m34s][HUGECTR][INFO]: Dumping dense optimizer states to file, successful\n",
      "[27d50h38m34s][HUGECTR][INFO]: Dumping untrainable weights to file, successful\n",
      "[27d50h38m42s][HUGECTR][INFO]: Iter: 4000 Time(1000 iters): 13.657525s Loss: 0.525948 lr:0.100000\n",
      "[27d50h38m50s][HUGECTR][INFO]: Iter: 5000 Time(1000 iters): 7.707744s Loss: 0.276702 lr:0.100000\n",
      "[27d50h38m57s][HUGECTR][INFO]: Iter: 6000 Time(1000 iters): 7.778739s Loss: 0.233238 lr:0.100000\n",
      "[27d50h39m10s][HUGECTR][INFO]: Evaluation, AUC: 0.948334\n",
      "[27d50h39m10s][HUGECTR][INFO]: Eval Time for 1000 iters: 3.018621s\n",
      "[27d50h39m10s][HUGECTR][INFO]: Rank0: Dump hash table from GPU0\n",
      "[27d50h39m10s][HUGECTR][INFO]: Rank0: Write hash table <key,slot_id,value> pairs to file\n",
      "[27d50h39m10s][HUGECTR][INFO]: Done\n",
      "[27d50h39m10s][HUGECTR][INFO]: Dumping sparse weights to files, successful\n",
      "[27d50h39m10s][HUGECTR][INFO]: Dumping sparse optimzer states to files, successful\n",
      "[27d50h39m10s][HUGECTR][INFO]: Dumping dense weights to file, successful\n",
      "[27d50h39m10s][HUGECTR][INFO]: Dumping dense optimizer states to file, successful\n",
      "[27d50h39m10s][HUGECTR][INFO]: Dumping untrainable weights to file, successful\n",
      "[27d50h39m90s][HUGECTR][INFO]: Iter: 7000 Time(1000 iters): 11.376806s Loss: 0.228977 lr:0.100000\n",
      "[27d50h39m17s][HUGECTR][INFO]: Iter: 8000 Time(1000 iters): 7.771812s Loss: 0.215747 lr:0.100000\n",
      "[27d50h39m24s][HUGECTR][INFO]: Iter: 9000 Time(1000 iters): 7.774650s Loss: 0.199474 lr:0.100000\n",
      "[27d50h39m27s][HUGECTR][INFO]: Evaluation, AUC: 0.950375\n",
      "[27d50h39m27s][HUGECTR][INFO]: Eval Time for 1000 iters: 3.025485s\n",
      "[27d50h39m27s][HUGECTR][INFO]: Rank0: Dump hash table from GPU0\n",
      "[27d50h39m28s][HUGECTR][INFO]: Rank0: Write hash table <key,slot_id,value> pairs to file\n",
      "[27d50h39m28s][HUGECTR][INFO]: Done\n",
      "[27d50h39m28s][HUGECTR][INFO]: Dumping sparse weights to files, successful\n",
      "[27d50h39m28s][HUGECTR][INFO]: Dumping sparse optimzer states to files, successful\n",
      "[27d50h39m28s][HUGECTR][INFO]: Dumping dense weights to file, successful\n",
      "[27d50h39m28s][HUGECTR][INFO]: Dumping dense optimizer states to file, successful\n",
      "[27d50h39m28s][HUGECTR][INFO]: Dumping untrainable weights to file, successful\n",
      "[27d50h39m36s][HUGECTR][INFO]: Iter: 10000 Time(1000 iters): 11.406690s Loss: 0.177070 lr:0.099995\n",
      "[27d50h39m44s][HUGECTR][INFO]: Iter: 11000 Time(1000 iters): 7.772864s Loss: 0.173291 lr:0.095058\n",
      "[27d50h39m51s][HUGECTR][INFO]: Iter: 12000 Time(1000 iters): 7.780023s Loss: 0.163796 lr:0.090245\n",
      "[27d50h39m54s][HUGECTR][INFO]: Evaluation, AUC: 0.948818\n",
      "[27d50h39m54s][HUGECTR][INFO]: Eval Time for 1000 iters: 3.040194s\n",
      "[27d50h39m54s][HUGECTR][INFO]: Rank0: Dump hash table from GPU0\n",
      "[27d50h39m54s][HUGECTR][INFO]: Rank0: Write hash table <key,slot_id,value> pairs to file\n",
      "[27d50h39m55s][HUGECTR][INFO]: Done\n",
      "[27d50h39m55s][HUGECTR][INFO]: Dumping sparse weights to files, successful\n",
      "[27d50h39m55s][HUGECTR][INFO]: Dumping sparse optimzer states to files, successful\n",
      "[27d50h39m55s][HUGECTR][INFO]: Dumping dense weights to file, successful\n",
      "[27d50h39m55s][HUGECTR][INFO]: Dumping dense optimizer states to file, successful\n",
      "[27d50h39m55s][HUGECTR][INFO]: Dumping untrainable weights to file, successful\n",
      "[27d50h40m30s][HUGECTR][INFO]: Iter: 13000 Time(1000 iters): 11.389084s Loss: 0.161345 lr:0.085558\n",
      "[27d50h40m11s][HUGECTR][INFO]: Iter: 14000 Time(1000 iters): 7.777649s Loss: 0.156193 lr:0.080996\n",
      "[27d50h40m18s][HUGECTR][INFO]: Iter: 15000 Time(1000 iters): 7.780823s Loss: 0.155823 lr:0.076558\n",
      "[27d50h40m21s][HUGECTR][INFO]: Evaluation, AUC: 0.944885\n",
      "[27d50h40m21s][HUGECTR][INFO]: Eval Time for 1000 iters: 3.053025s\n",
      "[27d50h40m21s][HUGECTR][INFO]: Rank0: Dump hash table from GPU0\n",
      "[27d50h40m21s][HUGECTR][INFO]: Rank0: Write hash table <key,slot_id,value> pairs to file\n",
      "[27d50h40m21s][HUGECTR][INFO]: Done\n",
      "[27d50h40m22s][HUGECTR][INFO]: Dumping sparse weights to files, successful\n",
      "[27d50h40m22s][HUGECTR][INFO]: Dumping sparse optimzer states to files, successful\n",
      "[27d50h40m22s][HUGECTR][INFO]: Dumping dense weights to file, successful\n",
      "[27d50h40m22s][HUGECTR][INFO]: Dumping dense optimizer states to file, successful\n",
      "[27d50h40m22s][HUGECTR][INFO]: Dumping untrainable weights to file, successful\n",
      "[27d50h40m30s][HUGECTR][INFO]: Iter: 16000 Time(1000 iters): 11.401766s Loss: 0.156800 lr:0.072246\n",
      "[27d50h40m37s][HUGECTR][INFO]: Iter: 17000 Time(1000 iters): 7.778629s Loss: 0.155561 lr:0.068058\n",
      "[27d50h40m45s][HUGECTR][INFO]: Iter: 18000 Time(1000 iters): 7.784451s Loss: 0.153913 lr:0.063996\n",
      "[27d50h40m48s][HUGECTR][INFO]: Evaluation, AUC: 0.943252\n",
      "[27d50h40m48s][HUGECTR][INFO]: Eval Time for 1000 iters: 3.072313s\n",
      "[27d50h40m48s][HUGECTR][INFO]: Rank0: Dump hash table from GPU0\n",
      "[27d50h40m48s][HUGECTR][INFO]: Rank0: Write hash table <key,slot_id,value> pairs to file\n",
      "[27d50h40m48s][HUGECTR][INFO]: Done\n",
      "[27d50h40m49s][HUGECTR][INFO]: Dumping sparse weights to files, successful\n",
      "[27d50h40m49s][HUGECTR][INFO]: Dumping sparse optimzer states to files, successful\n",
      "[27d50h40m49s][HUGECTR][INFO]: Dumping dense weights to file, successful\n",
      "[27d50h40m49s][HUGECTR][INFO]: Dumping dense optimizer states to file, successful\n",
      "[27d50h40m49s][HUGECTR][INFO]: Dumping untrainable weights to file, successful\n",
      "[27d50h40m57s][HUGECTR][INFO]: Iter: 19000 Time(1000 iters): 11.429412s Loss: 0.155201 lr:0.060059\n",
      "[27d50h41m40s][HUGECTR][INFO]: Iter: 20000 Time(1000 iters): 7.791202s Loss: 0.153704 lr:0.056246\n",
      "[27d50h41m12s][HUGECTR][INFO]: Iter: 21000 Time(1000 iters): 7.784212s Loss: 0.151300 lr:0.052559\n",
      "[27d50h41m15s][HUGECTR][INFO]: Evaluation, AUC: 0.943599\n",
      "[27d50h41m15s][HUGECTR][INFO]: Eval Time for 1000 iters: 3.076807s\n",
      "[27d50h41m15s][HUGECTR][INFO]: Rank0: Dump hash table from GPU0\n",
      "[27d50h41m15s][HUGECTR][INFO]: Rank0: Write hash table <key,slot_id,value> pairs to file\n",
      "[27d50h41m15s][HUGECTR][INFO]: Done\n",
      "[27d50h41m16s][HUGECTR][INFO]: Dumping sparse weights to files, successful\n",
      "[27d50h41m16s][HUGECTR][INFO]: Dumping sparse optimzer states to files, successful\n",
      "[27d50h41m16s][HUGECTR][INFO]: Dumping dense weights to file, successful\n",
      "[27d50h41m16s][HUGECTR][INFO]: Dumping dense optimizer states to file, successful\n",
      "[27d50h41m16s][HUGECTR][INFO]: Dumping untrainable weights to file, successful\n",
      "[27d50h41m24s][HUGECTR][INFO]: Iter: 22000 Time(1000 iters): 11.440185s Loss: 0.152405 lr:0.048997\n",
      "[27d50h41m32s][HUGECTR][INFO]: Iter: 23000 Time(1000 iters): 7.782261s Loss: 0.151492 lr:0.045559\n",
      "[27d50h41m39s][HUGECTR][INFO]: Iter: 24000 Time(1000 iters): 7.782142s Loss: 0.151045 lr:0.042247\n",
      "[27d50h41m42s][HUGECTR][INFO]: Evaluation, AUC: 0.943284\n",
      "[27d50h41m42s][HUGECTR][INFO]: Eval Time for 1000 iters: 3.067378s\n",
      "[27d50h41m42s][HUGECTR][INFO]: Rank0: Dump hash table from GPU0\n",
      "[27d50h41m42s][HUGECTR][INFO]: Rank0: Write hash table <key,slot_id,value> pairs to file\n",
      "[27d50h41m42s][HUGECTR][INFO]: Done\n",
      "[27d50h41m43s][HUGECTR][INFO]: Dumping sparse weights to files, successful\n",
      "[27d50h41m43s][HUGECTR][INFO]: Dumping sparse optimzer states to files, successful\n",
      "[27d50h41m43s][HUGECTR][INFO]: Dumping dense weights to file, successful\n",
      "[27d50h41m43s][HUGECTR][INFO]: Dumping dense optimizer states to file, successful\n",
      "[27d50h41m43s][HUGECTR][INFO]: Dumping untrainable weights to file, successful\n",
      "[27d50h41m51s][HUGECTR][INFO]: Iter: 25000 Time(1000 iters): 11.435866s Loss: 0.149226 lr:0.039059\n",
      "[27d50h41m59s][HUGECTR][INFO]: Iter: 26000 Time(1000 iters): 7.784969s Loss: 0.153820 lr:0.035997\n",
      "[27d50h42m60s][HUGECTR][INFO]: Iter: 27000 Time(1000 iters): 7.783091s Loss: 0.151346 lr:0.033060\n",
      "[27d50h42m90s][HUGECTR][INFO]: Evaluation, AUC: 0.943096\n",
      "[27d50h42m90s][HUGECTR][INFO]: Eval Time for 1000 iters: 3.074738s\n",
      "[27d50h42m90s][HUGECTR][INFO]: Rank0: Dump hash table from GPU0\n",
      "[27d50h42m90s][HUGECTR][INFO]: Rank0: Write hash table <key,slot_id,value> pairs to file\n",
      "[27d50h42m90s][HUGECTR][INFO]: Done\n",
      "[27d50h42m10s][HUGECTR][INFO]: Dumping sparse weights to files, successful\n",
      "[27d50h42m10s][HUGECTR][INFO]: Dumping sparse optimzer states to files, successful\n",
      "[27d50h42m10s][HUGECTR][INFO]: Dumping dense weights to file, successful\n",
      "[27d50h42m10s][HUGECTR][INFO]: Dumping dense optimizer states to file, successful\n",
      "[27d50h42m10s][HUGECTR][INFO]: Dumping untrainable weights to file, successful\n",
      "[27d50h42m18s][HUGECTR][INFO]: Iter: 28000 Time(1000 iters): 11.452641s Loss: 0.152006 lr:0.030247\n",
      "[27d50h42m26s][HUGECTR][INFO]: Iter: 29000 Time(1000 iters): 7.786806s Loss: 0.149876 lr:0.027560\n",
      "[27d50h42m33s][HUGECTR][INFO]: Iter: 30000 Time(1000 iters): 7.792531s Loss: 0.151513 lr:0.024998\n",
      "[27d50h42m36s][HUGECTR][INFO]: Evaluation, AUC: 0.942826\n",
      "[27d50h42m36s][HUGECTR][INFO]: Eval Time for 1000 iters: 3.083603s\n",
      "[27d50h42m36s][HUGECTR][INFO]: Rank0: Dump hash table from GPU0\n",
      "[27d50h42m36s][HUGECTR][INFO]: Rank0: Write hash table <key,slot_id,value> pairs to file\n",
      "[27d50h42m37s][HUGECTR][INFO]: Done\n",
      "[27d50h42m37s][HUGECTR][INFO]: Dumping sparse weights to files, successful\n",
      "[27d50h42m37s][HUGECTR][INFO]: Dumping sparse optimzer states to files, successful\n",
      "[27d50h42m37s][HUGECTR][INFO]: Dumping dense weights to file, successful\n",
      "[27d50h42m37s][HUGECTR][INFO]: Dumping dense optimizer states to file, successful\n",
      "[27d50h42m37s][HUGECTR][INFO]: Dumping untrainable weights to file, successful\n",
      "[27d50h42m45s][HUGECTR][INFO]: Iter: 31000 Time(1000 iters): 11.455721s Loss: 0.151030 lr:0.022560\n",
      "[27d50h42m53s][HUGECTR][INFO]: Iter: 32000 Time(1000 iters): 7.782581s Loss: 0.152924 lr:0.020248\n",
      "[27d50h43m00s][HUGECTR][INFO]: Iter: 33000 Time(1000 iters): 7.786242s Loss: 0.152005 lr:0.018060\n",
      "[27d50h43m30s][HUGECTR][INFO]: Evaluation, AUC: 0.942823\n",
      "[27d50h43m30s][HUGECTR][INFO]: Eval Time for 1000 iters: 3.053822s\n",
      "[27d50h43m30s][HUGECTR][INFO]: Rank0: Dump hash table from GPU0\n",
      "[27d50h43m30s][HUGECTR][INFO]: Rank0: Write hash table <key,slot_id,value> pairs to file\n",
      "[27d50h43m30s][HUGECTR][INFO]: Done\n",
      "[27d50h43m40s][HUGECTR][INFO]: Dumping sparse weights to files, successful\n",
      "[27d50h43m40s][HUGECTR][INFO]: Dumping sparse optimzer states to files, successful\n",
      "[27d50h43m40s][HUGECTR][INFO]: Dumping dense weights to file, successful\n",
      "[27d50h43m40s][HUGECTR][INFO]: Dumping dense optimizer states to file, successful\n",
      "[27d50h43m40s][HUGECTR][INFO]: Dumping untrainable weights to file, successful\n",
      "[27d50h43m12s][HUGECTR][INFO]: Iter: 34000 Time(1000 iters): 11.396781s Loss: 0.150203 lr:0.015998\n",
      "[27d50h43m20s][HUGECTR][INFO]: Iter: 35000 Time(1000 iters): 7.784927s Loss: 0.151163 lr:0.014061\n",
      "[27d50h43m27s][HUGECTR][INFO]: Iter: 36000 Time(1000 iters): 7.789356s Loss: 0.154688 lr:0.012248\n",
      "[27d50h43m30s][HUGECTR][INFO]: Evaluation, AUC: 0.942963\n",
      "[27d50h43m30s][HUGECTR][INFO]: Eval Time for 1000 iters: 2.981289s\n",
      "[27d50h43m30s][HUGECTR][INFO]: Rank0: Dump hash table from GPU0\n",
      "[27d50h43m30s][HUGECTR][INFO]: Rank0: Write hash table <key,slot_id,value> pairs to file\n",
      "[27d50h43m30s][HUGECTR][INFO]: Done\n",
      "[27d50h43m31s][HUGECTR][INFO]: Dumping sparse weights to files, successful\n",
      "[27d50h43m31s][HUGECTR][INFO]: Dumping sparse optimzer states to files, successful\n",
      "[27d50h43m31s][HUGECTR][INFO]: Dumping dense weights to file, successful\n",
      "[27d50h43m31s][HUGECTR][INFO]: Dumping dense optimizer states to file, successful\n",
      "[27d50h43m31s][HUGECTR][INFO]: Dumping untrainable weights to file, successful\n",
      "[27d50h43m39s][HUGECTR][INFO]: Iter: 37000 Time(1000 iters): 11.319562s Loss: 0.153160 lr:0.010561\n",
      "[27d50h43m46s][HUGECTR][INFO]: Iter: 38000 Time(1000 iters): 7.785207s Loss: 0.153620 lr:0.008999\n",
      "[27d50h43m54s][HUGECTR][INFO]: Iter: 39000 Time(1000 iters): 7.789169s Loss: 0.150560 lr:0.007561\n",
      "[27d50h43m57s][HUGECTR][INFO]: Evaluation, AUC: 0.942710\n",
      "[27d50h43m57s][HUGECTR][INFO]: Eval Time for 1000 iters: 3.058611s\n",
      "[27d50h43m57s][HUGECTR][INFO]: Rank0: Dump hash table from GPU0\n",
      "[27d50h43m57s][HUGECTR][INFO]: Rank0: Write hash table <key,slot_id,value> pairs to file\n",
      "[27d50h43m57s][HUGECTR][INFO]: Done\n",
      "[27d50h43m58s][HUGECTR][INFO]: Dumping sparse weights to files, successful\n",
      "[27d50h43m58s][HUGECTR][INFO]: Dumping sparse optimzer states to files, successful\n",
      "[27d50h43m58s][HUGECTR][INFO]: Dumping dense weights to file, successful\n",
      "[27d50h43m58s][HUGECTR][INFO]: Dumping dense optimizer states to file, successful\n",
      "[27d50h43m58s][HUGECTR][INFO]: Dumping untrainable weights to file, successful\n",
      "[27d50h44m60s][HUGECTR][INFO]: Iter: 40000 Time(1000 iters): 11.409290s Loss: 0.152684 lr:0.006249\n",
      "[27d50h44m13s][HUGECTR][INFO]: Iter: 41000 Time(1000 iters): 7.782864s Loss: 0.150632 lr:0.005061\n",
      "[27d50h44m21s][HUGECTR][INFO]: Iter: 42000 Time(1000 iters): 7.783673s Loss: 0.151143 lr:0.003999\n",
      "[27d50h44m24s][HUGECTR][INFO]: Evaluation, AUC: 0.942747\n",
      "[27d50h44m24s][HUGECTR][INFO]: Eval Time for 1000 iters: 3.085488s\n",
      "[27d50h44m24s][HUGECTR][INFO]: Rank0: Dump hash table from GPU0\n",
      "[27d50h44m24s][HUGECTR][INFO]: Rank0: Write hash table <key,slot_id,value> pairs to file\n",
      "[27d50h44m24s][HUGECTR][INFO]: Done\n",
      "[27d50h44m25s][HUGECTR][INFO]: Dumping sparse weights to files, successful\n",
      "[27d50h44m25s][HUGECTR][INFO]: Dumping sparse optimzer states to files, successful\n",
      "[27d50h44m25s][HUGECTR][INFO]: Dumping dense weights to file, successful\n",
      "[27d50h44m25s][HUGECTR][INFO]: Dumping dense optimizer states to file, successful\n",
      "[27d50h44m25s][HUGECTR][INFO]: Dumping untrainable weights to file, successful\n",
      "[27d50h44m33s][HUGECTR][INFO]: Iter: 43000 Time(1000 iters): 11.479904s Loss: 0.149410 lr:0.003062\n",
      "[27d50h44m40s][HUGECTR][INFO]: Iter: 44000 Time(1000 iters): 7.782824s Loss: 0.154166 lr:0.002249\n",
      "[27d50h44m48s][HUGECTR][INFO]: Iter: 45000 Time(1000 iters): 7.786467s Loss: 0.150872 lr:0.001562\n",
      "[27d50h44m51s][HUGECTR][INFO]: Evaluation, AUC: 0.942651\n",
      "[27d50h44m51s][HUGECTR][INFO]: Eval Time for 1000 iters: 3.048637s\n",
      "[27d50h44m51s][HUGECTR][INFO]: Rank0: Dump hash table from GPU0\n",
      "[27d50h44m51s][HUGECTR][INFO]: Rank0: Write hash table <key,slot_id,value> pairs to file\n",
      "[27d50h44m51s][HUGECTR][INFO]: Done\n",
      "[27d50h44m52s][HUGECTR][INFO]: Dumping sparse weights to files, successful\n",
      "[27d50h44m52s][HUGECTR][INFO]: Dumping sparse optimzer states to files, successful\n",
      "[27d50h44m52s][HUGECTR][INFO]: Dumping dense weights to file, successful\n",
      "[27d50h44m52s][HUGECTR][INFO]: Dumping dense optimizer states to file, successful\n",
      "[27d50h44m52s][HUGECTR][INFO]: Dumping untrainable weights to file, successful\n",
      "[27d50h45m00s][HUGECTR][INFO]: Iter: 46000 Time(1000 iters): 11.403682s Loss: 0.153579 lr:0.001000\n",
      "[27d50h45m70s][HUGECTR][INFO]: Iter: 47000 Time(1000 iters): 7.787644s Loss: 0.150713 lr:0.000562\n",
      "[27d50h45m15s][HUGECTR][INFO]: Iter: 48000 Time(1000 iters): 7.782739s Loss: 0.150775 lr:0.000250\n",
      "[27d50h45m18s][HUGECTR][INFO]: Evaluation, AUC: 0.942699\n",
      "[27d50h45m18s][HUGECTR][INFO]: Eval Time for 1000 iters: 2.995483s\n",
      "[27d50h45m18s][HUGECTR][INFO]: Rank0: Dump hash table from GPU0\n",
      "[27d50h45m18s][HUGECTR][INFO]: Rank0: Write hash table <key,slot_id,value> pairs to file\n",
      "[27d50h45m18s][HUGECTR][INFO]: Done\n",
      "[27d50h45m19s][HUGECTR][INFO]: Dumping sparse weights to files, successful\n",
      "[27d50h45m19s][HUGECTR][INFO]: Dumping sparse optimzer states to files, successful\n",
      "[27d50h45m19s][HUGECTR][INFO]: Dumping dense weights to file, successful\n",
      "[27d50h45m19s][HUGECTR][INFO]: Dumping dense optimizer states to file, successful\n",
      "[27d50h45m19s][HUGECTR][INFO]: Dumping untrainable weights to file, successful\n",
      "[27d50h45m27s][HUGECTR][INFO]: Iter: 49000 Time(1000 iters): 11.701493s Loss: 0.153675 lr:0.000062\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 python3 hugectr_dlrm_movielens.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "## 4. Answer item similarity with DLRM embedding\n",
    "\n",
    "In this section, we demonstrate how the output of HugeCTR training can be used to carry out simple inference tasks. Specifically, we will show that the movie embeddings can be used for simple item-to-item similarity queries. Such a simple inference can be used as an efficient candidate generator to generate a small set of cadidates prior to deep learning model re-ranking. \n",
    "\n",
    "First, we read the embedding tables and extract the movie embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct \n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "key_type = 'I32' # {'I64', 'I32'}, default is 'I32'\n",
    "key_type_map = {\"I32\": [\"I\", 4], \"I64\": [\"q\", 8]}\n",
    "\n",
    "embedding_vec_size = 64\n",
    "\n",
    "HUGE_CTR_VERSION = 2.21 # set HugeCTR version here, 2.2 for v2.2, 2.21 for v2.21\n",
    "\n",
    "if HUGE_CTR_VERSION <= 2.2:\n",
    "    each_key_size = key_type_map[key_type][1] + key_type_map[key_type][1] + 4 * embedding_vec_size\n",
    "else:\n",
    "    each_key_size = key_type_map[key_type][1] + 8 + 4 * embedding_vec_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_table = [{},{}]\n",
    "\n",
    "with open('./hugeCTR_saved_model_DLRM/0_sparse_9000.model', 'rb') as file:\n",
    "    try:\n",
    "        while True:\n",
    "            buffer = file.read(each_key_size)\n",
    "            if len(buffer) == 0:\n",
    "                break\n",
    "            if HUGE_CTR_VERSION <= 2.2:\n",
    "                key, slot_id = struct.unpack(\"2\" + key_type_map[key_type][0], \n",
    "                                             buffer[0: 2*key_type_map[key_type][1]])\n",
    "                values = struct.unpack(str(embedding_vec_size) + \"f\", buffer[2*key_type_map[key_type][1]: ])\n",
    "            else:\n",
    "                key = struct.unpack(key_type_map[key_type][0], buffer[0 : key_type_map[key_type][1]])[0]\n",
    "                slot_id = struct.unpack(\"Q\", buffer[key_type_map[key_type][1] : key_type_map[key_type][1] + 8])[0]\n",
    "                values = struct.unpack(str(embedding_vec_size) + \"f\", buffer[key_type_map[key_type][1] + 8: ])\n",
    "            if slot_id==0:\n",
    "                embedding_table[slot_id][key] = values\n",
    "            elif slot_id==1:\n",
    "                embedding_table[slot_id][key - 138493] = values \n",
    "            else:\n",
    "                raise(Exception(\"Slot ID not found - %d\"%slot_id))\n",
    "            \n",
    "    except BaseException as error:\n",
    "        print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_embedding = np.zeros((26744, embedding_vec_size), dtype='float')\n",
    "for i in range(len(embedding_table[1])):\n",
    "    item_embedding[i] = embedding_table[1][i]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26744"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding_table[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer nearest neighbor queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def find_similar_movies(nn_movie_id, item_embedding, k=10, metric=\"euclidean\"):\n",
    "    #find the top K similar items according to one of the distance metric: cosine or euclidean\n",
    "    sim = 1-cdist(item_embedding, item_embedding[nn_movie_id].reshape(1, -1), metric=metric)\n",
    "   \n",
    "    return sim.squeeze().argsort()[-k:][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./mappings.pickle', 'rb') as handle:\n",
    "    movies_mapping = pickle.load(handle)[\"items\"]\n",
    "\n",
    "nn_to_movies = movies_mapping\n",
    "movies_to_nn = {}\n",
    "for i in range(len(movies_mapping)):\n",
    "    movies_to_nn[movies_mapping[i]] = i\n",
    "\n",
    "import pandas as pd\n",
    "movies = pd.read_csv(\"./data/ml-20m/movies.csv\", index_col=\"movieId\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:  Toy Story (1995) Adventure|Animation|Children|Comedy|Fantasy\n",
      "Similar movies: \n",
      "1 Toy Story (1995) Adventure|Animation|Children|Comedy|Fantasy\n",
      "2571 Matrix, The (1999) Action|Sci-Fi|Thriller\n",
      "457 Fugitive, The (1993) Thriller\n",
      "50 Usual Suspects, The (1995) Crime|Mystery|Thriller\n",
      "110 Braveheart (1995) Action|Drama|War\n",
      "32 Twelve Monkeys (a.k.a. 12 Monkeys) (1995) Mystery|Sci-Fi|Thriller\n",
      "1210 Star Wars: Episode VI - Return of the Jedi (1983) Action|Adventure|Sci-Fi\n",
      "150 Apollo 13 (1995) Adventure|Drama|IMAX\n",
      "592 Batman (1989) Action|Crime|Thriller\n",
      "589 Terminator 2: Judgment Day (1991) Action|Sci-Fi\n",
      "=================================\n",
      "\n",
      "Query:  Jumanji (1995) Adventure|Children|Fantasy\n",
      "Similar movies: \n",
      "2 Jumanji (1995) Adventure|Children|Fantasy\n",
      "3996 Crouching Tiger, Hidden Dragon (Wo hu cang long) (2000) Action|Drama|Romance\n",
      "4886 Monsters, Inc. (2001) Adventure|Animation|Children|Comedy|Fantasy\n",
      "2396 Shakespeare in Love (1998) Comedy|Drama|Romance\n",
      "1206 Clockwork Orange, A (1971) Crime|Drama|Sci-Fi|Thriller\n",
      "454 Firm, The (1993) Drama|Thriller\n",
      "95 Broken Arrow (1996) Action|Adventure|Thriller\n",
      "4973 Amelie (Fabuleux destin d'Amélie Poulain, Le) (2001) Comedy|Romance\n",
      "1307 When Harry Met Sally... (1989) Comedy|Romance\n",
      "2706 American Pie (1999) Comedy|Romance\n",
      "=================================\n",
      "\n",
      "Query:  Grumpier Old Men (1995) Comedy|Romance\n",
      "Similar movies: \n",
      "3 Grumpier Old Men (1995) Comedy|Romance\n",
      "1320 Alien³ (a.k.a. Alien 3) (1992) Action|Horror|Sci-Fi|Thriller\n",
      "494 Executive Decision (1996) Action|Adventure|Thriller\n",
      "151 Rob Roy (1995) Action|Drama|Romance|War\n",
      "552 Three Musketeers, The (1993) Action|Adventure|Comedy|Romance\n",
      "466 Hot Shots! Part Deux (1993) Action|Comedy|War\n",
      "1028 Mary Poppins (1964) Children|Comedy|Fantasy|Musical\n",
      "196 Species (1995) Horror|Sci-Fi\n",
      "1676 Starship Troopers (1997) Action|Sci-Fi\n",
      "5618 Spirited Away (Sen to Chihiro no kamikakushi) (2001) Adventure|Animation|Fantasy\n",
      "=================================\n",
      "\n",
      "Query:  Waiting to Exhale (1995) Comedy|Drama|Romance\n",
      "Similar movies: \n",
      "4 Waiting to Exhale (1995) Comedy|Drama|Romance\n",
      "2690 Ideal Husband, An (1999) Comedy|Romance\n",
      "4728 Rat Race (2001) Comedy\n",
      "2118 Dead Zone, The (1983) Thriller\n",
      "85414 Source Code (2011) Action|Drama|Mystery|Sci-Fi|Thriller\n",
      "674 Barbarella (1968) Adventure|Comedy|Sci-Fi\n",
      "41285 Match Point (2005) Crime|Drama|Romance\n",
      "8865 Sky Captain and the World of Tomorrow (2004) Action|Adventure|Sci-Fi\n",
      "45186 Mission: Impossible III (2006) Action|Adventure|Thriller\n",
      "3359 Breaking Away (1979) Comedy|Drama\n",
      "=================================\n",
      "\n",
      "Query:  Father of the Bride Part II (1995) Comedy\n",
      "Similar movies: \n",
      "5 Father of the Bride Part II (1995) Comedy\n",
      "5669 Bowling for Columbine (2002) Documentary\n",
      "2001 Lethal Weapon 2 (1989) Action|Comedy|Crime|Drama\n",
      "1777 Wedding Singer, The (1998) Comedy|Romance\n",
      "913 Maltese Falcon, The (1941) Film-Noir|Mystery\n",
      "1645 Devil's Advocate, The (1997) Drama|Mystery|Thriller\n",
      "33493 Star Wars: Episode III - Revenge of the Sith (2005) Action|Adventure|Sci-Fi\n",
      "265 Like Water for Chocolate (Como agua para chocolate) (1992) Drama|Fantasy|Romance\n",
      "1376 Star Trek IV: The Voyage Home (1986) Adventure|Comedy|Sci-Fi\n",
      "112 Rumble in the Bronx (Hont faan kui) (1995) Action|Adventure|Comedy|Crime\n",
      "=================================\n",
      "\n",
      "Query:  Heat (1995) Action|Crime|Thriller\n",
      "Similar movies: \n",
      "6 Heat (1995) Action|Crime|Thriller\n",
      "919 Wizard of Oz, The (1939) Adventure|Children|Fantasy|Musical\n",
      "21 Get Shorty (1995) Comedy|Crime|Thriller\n",
      "6874 Kill Bill: Vol. 1 (2003) Action|Crime|Thriller\n",
      "3114 Toy Story 2 (1999) Adventure|Animation|Children|Comedy|Fantasy\n",
      "1923 There's Something About Mary (1998) Comedy|Romance\n",
      "1206 Clockwork Orange, A (1971) Crime|Drama|Sci-Fi|Thriller\n",
      "2706 American Pie (1999) Comedy|Romance\n",
      "1961 Rain Man (1988) Drama\n",
      "2329 American History X (1998) Crime|Drama\n",
      "=================================\n",
      "\n",
      "Query:  Sabrina (1995) Comedy|Romance\n",
      "Similar movies: \n",
      "7 Sabrina (1995) Comedy|Romance\n",
      "2657 Rocky Horror Picture Show, The (1975) Comedy|Horror|Musical|Sci-Fi\n",
      "552 Three Musketeers, The (1993) Action|Adventure|Comedy|Romance\n",
      "1777 Wedding Singer, The (1998) Comedy|Romance\n",
      "1250 Bridge on the River Kwai, The (1957) Adventure|Drama|War\n",
      "2001 Lethal Weapon 2 (1989) Action|Comedy|Crime|Drama\n",
      "70 From Dusk Till Dawn (1996) Action|Comedy|Horror|Thriller\n",
      "2804 Christmas Story, A (1983) Children|Comedy\n",
      "3421 Animal House (1978) Comedy\n",
      "903 Vertigo (1958) Drama|Mystery|Romance|Thriller\n",
      "=================================\n",
      "\n",
      "Query:  Tom and Huck (1995) Adventure|Children\n",
      "Similar movies: \n",
      "8 Tom and Huck (1995) Adventure|Children\n",
      "3111 Places in the Heart (1984) Drama\n",
      "1848 Borrowers, The (1997) Adventure|Children|Comedy|Fantasy\n",
      "4546 Vanishing, The (Spoorloos) (1988) Drama|Thriller\n",
      "1649 Fast, Cheap & Out of Control (1997) Documentary\n",
      "2372 Fletch Lives (1989) Comedy\n",
      "1541 Addicted to Love (1997) Comedy|Romance\n",
      "2922 Hang 'Em High (1968) Crime|Drama|Western\n",
      "4867 Riding in Cars with Boys (2001) Comedy|Drama\n",
      "556 War Room, The (1993) Documentary\n",
      "=================================\n",
      "\n",
      "Query:  Sudden Death (1995) Action\n",
      "Similar movies: \n",
      "9 Sudden Death (1995) Action\n",
      "374 Richie Rich (1994) Children|Comedy\n",
      "76251 Kick-Ass (2010) Action|Comedy\n",
      "1688 Anastasia (1997) Adventure|Animation|Children|Drama|Musical\n",
      "2139 Secret of NIMH, The (1982) Adventure|Animation|Children|Drama\n",
      "262 Little Princess, A (1995) Children|Drama\n",
      "3893 Nurse Betty (2000) Comedy|Crime|Drama|Romance|Thriller\n",
      "3168 Easy Rider (1969) Adventure|Drama\n",
      "3155 Anna and the King (1999) Drama|Romance\n",
      "88125 Harry Potter and the Deathly Hallows: Part 2 (2011) Action|Adventure|Drama|Fantasy|Mystery|IMAX\n",
      "=================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for movie_ID in range(1,10):\n",
    "    try:\n",
    "        print(\"Query: \", movies.loc[movie_ID][\"title\"], movies.loc[movie_ID][\"genres\"])\n",
    "\n",
    "        print(\"Similar movies: \")\n",
    "        similar_movies = find_similar_movies(movies_to_nn[movie_ID], item_embedding)\n",
    "\n",
    "        for i in similar_movies:\n",
    "            print(nn_to_movies[i], movies.loc[nn_to_movies[i]][\"title\"], movies.loc[nn_to_movies[i]][\"genres\"])\n",
    "        print(\"=================================\\n\")\n",
    "    except Exception as e:\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
