<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Mixed Precision &mdash; SparseOperationKit  documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Performance" href="../performance/index.html" />
    <link rel="prev" title="Demo model using Dense Embedding Layer" href="dense_demo.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> SparseOperationKit
          </a>
              <div class="version">
                1.1.2
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../intro_link.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../features/features.html">Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="../get_started/get_started.html">Get Started</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Examples &amp; Tutorials</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="dlrm.html">DLRM</a></li>
<li class="toctree-l2"><a class="reference internal" href="dense_demo.html">DenseDemo</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Mixed Precision</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#tensorflow-2-x">TensorFlow 2.x</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#enable-mixed-precision">enable mixed precision</a></li>
<li class="toctree-l4"><a class="reference internal" href="#loss-scaling">loss scaling</a></li>
<li class="toctree-l4"><a class="reference internal" href="#example-codes">example codes</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#tensorflow-1-15">TensorFlow 1.15</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id1">enable mixed precision</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id2">loss scaling</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id3">example codes</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../performance/index.html">Performance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/index.html">API Docs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../release_notes/release_notes.html">Release Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../env_vars/env_vars.html">Environment Variables</a></li>
<li class="toctree-l1"><a class="reference internal" href="../known_issues/issues.html">Known Issues</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">SparseOperationKit</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="index.html">Examples and Tutorials</a> &raquo;</li>
      <li>Mixed Precision</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/examples/amp.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section class="tex2jax_ignore mathjax_ignore" id="mixed-precision">
<h1>Mixed Precision<a class="headerlink" href="#mixed-precision" title="Permalink to this headline"></a></h1>
<p>Mixed precision is a way to make an application run faster and use less memory. It use both 16-bit and 32-bit floating-point types during model training.</p>
<p>SparseOperationKit follows TensorFlow’s pattern to enable mixed precision training, see this <a class="reference external" href="https://tensorflow.google.cn/guide/mixed_precision?hl=en">link</a>.</p>
<section id="tensorflow-2-x">
<h2>TensorFlow 2.x<a class="headerlink" href="#tensorflow-2-x" title="Permalink to this headline"></a></h2>
<p>This section illustrates how to enable mixed precision training in TF 2.x.</p>
<section id="enable-mixed-precision">
<h3>enable mixed precision<a class="headerlink" href="#enable-mixed-precision" title="Permalink to this headline"></a></h3>
<p>To use mixed precision in your model training, you just need to add the following two lines of code to your training script and keep other parts untouched.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">policy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">mixed_precision</span><span class="o">.</span><span class="n">Policy</span><span class="p">(</span><span class="s2">&quot;mixed_float16&quot;</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">mixed_precision</span><span class="o">.</span><span class="n">set_global_policy</span><span class="p">(</span><span class="n">policy</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="loss-scaling">
<h3>loss scaling<a class="headerlink" href="#loss-scaling" title="Permalink to this headline"></a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">float16</span></code> data type has a narrow dynamic range compared to <code class="docutils literal notranslate"><span class="pre">float32</span></code>, then <code class="docutils literal notranslate"><span class="pre">float16</span></code> might lead model training to underflow or overflow problems. Loss scaling is a technique to avoid numeric underflow.</p>
<p>TensorFlow provides an optimizer wrapper for loss scaling.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">()</span> <span class="c1"># could be other optimizers as well</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">mixed_precision</span><span class="o">.</span><span class="n">LossScaleOptimizer</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>
</pre></div>
</div>
<p>In the training loop, this optimizer wrapper should be used to calculate the scaled loss value. After the backward propagation and before trainable parameters’ updating, that wrapper should be used to get the correct gradients. Because during backward propagation, loss is scaled, therefore gradients is also scaled, and when updating, you need to scale it back to correct value.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@tf</span><span class="o">.</span><span class="n">function</span>
<span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
        <span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_object</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
        <span class="n">scaled_loss</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">get_scaled_loss</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span> <span class="c1"># this API should be called to get scaled loss</span>
    <span class="n">emb_variables</span><span class="p">,</span> <span class="n">other_variables</span> <span class="o">=</span>\
         <span class="n">sok</span><span class="o">.</span><span class="n">split_embedding_variable_from_others</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">)</span>
    <span class="n">scaled_emb_gradients</span><span class="p">,</span> <span class="n">scaled_other_gradients</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">scaled_loss</span><span class="p">,</span> 
                                            <span class="p">[</span><span class="n">emb_variables</span><span class="p">,</span> <span class="n">other_variables</span><span class="p">])</span>
    <span class="c1"># use this API to scale embedding gradients back to correct value</span>
    <span class="n">emb_gradients</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">get_unscaled_gradients</span><span class="p">(</span><span class="n">scaled_emb_gradients</span><span class="p">)</span> 
    <span class="c1"># use this API to scale other gradients back to correct value</span>
    <span class="n">other_gradients</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">get_unscaled_gradients</span><span class="p">(</span><span class="n">scaled_other_gradients</span><span class="p">)</span> 
    <span class="n">optimizer</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">emb_gradients</span><span class="p">,</span> <span class="n">emb_variables</span><span class="p">),</span>
                              <span class="n">experimental_aggregate_gradients</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">other_gradients</span><span class="p">,</span> <span class="n">other_variables</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">loss</span>
</pre></div>
</div>
</section>
<section id="example-codes">
<h3>example codes<a class="headerlink" href="#example-codes" title="Permalink to this headline"></a></h3>
<p>You can find mixed precision example using TensorFlow 2.x in <a class="reference external" href="https://github.com/NVIDIA/HugeCTR/tree/master/sparse_operation_kit/documents/tutorials/MixedPrecision"><code class="docutils literal notranslate"><span class="pre">sparse_operation_kit/documents/tutorials/MixedPrecision</span></code></a>. And use following command to launch it.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>$ <span class="nb">cd</span> sparse_operation_kit/documents/tutorials/MixedPrecision/
$ python amp_tf2.py
</pre></div>
</div>
</section>
</section>
<section id="tensorflow-1-15">
<h2>TensorFlow 1.15<a class="headerlink" href="#tensorflow-1-15" title="Permalink to this headline"></a></h2>
<p>This section illustrates how to enable mixed precision training in TF 1.15.</p>
<section id="id1">
<h3>enable mixed precision<a class="headerlink" href="#id1" title="Permalink to this headline"></a></h3>
<p>To use mixed precision in your model training, you just need to add the following four lines of code to your training script and keep other parts untouched.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">tensorflow.python.keras.engine</span> <span class="kn">import</span> <span class="n">base_layer_utils</span>
<span class="n">base_layer_utils</span><span class="o">.</span><span class="n">enable_v2_dtype_behavior</span><span class="p">()</span>

<span class="n">policy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">mixed_precision</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">Policy</span><span class="p">(</span><span class="s2">&quot;mixed_float16&quot;</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">mixed_precision</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">set_policy</span><span class="p">(</span><span class="n">policy</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="id2">
<h3>loss scaling<a class="headerlink" href="#id2" title="Permalink to this headline"></a></h3>
<p>Similarily, optimizer wrapper should be used for loss scaling.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">()</span> <span class="c1"># could be other optimizers as well</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">sok</span><span class="o">.</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">mixed_precision</span><span class="o">.</span><span class="n">LossScaleOptimizer</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">sok.tf.keras.mixed_precision.LossScaleOptimizer</span></code> is an optimized wrapper based on <code class="docutils literal notranslate"><span class="pre">tf.keras.mixed_precision.experimental.LossScaleOptimizer</span></code>, and it is only available in TensorFlow 1.15.</p>
<p>They use the same arguments, you can find the arguments <a class="reference external" href="https://tensorflow.google.cn/versions/r1.15/api_docs/python/tf/keras/mixed_precision/experimental/LossScaleOptimizer?hl=en"><strong>here</strong></a>.</p>
<p>In training loop, this optimizer wrapper should be used to calculate the scaled loss value, and get the unscaled gradients before parameters’ updating.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_object</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
    <span class="c1"># use this API to get scaled loss value</span>
    <span class="n">scaled_loss</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">get_scaled_loss</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
    <span class="n">scaled_gradients</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">gradients</span><span class="p">(</span><span class="n">scaled_loss</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">)</span>
    <span class="c1"># distinguish embedding variables and other variabels</span>
    <span class="c1"># because they need different processing.</span>
    <span class="n">emb_variables</span><span class="p">,</span> <span class="n">other_variables</span> <span class="o">=</span>\
        <span class="n">sok</span><span class="o">.</span><span class="n">split_embedding_variable_from_others</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">)</span>
    <span class="n">scaled_emb_gradients</span><span class="p">,</span> <span class="n">scaled_other_gradients</span> <span class="o">=</span>\
        <span class="n">scaled_gradients</span><span class="p">[:</span><span class="nb">len</span><span class="p">(</span><span class="n">emb_variables</span><span class="p">)],</span> <span class="n">scaled_gradients</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">emb_variables</span><span class="p">):]</span>
    <span class="c1"># use this API to scale embedding gradients back to correct value</span>
    <span class="n">emb_gradients</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">get_unscaled_gradients</span><span class="p">(</span><span class="n">scaled_emb_gradients</span><span class="p">)</span>
    <span class="c1"># use this API to scale other gradients back to correct value</span>
    <span class="n">other_gradients</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">get_unscaled_gradients</span><span class="p">(</span><span class="n">scaled_other_gradients</span><span class="p">)</span>
    <span class="n">other_gradients</span> <span class="o">=</span> <span class="p">[</span><span class="n">hvd</span><span class="o">.</span><span class="n">allreduce</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span> <span class="k">for</span> <span class="n">grad</span> <span class="ow">in</span> <span class="n">other_gradients</span><span class="p">]</span>
    <span class="n">emb_train_op</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">emb_gradients</span><span class="p">,</span> <span class="n">emb_variables</span><span class="p">))</span>
    <span class="n">other_train_op</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">other_gradients</span><span class="p">,</span> <span class="n">other_variables</span><span class="p">))</span>

    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">([</span><span class="n">emb_train_op</span><span class="p">,</span> <span class="n">other_train_op</span><span class="p">]):</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">identify</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="id3">
<h3>example codes<a class="headerlink" href="#id3" title="Permalink to this headline"></a></h3>
<p>You can find mixed precision example using TensorFlow 1.15 in <a class="reference external" href="https://github.com/NVIDIA/HugeCTR/tree/master/sparse_operation_kit/documents/tutorials/MixedPrecision"><code class="docutils literal notranslate"><span class="pre">sparse_operation_kit/documents/tutorials/MixedPrecision</span></code></a>. And use following command to launch it.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>$ <span class="nb">cd</span> sparse_operation_kit/documents/tutorials/MixedPrecision/
$ mpiexec --allow-run-as-root -np &lt;gpu-number&gt; python amp_tf1.py
</pre></div>
</div>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="dense_demo.html" class="btn btn-neutral float-left" title="Demo model using Dense Embedding Layer" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../performance/index.html" class="btn btn-neutral float-right" title="Performance" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, NVIDIA.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  
<div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    <span class="fa fa-book"> Other Versions</span>
    master: sok_v1.1.2
    <span class="fa fa-caret-down"></span>
  </span>
  <div class="rst-other-versions">
    <dl>
      <dt>Tags</dt>
      <dd><a href="../../sok_v1.0.0/index.html">sok_v1.0.0</a></dd>
      <dd><a href="../../sok_v1.1.0/index.html">sok_v1.1.0</a></dd>
      <dd><a href="../../sok_v1.1.1/examples/amp.html">sok_v1.1.1</a></dd>
      <dd><a href="amp.html">sok_v1.1.2</a></dd>
    </dl>
  </div>
</div><script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>