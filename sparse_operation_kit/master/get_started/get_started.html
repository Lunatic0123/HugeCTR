<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Get Started With SparseOperationKit &mdash; SparseOperationKit  documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Examples" href="../examples/index.html" />
    <link rel="prev" title="Features in SparseOperationKit" href="../features/features.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> SparseOperationKit
          </a>
              <div class="version">
                1.1.0
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../intro_link.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../features/features.html">Features</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Get Started</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#install-sparseoperationkit">Install SparseOperationKit</a></li>
<li class="toctree-l2"><a class="reference internal" href="#import-sparseoperationkit">Import SparseOperationKit</a></li>
<li class="toctree-l2"><a class="reference internal" href="#tensorflow-2-x">TensorFlow 2.x</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#define-a-model-with-tensorflow">Define a model with TensorFlow</a></li>
<li class="toctree-l3"><a class="reference internal" href="#use-sparseoperationkit-with-tf-distribute-strategy">Use SparseOperationKit with tf.distribute.Strategy</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#with-tf-distribute-mirroredstrategy">with tf.distribute.MirroredStrategy</a></li>
<li class="toctree-l4"><a class="reference internal" href="#with-tf-distribute-multiworkermirroredstrategy">With tf.distribute.MultiWorkerMirroredStrategy</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#use-sparseoperationkit-with-horovod">Use SparseOperationKit with Horovod</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#tensorflow-1-15">TensorFlow 1.15</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#using-sparseoperationkit-with-horovod">Using SparseOperationKit with Horovod</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../examples/index.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../performance/index.html">Performance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/index.html">API Docs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../release_notes/release_notes.html">Release Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../env_vars/env_vars.html">Environment Variables</a></li>
<li class="toctree-l1"><a class="reference internal" href="../known_issues/issues.html">Known Issues</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">SparseOperationKit</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Get Started With SparseOperationKit</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/get_started/get_started.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section class="tex2jax_ignore mathjax_ignore" id="get-started-with-sparseoperationkit">
<h1>Get Started With SparseOperationKit<a class="headerlink" href="#get-started-with-sparseoperationkit" title="Permalink to this headline"></a></h1>
<p>This document will walk you through simple demos to get you familiar with SparseOperationKit.</p>
<div class="admonition note">
<p class="admonition-title">See also</p>
<p>For experts or more examples, please refer to Examples section</p>
</div>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>In this document and other examples in SOK, you are assumed to be familiar with TensorFlow and other related tools.</p>
</div>
<section id="install-sparseoperationkit">
<h2>Install SparseOperationKit<a class="headerlink" href="#install-sparseoperationkit" title="Permalink to this headline"></a></h2>
<p>Please refer to the <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/sparse_operation_kit/master/intro_link.html#installation"><em>Installation</em> section</a> to install SparseOperationKit to your system.</p>
</section>
<section id="import-sparseoperationkit">
<h2>Import SparseOperationKit<a class="headerlink" href="#import-sparseoperationkit" title="Permalink to this headline"></a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">sparse_operation_kit</span> <span class="k">as</span> <span class="nn">sok</span>
</pre></div>
</div>
<p>Now, SOK supports TensorFlow 1.15 and 2.x, and it will detect the version of TensorFlow automatically. And SOK API signatures are identical for TensorFlow 2.x and TensorFlow 1.15.</p>
</section>
<section id="tensorflow-2-x">
<h2>TensorFlow 2.x<a class="headerlink" href="#tensorflow-2-x" title="Permalink to this headline"></a></h2>
<section id="define-a-model-with-tensorflow">
<h3>Define a model with TensorFlow<a class="headerlink" href="#define-a-model-with-tensorflow" title="Permalink to this headline"></a></h3>
<p>The structure of this demo model is depicted in Fig 1.</p>
<p><br><img alt="../_images/demo_model_structure1.png" src="../_images/demo_model_structure1.png" /></br></p>
<center><b>Fig 1. The structure of demo model</b></center>
<br>
<br>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="k">class</span> <span class="nc">DemoModel</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Model</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">max_vocabulary_size_per_gpu</span><span class="p">,</span>
                 <span class="n">slot_num</span><span class="p">,</span>
                 <span class="n">nnz_per_slot</span><span class="p">,</span>
                 <span class="n">embedding_vector_size</span><span class="p">,</span>
                 <span class="n">num_of_dense_layers</span><span class="p">,</span>
                 <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DemoModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">max_vocabulary_size_per_gpu</span> <span class="o">=</span> <span class="n">max_vocabulary_size_per_gpu</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">slot_num</span> <span class="o">=</span> <span class="n">slot_num</span>            <span class="c1"># the number of feature-fileds per sample</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">nnz_per_slot</span> <span class="o">=</span> <span class="n">nnz_per_slot</span>    <span class="c1"># the number of valid keys per feature-filed</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding_vector_size</span> <span class="o">=</span> <span class="n">embedding_vector_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_of_dense_layers</span> <span class="o">=</span> <span class="n">num_of_dense_layers</span>

        <span class="c1"># this embedding layer will concatenate each key&#39;s embedding vector</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding_layer</span> <span class="o">=</span> <span class="n">sok</span><span class="o">.</span><span class="n">All2AllDenseEmbedding</span><span class="p">(</span>
                    <span class="n">max_vocabulary_size_per_gpu</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_vocabulary_size_per_gpu</span><span class="p">,</span>
                    <span class="n">embedding_vec_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">embedding_vector_size</span><span class="p">,</span>
                    <span class="n">slot_num</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">slot_num</span><span class="p">,</span>
                    <span class="n">nnz_per_slot</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">nnz_per_slot</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">dense_layers</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_of_dense_layers</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">layer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dense_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layer</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">out_layer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="c1"># its shape is [batchsize, slot_num, nnz_per_slot, embedding_vector_size]</span>
        <span class="n">emb_vector</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding_layer</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">training</span><span class="p">)</span>

        <span class="c1"># reshape this tensor, so that it can be processed by Dense layer</span>
        <span class="n">emb_vector</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">emb_vector</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">slot_num</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">nnz_per_slot</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding_vector_size</span><span class="p">])</span>

        <span class="n">hidden</span> <span class="o">=</span> <span class="n">emb_vector</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense_layers</span><span class="p">:</span>
            <span class="n">hidden</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">hidden</span><span class="p">)</span>

        <span class="n">logit</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_layer</span><span class="p">(</span><span class="n">hidden</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">logit</span>
</pre></div>
</div>
</section>
<section id="use-sparseoperationkit-with-tf-distribute-strategy">
<h3>Use SparseOperationKit with tf.distribute.Strategy<a class="headerlink" href="#use-sparseoperationkit-with-tf-distribute-strategy" title="Permalink to this headline"></a></h3>
<p>SparseOperationKit is compatible with <code class="docutils literal notranslate"><span class="pre">tf.distribute.Strategy</span></code>. More specificly, <code class="docutils literal notranslate"><span class="pre">tf.distribute.MirroredStrategy</span></code> and <code class="docutils literal notranslate"><span class="pre">tf.distribute.MultiWorkerMirroredStrategy</span></code>.</p>
<section id="with-tf-distribute-mirroredstrategy">
<h4>with tf.distribute.MirroredStrategy<a class="headerlink" href="#with-tf-distribute-mirroredstrategy" title="Permalink to this headline"></a></h4>
<p>Documents for <a class="reference external" href="https://tensorflow.google.cn/api_docs/python/tf/distribute/MirroredStrategy?hl=en">tf.distribute.MirroredStrategy</a>. <code class="docutils literal notranslate"><span class="pre">tf.distribute.MirroredStrategy</span></code> is a tool to support data-parallel synchronized training in single machine, where there exists multiple GPUs.</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>The programming model for MirroredStrategy is single-process &amp; multi-threads. But due to the GIL in CPython interpreter, it is hard to fully leverage all available CPU cores, which might impact the end-to-end training / inference performance. Therefore, MirroredStrategy is not recommended for multiple GPUs synchronized training.</p>
</div>
<p><em><strong>create MirroredStrategy</strong></em></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">strategy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">distribute</span><span class="o">.</span><span class="n">MirroredStrategy</span><span class="p">()</span>
</pre></div>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>By default, MirroredStrategy will use all available GPUs in one machine. If you want to specify how many GPUs are used or which GPUs are used for synchronized training, please set <code class="docutils literal notranslate"><span class="pre">CUDA_VISIBLE_DEVICES</span></code> or <code class="docutils literal notranslate"><span class="pre">tf.config.set_visible_devices</span></code>.</p>
</div>
<p><em><strong>create model instance under MirroredStrategy.scope</strong></em></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">global_batch_size</span> <span class="o">=</span> <span class="mi">65536</span>
<span class="n">use_tf_opt</span> <span class="o">=</span> <span class="kc">True</span>

<span class="k">with</span> <span class="n">strategy</span><span class="o">.</span><span class="n">scope</span><span class="p">():</span>
    <span class="n">sok</span><span class="o">.</span><span class="n">Init</span><span class="p">(</span><span class="n">global_batch_size</span><span class="o">=</span><span class="n">global_batch_size</span><span class="p">)</span>

    <span class="n">model</span> <span class="o">=</span> <span class="n">DemoModel</span><span class="p">(</span>
        <span class="n">max_vocabulary_size_per_gpu</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span>
        <span class="n">slot_num</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
        <span class="n">nnz_per_slot</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
        <span class="n">embedding_vector_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
        <span class="n">num_of_dense_layers</span><span class="o">=</span><span class="mi">7</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">use_tf_opt</span><span class="p">:</span>
        <span class="n">emb_opt</span> <span class="o">=</span> <span class="n">sok</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">emb_opt</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

    <span class="n">dense_opt</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
</pre></div>
</div>
<p>For a DNN model built with SOK, <code class="docutils literal notranslate"><span class="pre">sok.Init</span></code> must be used to conduct initilizations. Please see <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/sparse_operation_kit/master/api/init.html#module-sparse_operation_kit.core.initialize">its API document</a>.</p>
<p><em><strong>define training step</strong></em></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">loss_fn</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">BinaryCrossentropy</span><span class="p">(</span><span class="n">from_logits</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">Reduction</span><span class="o">.</span><span class="n">NONE</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">_replica_loss</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">logits</span><span class="p">):</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">logits</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">compute_average_loss</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">global_batch_size</span><span class="o">=</span><span class="n">global_batch_size</span><span class="p">)</span>

<span class="nd">@tf</span><span class="o">.</span><span class="n">function</span>
<span class="k">def</span> <span class="nf">_train_step</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">_replica_loss</span><span class="p">(</span><span class="n">lables</span><span class="p">,</span> <span class="n">logits</span><span class="p">)</span>
    <span class="n">emb_var</span><span class="p">,</span> <span class="n">other_var</span> <span class="o">=</span> <span class="n">sok</span><span class="o">.</span><span class="n">split_embedding_variable_from_others</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">)</span>
    <span class="n">grads</span><span class="p">,</span> <span class="n">emb_grads</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="p">[</span><span class="n">other_var</span><span class="p">,</span> <span class="n">emb_var</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">use_tf_opt</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">sok</span><span class="o">.</span><span class="n">OptimizerScope</span><span class="p">(</span><span class="n">emb_var</span><span class="p">):</span>
            <span class="n">emb_opt</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">emb_grads</span><span class="p">,</span> <span class="n">emb_var</span><span class="p">),</span>
                                    <span class="n">experimental_aggregate_gradients</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">emb_opt</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">emb_grads</span><span class="p">,</span> <span class="n">emb_var</span><span class="p">),</span>
                                <span class="n">experimental_aggregate_gradients</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">dense_opt</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">other_var</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">loss</span>
</pre></div>
</div>
<p>If you are using native TensorFlow optimizers, such as <code class="docutils literal notranslate"><span class="pre">tf.keras.optimizers.Adam</span></code>, then <code class="docutils literal notranslate"><span class="pre">sok.OptimizerScope</span></code> must be used. Please see <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/sparse_operation_kit/master/api/utils/opt_scope.html#sparseoperationkit-optimizer-scope">its API document</a>.</p>
<p><em><strong>start training</strong></em></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">dataset</span> <span class="o">=</span> <span class="o">...</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dataset</span><span class="p">):</span>
    <span class="n">replica_loss</span> <span class="o">=</span> <span class="n">strategy</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">_train_step</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">))</span>
    <span class="n">total_loss</span> <span class="o">=</span> <span class="n">strategy</span><span class="o">.</span><span class="n">reduce</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">distribute</span><span class="o">.</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">SUM</span><span class="p">,</span> <span class="n">replica_loss</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;[SOK INFO]: Iteration: </span><span class="si">{}</span><span class="s2">, loss: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">total_loss</span><span class="p">))</span>
</pre></div>
</div>
<p>After these steps, the <code class="docutils literal notranslate"><span class="pre">DemoModel</span></code> will be successfully trained.</p>
</section>
<section id="with-tf-distribute-multiworkermirroredstrategy">
<h4>With tf.distribute.MultiWorkerMirroredStrategy<a class="headerlink" href="#with-tf-distribute-multiworkermirroredstrategy" title="Permalink to this headline"></a></h4>
<p>Documents for <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/distribute/MultiWorkerMirroredStrategy">tf.distribute.MultiWorkerMirroredStrategy</a>. <code class="docutils literal notranslate"><span class="pre">tf.distribute.MultiWorkerMirroredStrategy</span></code> is a tool to support data-parallel synchronized training in multiple machines, where there exists multiple GPUs in each machine.</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>The programming model for MultiWorkerMirroredStrategy is multi-processes &amp; multi-threads. Each process owns multi-threads and controls all available GPUs in single machine. Due to the GIL in CPython interpreter, it is hard to fully leverage all available CPU cores in each machine, which might impact the end-to-end training / inference performance. Therefore, it is recommended to use multiple processes in each machine, and each process controls one GPU.</p>
</div>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>By default, MultiWorkerMirroredStrategy will use all available GPUs in each process. Please set <code class="docutils literal notranslate"><span class="pre">CUDA_VISIBLE_DEVICES</span></code> or <code class="docutils literal notranslate"><span class="pre">tf.config.set_visible_devices</span></code> for each process to make each process controls different GPU.</p>
</div>
<p><em><strong>create MultiWorkerMirroredStrategy</strong></em></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span><span class="o">,</span> <span class="nn">json</span>

<span class="n">worker_num</span> <span class="o">=</span> <span class="mi">8</span> <span class="c1"># how many GPUs are used</span>
<span class="n">task_id</span> <span class="o">=</span> <span class="mi">0</span>    <span class="c1"># this process controls which GPU</span>

<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;CUDA_VISIBLE_DEVICES&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">task_id</span><span class="p">)</span> <span class="c1"># this procecss only controls this GPU</span>

<span class="n">port</span> <span class="o">=</span> <span class="mi">12345</span> <span class="c1"># could be arbitrary unused port on this machine</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;TF_CONFIG&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">({</span>
    <span class="s2">&quot;cluster&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;worker&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;localhost:&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">port</span> <span class="o">+</span> <span class="n">i</span><span class="p">)</span>
                            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">worker_num</span><span class="p">)]},</span>
    <span class="s2">&quot;task&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;worker&quot;</span><span class="p">,</span> <span class="s2">&quot;index&quot;</span><span class="p">:</span> <span class="n">task_id</span><span class="p">}</span>
<span class="p">})</span>
<span class="n">strategy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">distribute</span><span class="o">.</span><span class="n">MultiWorkerMirroredStrategy</span><span class="p">()</span>
</pre></div>
</div>
<p><em><strong>Other Steps</strong></em><br>
The steps <em><strong>create model instance under MultiWorkerMirroredStrategy.scope</strong></em>, <em><strong>define training step</strong></em> and <em><strong>start training</strong></em> are the same as which are described in <a class="reference external" href="#with-tf-distribute-mirroredstrategy">with tf.distribute.MirroredStrategy</a>. Please check that section.</p>
<p><em><strong>launch training program</strong></em><br>
Because multiple CPU processes are used in each machine for synchronized training, therefore <code class="docutils literal notranslate"><span class="pre">MPI</span></code> can be used to launch this program. For example:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>$ mpiexec -np <span class="m">8</span> <span class="o">[</span>mpi-args<span class="o">]</span> python3 main.py <span class="o">[</span>python-args<span class="o">]</span>
</pre></div>
</div>
</section>
</section>
<section id="use-sparseoperationkit-with-horovod">
<h3>Use SparseOperationKit with Horovod<a class="headerlink" href="#use-sparseoperationkit-with-horovod" title="Permalink to this headline"></a></h3>
<p>SparseOperationKit is also compatible with Horovod which is similar to <code class="docutils literal notranslate"><span class="pre">tf.distribute.MultiWorkerMirroredStrategy</span></code>.</p>
<p><em><strong>initialize horovod for tensorflow</strong></em></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">horovod.tensorflow</span> <span class="k">as</span> <span class="nn">hvd</span>
<span class="n">hvd</span><span class="o">.</span><span class="n">init</span><span class="p">()</span>

<span class="kn">import</span> <span class="nn">os</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;CUDA_VISIBLE_DEVICES&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">hvd</span><span class="o">.</span><span class="n">local_rank</span><span class="p">())</span> <span class="c1"># this process only controls one GPU</span>
</pre></div>
</div>
<p><em><strong>create model instance</strong></em></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">global_batch_size</span> <span class="o">=</span> <span class="mi">65536</span>
<span class="n">use_tf_opt</span> <span class="o">=</span> <span class="kc">True</span>

<span class="n">sok</span><span class="o">.</span><span class="n">Init</span><span class="p">(</span><span class="n">global_batch_size</span><span class="o">=</span><span class="n">global_batch_size</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">DemoModel</span><span class="p">(</span><span class="n">max_vocabulary_size_per_gpu</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span>
                  <span class="n">slot_num</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                  <span class="n">nnz_per_slot</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                  <span class="n">embedding_vector_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
                  <span class="n">num_of_dense_layers</span><span class="o">=</span><span class="mi">7</span><span class="p">)</span>

<span class="k">if</span> <span class="ow">not</span> <span class="n">use_tf_opt</span><span class="p">:</span>
    <span class="n">emb_opt</span> <span class="o">=</span> <span class="n">sok</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">emb_opt</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="n">dense_opt</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
</pre></div>
</div>
<p>For a DNN model built with SOK, <code class="docutils literal notranslate"><span class="pre">sok.Init()</span></code> must be called to conduct initializations. Please see <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/sparse_operation_kit/master/api/init.html#module-sparse_operation_kit.core.initialize">its API document</a>.</p>
<p><em><strong>define training step</strong></em></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">loss_fn</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">BinaryCrossentropy</span><span class="p">(</span><span class="n">from_logits</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">Reduction</span><span class="o">.</span><span class="n">NONE</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">_replica_loss</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">logits</span><span class="p">):</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">logits</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">compute_average_loss</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">global_batch_size</span><span class="o">=</span><span class="n">global_batch_size</span><span class="p">)</span>

<span class="nd">@tf</span><span class="o">.</span><span class="n">function</span>
<span class="k">def</span> <span class="nf">_train_step</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">first_batch</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">_replica_loss</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">logits</span><span class="p">)</span>
    <span class="n">emb_var</span><span class="p">,</span> <span class="n">other_var</span> <span class="o">=</span> <span class="n">sok</span><span class="o">.</span><span class="n">split_embedding_variable_from_others</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">)</span>
    <span class="n">emb_grads</span><span class="p">,</span> <span class="n">other_grads</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="p">[</span><span class="n">emb_var</span><span class="p">,</span> <span class="n">other_var</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">use_tf_opt</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">sok</span><span class="o">.</span><span class="n">OptimizerScope</span><span class="p">(</span><span class="n">emb_var</span><span class="p">):</span>
            <span class="n">emb_opt</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">emb_grads</span><span class="p">,</span> <span class="n">emb_var</span><span class="p">),</span>
                                    <span class="n">experimental_aggregate_gradients</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">emb_opt</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">emb_grads</span><span class="p">,</span> <span class="n">emb_var</span><span class="p">),</span>
                                <span class="n">experimental_aggregate_gradients</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    
    <span class="n">other_grads</span> <span class="o">=</span> <span class="p">[</span><span class="n">hvd</span><span class="o">.</span><span class="n">allreduce</span><span class="p">(</span><span class="n">grads</span><span class="p">)</span> <span class="k">for</span> <span class="n">grads</span> <span class="ow">in</span> <span class="n">other_grads</span><span class="p">]</span>
    <span class="n">dense_opt</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">other_grads</span><span class="p">,</span> <span class="n">other_var</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">first_batch</span><span class="p">:</span>
        <span class="n">hvd</span><span class="o">.</span><span class="n">broadcast_variables</span><span class="p">(</span><span class="n">other_var</span><span class="p">,</span> <span class="n">root_rank</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">hvd</span><span class="o">.</span><span class="n">broadcast_variables</span><span class="p">(</span><span class="n">dense_opt</span><span class="o">.</span><span class="n">variables</span><span class="p">(),</span> <span class="n">root_rank</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">loss</span>
</pre></div>
</div>
<p>If you are using native TensorFlow optimizers, such as <code class="docutils literal notranslate"><span class="pre">tf.keras.optimizers.Adam</span></code>, then <code class="docutils literal notranslate"><span class="pre">sok.OptimizerScope</span></code> must be used. Please see <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/sparse_operation_kit/master/api/utils/opt_scope.html#sparseoperationkit-optimizer-scope">its API document</a>.</p>
<p><em><strong>start training</strong></em></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">dataset</span> <span class="o">=</span> <span class="o">...</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dataset</span><span class="p">):</span>
    <span class="n">replica_loss</span> <span class="o">=</span> <span class="n">_train_step</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="mi">0</span> <span class="o">==</span> <span class="n">i</span><span class="p">)</span>
    <span class="n">total_loss</span> <span class="o">=</span> <span class="n">hvd</span><span class="o">.</span><span class="n">allreduce</span><span class="p">(</span><span class="n">replica_loss</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;[SOK INFO]: Iteration: </span><span class="si">{}</span><span class="s2">, loss: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">total_loss</span><span class="p">))</span>
</pre></div>
</div>
<p><em><strong>launch training program</strong></em><br>
You can use <code class="docutils literal notranslate"><span class="pre">horovodrun</span></code> or <code class="docutils literal notranslate"><span class="pre">mpiexec</span></code> to launch multiple processes in each machine for synchronized training. For example:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>$ horovodrun -np <span class="m">8</span> -H localhost:8 python3 main.py <span class="o">[</span>python-args<span class="o">]</span>
</pre></div>
</div>
</section>
</section>
<section id="tensorflow-1-15">
<h2>TensorFlow 1.15<a class="headerlink" href="#tensorflow-1-15" title="Permalink to this headline"></a></h2>
<p>SOK is also compatible with TensorFlow 1.15. Due to some restrictions in TF 1.15, only Horovod can be used as the communication tool.</p>
<section id="using-sparseoperationkit-with-horovod">
<h3>Using SparseOperationKit with Horovod<a class="headerlink" href="#using-sparseoperationkit-with-horovod" title="Permalink to this headline"></a></h3>
<p><em><strong>initialize horovod for tensorflow</strong></em></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">horovod.tensorflow</span> <span class="k">as</span> <span class="nn">hvd</span>
<span class="n">hvd</span><span class="o">.</span><span class="n">init</span><span class="p">()</span>

<span class="kn">import</span> <span class="nn">os</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;CUDA_VISIBLE_DEVICES&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">hvd</span><span class="o">.</span><span class="n">local_rank</span><span class="p">())</span> <span class="c1"># this process only controls one GPU</span>
</pre></div>
</div>
<p><em><strong>create model instance</strong></em></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">global_batch_size</span> <span class="o">=</span> <span class="mi">65536</span>
<span class="n">use_tf_opt</span> <span class="o">=</span> <span class="kc">True</span>

<span class="n">sok_init_op</span> <span class="o">=</span> <span class="n">sok</span><span class="o">.</span><span class="n">Init</span><span class="p">(</span><span class="n">global_batch_size</span><span class="o">=</span><span class="n">global_batch_size</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">DemoModel</span><span class="p">(</span><span class="n">max_vocabulary_size_per_gpu</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span>
                  <span class="n">slot_num</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                  <span class="n">nnz_per_slot</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                  <span class="n">embedding_vector_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
                  <span class="n">num_of_dense_layers</span><span class="o">=</span><span class="mi">7</span><span class="p">)</span>

<span class="k">if</span> <span class="ow">not</span> <span class="n">use_tf_opt</span><span class="p">:</span>
    <span class="n">emb_opt</span> <span class="o">=</span> <span class="n">sok</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">emb_opt</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">dense_opt</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
</pre></div>
</div>
<p>For a DNN model built with SOK, <code class="docutils literal notranslate"><span class="pre">sok.Init</span></code> must be used to conduct initilizations. Please see <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/sparse_operation_kit/master/api/init.html#module-sparse_operation_kit.core.initialize">its API document</a>.</p>
<p><em><strong>define training step</strong></em></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">loss_fn</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">BinaryCrossentropy</span><span class="p">(</span><span class="n">from_logits</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;none&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">_replica_loss</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">logits</span><span class="p">):</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">logits</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">compute_average_loss</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">global_batch_size</span><span class="o">=</span><span class="n">global_batch_size</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">training</span><span class="p">):</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">training</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">_replica_loss</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">logit</span><span class="p">)</span>
    <span class="n">emb_var</span><span class="p">,</span> <span class="n">other_var</span> <span class="o">=</span> <span class="n">sok</span><span class="o">.</span><span class="n">split_embedding_variable_from_others</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">)</span>
    <span class="n">grads</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">gradients</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">emb_var</span> <span class="o">+</span> <span class="n">other_var</span><span class="p">,</span> <span class="n">colocate_gradients_with_ops</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">emb_grads</span><span class="p">,</span> <span class="n">other_grads</span> <span class="o">=</span> <span class="n">grads</span><span class="p">[:</span><span class="nb">len</span><span class="p">(</span><span class="n">emb_var</span><span class="p">)],</span> <span class="n">grads</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">emb_var</span><span class="p">):]</span>

    <span class="k">if</span> <span class="n">use_tf_opt</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">sok</span><span class="o">.</span><span class="n">OptimizerScope</span><span class="p">(</span><span class="n">emb_var</span><span class="p">):</span>
            <span class="n">emb_train_op</span> <span class="o">=</span> <span class="n">emb_opt</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">emb_grads</span><span class="p">,</span> <span class="n">emb_var</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">emb_train_op</span> <span class="o">=</span> <span class="n">emb_opt</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">emb_grads</span><span class="p">,</span> <span class="n">emb_var</span><span class="p">))</span>

    <span class="n">other_grads</span> <span class="o">=</span> <span class="p">[</span><span class="n">hvd</span><span class="o">.</span><span class="n">allreduce</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span> <span class="k">for</span> <span class="n">grad</span> <span class="ow">in</span> <span class="n">other_grads</span><span class="p">]</span>
    <span class="n">other_train_op</span> <span class="o">=</span> <span class="n">dense_opt</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">other_grads</span><span class="p">,</span> <span class="n">other_var</span><span class="p">))</span>

    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">([</span><span class="n">emb_train_op</span><span class="p">,</span> <span class="n">other_train_op</span><span class="p">]):</span>
        <span class="n">total_loss</span> <span class="o">=</span> <span class="n">hvd</span><span class="o">.</span><span class="n">reduce</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
        <span class="n">total_loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">total_loss</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">total_loss</span>
</pre></div>
</div>
<p>If you are using native TensorFlow optimizers, such as <code class="docutils literal notranslate"><span class="pre">tf.keras.optimizers.Adam</span></code>, then <code class="docutils literal notranslate"><span class="pre">sok.OptimizerScope</span></code> must be used. Please see <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/sparse_operation_kit/master/api/utils/opt_scope.html#sparseoperationkit-optimizer-scope">its API document</a>.</p>
<p><em><strong>start training</strong></em></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">dataset</span> <span class="o">=</span> <span class="o">...</span>

<span class="n">loss</span> <span class="o">=</span> <span class="n">train_step</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>

<span class="n">init_op</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">group</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">(),</span>
                   <span class="n">tf</span><span class="o">.</span><span class="n">local_variables_initializer</span><span class="p">())</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">sok_init_op</span><span class="p">)</span>
    <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">init_op</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">iterations</span><span class="p">):</span>
        <span class="n">loss_v</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;[SOK INFO]: Iteration: </span><span class="si">{}</span><span class="s2">, loss: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">step</span><span class="p">,</span> <span class="n">loss_v</span><span class="p">))</span>
</pre></div>
</div>
<p><strong>Please be noted that <code class="docutils literal notranslate"><span class="pre">sok_init_op</span></code> must be the first step in <code class="docutils literal notranslate"><span class="pre">sess.run</span></code>, even before variables initialization.</strong></p>
<p><em><strong>launch training program</strong></em>
You can use <code class="docutils literal notranslate"><span class="pre">horovodrun</span></code> or <code class="docutils literal notranslate"><span class="pre">mpiexec</span></code> to launch multiple processes in each machine for synchronized training. For example:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>$ horovodrun -np <span class="m">8</span> -H localhost:8 python main.py <span class="o">[</span>args<span class="o">]</span>
</pre></div>
</div>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../features/features.html" class="btn btn-neutral float-left" title="Features in SparseOperationKit" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../examples/index.html" class="btn btn-neutral float-right" title="Examples" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, NVIDIA.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  
<div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    <span class="fa fa-book"> Other Versions</span>
    master: sok_v1.1.0
    <span class="fa fa-caret-down"></span>
  </span>
  <div class="rst-other-versions">
    <dl>
      <dt>Tags</dt>
      <dd><a href="../../sok_v1.0.0/get_started/get_started.html">sok_v1.0.0</a></dd>
      <dd><a href="get_started.html">sok_v1.1.0</a></dd>
    </dl>
  </div>
</div><script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>