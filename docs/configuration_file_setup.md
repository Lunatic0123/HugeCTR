# Configuration File Setup

The configuration file should be formatted using the JSON format. Here are some examples of how the configuration file should be formatted:
* [simple_sparse_embedding_fp32.json](../test/utest/simple_sparse_embedding_fp32.json)
* [dlrm_fp16_64k.json](../samples/dlrm/dlrm_fp16_64k.json)

There are three main JSON clauses in a configuration file:
* solver: Specifies various details such as active GPU list, batchsize, and model_file.
* optimizer: Specifies the type of optimizer and its hyperparameters.
* layers: Specifies training/evaluation data (and their paths), embeddings, and dense layers. Note that embeddings must precede the dense layers.

**NOTE**: They can be specified in any order.

## Solver ##

**NOTE**: You do not need to include the Solver clause in your configuration file when training models using the Python interface.

The solver clause contains the configuration for training resource and task items, which include the following parameters:
* `lr_policy`: Only currently supports `fixed`.
* `display`: Intervals to print loss on the screen.
* `gpu`: GPU indices used in the training process, which has two levels. For example: [[0,1],[1,2]] indicates that two nodes are used in the first node. GPUs 0 and 1 are used while GPUs 1 and 2 are used for the second node. It is also possible to specify non-continuous GPU indices such as [0, 2, 4, 7].
* `batchsize`: Minibatch used in training.
* `batchsize_eval`: Minibatch used in evaluation.
* `snapshot`: Intervals to save a checkpoint in the file with the prefix of `snapshot_prefix`.
* `eval_interval`: Evaluation intervals for a test set.
* `eval_batches`: Number of batches that will be used in the loss calculation of the evaluation. HugeCTR will print the average loss of the batches.
* `dense model_file`: File used for existing dense models. This file isn't needed for training that is being performed from scratch.
* `sparse_model_file`: File used for sparse models. If using HugeCTR version 2.1, please note that multi-embeddings only support one model. Therefore, each embedding will have one model file. This file isn't required for training that is being performed from scratch.
* `mixed_precision`: Enables mixed precision training with the scaler specified here. Only 128,256, 512, and 1024 scalers are supported.
* `enable_tf32_compute`: If you want to accelerate FP32 matrix multiplications within the FullyConnectedLayer and InteractionLayer, set this value to `true`. Its default value is `false`.
* `eval_metrics`: List of enabled evaluation metrics. You can use either `AUC` or `AverageLoss`, or both of them. For AUC, you can set its threshold, such as ["AUC:0.8025"], so that the training terminates when it reaches that threshold. By default, the threshold is unset.
* `input_key_type`: If your dataset format is `Norm`, you can choose the data type of each input key. The default is I32. For a `Parquet` dataset format to be generated by NVTabular, only I64 is allowed while I32 must be specified in order to use the `Raw` dataset format.

For example:
```json
 "solver": {
    "lr_policy": "fixed",
    "display": 1000,
    "max_iter": 300000,
    "gpu": [0],
    "batchsize": 512,
    "batchsize_eval": 256,
    "snapshot": 10000000,
    "snapshot_prefix": "./",
    "eval_interval": 1000,
    "eval_batches": 60,
    "mixed_precision": 256,
    "eval_metrics": ["AUC:0.8025"],
    "dense model_file": "./dense_model.bin",
    "sparse_model_file": ["./sparse_model1.bin","./sparse_model2.bin"]
  }
```

**NOTE**: Different batch sizes can be set in training and evaluation respectively. For example:
```
"batchsize": 512,
"batchsize_eval": 256,
```

## Optimizer ##
Optimizers are used in both dense and sparse models. We support the following optimizers: Adam, MomentumSGD, Nesterov, and SGD. All four optimizers support FP16. Note that different optimizers can be supported in the dense and embedding parts of the model. To enable specific optimizers in embeddings, please just add the optimizer clause to the embedding layer. Otherwise, the embedding layer will use the same optimizer as the dense part.

The embedding update supports three algorithms specified with `update_type`:
* `Local` (default value): The optimizer will only update the hot columns of an embedding in each iteration.
* `Global`: The optimizer will update all the columns. The embedding update type takes longer than the other embedding update types.
* `LazyGlobal`: The optimizer will only update the hot columns of an embedding in each iteration while using different semantics from the *local* and *global* updates.

For example:
```json
"optimizer": {
  "type": "Adam",
  "update_type": "Global",
  "adam_hparam": {
    "learning_rate": 0.001,
    "beta1": 0.9,
    "beta2": 0.999,
    "epsilon": 0.0000001
  }
}
"optimizer": {
  "type": "MomentumSGD",
  "update_type": "Local",
  "momentum_sgd_hparam": {
    "learning_rate": 0.01,
    "momentum_factor": 0.0
  }
}
"optimizer": {
  "type": "Nesterov",
  "update_type": "Global",
  "nesterov_hparam": {
    "learning_rate": 0.01,
    "momentum_factor": 0.0
  }
}
```

HugeCTR supports SGD optimizer and learning rate scheduling. For additional information, see [SGD optimizer and learning rate scheduling](docs/hugectr_user_guide.md#SGD-optimizer-and-learning-rate-scheduling).

## Layers ##
The clause `layer` is the network configuration that specifies the following layers:
* [data](#data)
* [embedding](#embedding)
* [dense](#dense)

### Data Layers ###
`Data` is considered the first layer in a configuration file. The sparse (embedding) and dense layers can access its inputs with their specified names.

We support the following dataset formats within our data layers:
* Norm
* Raw
* Paraquet

<div align=center><img width="80%" height="80%" src ="user_guide_src/dataset_format.png"/></div>
<div align=center>Fig. 1: (a) Norm (b) Raw (c) Parquet Dataset Formats</div>

<br>

For additional information about how to use these dataset formats with your configuration file, see [this section](#data-format).

#### Norm ####
To maximize the data loading performance and minimize the storage, the Norm dataset format consists of a collection of binary data files and an ASCII formatted file list. The model file should specify the file name of the training and testing (evaluation) set, maximum elements (key) in a sample, and the label dimensions as shown in Fig. 1 (a).

##### Data Files #####
A data file is the minimum reading granularity for a reading thread, so at least 10 files in each file list are required to achieve the best performance. A data file consists of a header and actual tabular data.

Header Definition:
```c
typedef struct DataSetHeader_ {
  long long error_check;        // 0: no error check; 1: check_num
  long long number_of_records;  // the number of samples in this data file
  long long label_dim;          // dimension of label
  long long dense_dim;          // dimension of dense feature
  long long slot_num;           // slot_num for each embedding
  long long reserved[3];        // reserved for future use
} DataSetHeader;
```

Data Definition (each sample):
```c
typedef struct Data_{
  int length;                   // bytes in this sample (optional: only in check_sum mode )
  float label[label_dim];
  float dense[dense_dim];
  Slot slots[slot_num];
  char checkbits;                // checkbit for this sample (optional: only in checksum mode)
} Data;

typedef struct Slot_{
  int nnz;
  unsigned int*  keys; // changeable to `long long` with `"input_key_type"` in `solver` object of the configuration file.
} Slot;
```

The Data field often has a lot of samples. Each sample starts with the labels formatted as integers and followed by `nnz` (number of nonzero) with the input key using the long long (or unsigned int) format as shown in Fig. 1 (a).

The input keys for categorical are distributed to the slots with no overlap allowed. For example: `slot[0] = {0,10,32,45}, slot[1] = {1,2,5,67}`. If there is any overlap, it will cause an undefined behavior. For example, given `slot[0] = {0,10,32,45}, slot[1] = {1,10,5,67}`, the table looking up the `10` key will produce different results based on how the slots are assigned to the GPUs.

##### File List
The first line of a file list should be the number of data files in the dataset with the paths to those files listed below as shown here:
```shell
$ cat simple_sparse_embedding_file_list.txt
10
./simple_sparse_embedding/simple_sparse_embedding0.data
./simple_sparse_embedding/simple_sparse_embedding1.data
./simple_sparse_embedding/simple_sparse_embedding2.data
./simple_sparse_embedding/simple_sparse_embedding3.data
./simple_sparse_embedding/simple_sparse_embedding4.data
./simple_sparse_embedding/simple_sparse_embedding5.data
./simple_sparse_embedding/simple_sparse_embedding6.data
./simple_sparse_embedding/simple_sparse_embedding7.data
./simple_sparse_embedding/simple_sparse_embedding8.data
./simple_sparse_embedding/simple_sparse_embedding9.data
```

Please note the following:
* All the nodes will share the same file list in training.
* `dense` and `sparse` should be configured in which dense_dim is set to 0 if no dense feature is involved. `dense` refers to the dense input and `sparse` refers to the `sparse` input. `sparse` should be an array here since we support multiple `embedding` and each one requires a `sparse` input.
* The sparse input `type`should be consistent with the following embeddings:
  - `slot_num`: Number of slots used in this training set. All the weight vectors that get out of a slot will be reduced into one vector after the embedding lookup. The sum of `slot_num` in each sparse input should be consistent with the slot number defined in the header of the training file.
  - `cache_eval_data`: To cache evaluation data on device, set this parameter to `true` to restrict the memory that will be used.
  - `max_nnz`: 1 can be specified to restrict the amount of memory used.

For example:
```json
    {
      "name": "data",
      "type": "Data",
      "source": "./file_list.txt",
      "eval_source": "./file_list_test.txt",
      "check": "Sum",
      "label": {
        "top": "label",
        "label_dim": 1
      },
      "dense": {
        "top": "dense",
        "dense_dim": 13
      },
      "sparse": [
        {
          "top": "data1",
          "type": "DistributedSlot",
          "max_feature_num_per_sample": 30,
          "slot_num": 26
        }
      ]
    }
```

#### Raw ####
The Raw dataset format is different from the Norm dataset format in that the training data appears in one binary file using int32. Fig. 1 (b) shows the structure of a Raw dataset sample.

**NOTE:** Only one-hot data is accepted with this format.

The following should be set in the configuration file:
* Number of train samples
* Number of evaluation samples
* Slot size of the embedding in the `"data"` clause
* Dense
* Category
* Label dimension

For example:
```json
{
"format": "Raw",
"num_samples": 4195155968,
"slot_size_array": [39884406, 39043, 17289, 7420, 20263, 3, 7120, 1543, 63, 38532951,  2953546,   403346, 10, 2208, 11938, 155, 4, 976, 14, 39979771, 25641295, 39664984, 585935, 12972, 108, 36],
"eval_num_samples": 89137319,
}
```

A proxy in C struct for a sample:
```c
typedef struct Data_{
  int label[label_dim];
  int dense[dense_dim];
  int category[sparse_dim];
} Data;
```

When using the Raw dataset format, a user must preprocess their own dataset to generate the continuous keys for each slot, and specify the list of the slot sizes with the `slot_size_array` option. Therefore, when referencing the configuration snippet above, we assume that slot 0 has the continuous keyset `{0, 1, 2 ... 39884405}` while slot 1 has its keyset on a different space `{0, 1, 2 ... 39043}`.

#### Parquet ####
Parquet is a column-oriented, open source, and free data format. It is available to any project in the Apache Hadoop ecosystem. To reduce file size, it supports compression and encoding. Fig. 1 (c) shows an example Parquet dataset. For additional information, see [its official documentation](https://parquet.apache.org/documentation/latest/).

Please note the following:
* Nested column types are not currently supported in the Parquet data loader.
* Any missing values in a column are not allowed.
* Like the Norm dataset format, the label and dense feature columns should use the float format.
* The Slot feature columns should use the Int64 format.
* The data columns within the Parquet file can be arranged in any order.
* To obtain the required information from all the rows in each parquet file and column index mapping for each label, dense (numerical), and slot (categorical) feature, a separate `_metadata.json` file is required.

For example:
```
{
"file_stats": [{"file_name": "file1.parquet", "num_rows": 6528076}, {"file_name": "file2.parquet", "num_rows": 6528076}],
"cats": [{"col_name": "C11", "index": 24}, {"col_name": "C24", "index": 37}, {"col_name": "C17", "index": 30}, {"col_name": "C7", "index": 20}, {"col_name": "C6", "index": 19}],
"conts": [{"col_name": "I5", "index": 5}, {"col_name": "I13", "index": 13}, {"col_name": "I2", "index": 2}, {"col_name": "I10", "index": 10}],
"labels": [{"col_name": "label", "index": 0}]
}
```

```json
  "layers": [
      {
       "name": "data",
       "type": "Data",
       "format": "Parquet",
       "slot_size_array": [220817330, 126535808, 3014529, 400781, 11, 2209, 11869, 148, 4, 977, 15, 38713, 283898298, 
        39644599, 181767044, 584616, 12883, 109, 37, 17177, 7425, 20266, 4, 7085, 1535, 64],
       "source": "_file_list.txt",
       "eval_source": "_file_list.txt",
       "check": "None",
       "label": {
              "top": "label",
              "label_dim": 1
       },
       "dense": {
              "top": "dense",
              "dense_dim": 13
       },
       "sparse": [
              {
              "top": "data1",
              "type": "LocalizedSlot",
              "max_feature_num_per_sample": 30,
              "max_nnz": 1,
              "slot_num": 26
              }
        ]
      }
```

Similar to the Raw dataset format, you must preprocess your own dataset to generate the continuous keys for each slot, and specify the list of the slot sizes with the `slot_size_array` option. Therefore, in the configuration snippet noted above, we assume that slot 0 has the continuous keyset `{0, 1, 2 ... 220817329}` and slot 1 has its keyset on a different space `{0, 1, 2 ... 126535807}`.

The following parameters are also configurable:
* `format`: `Raw` or `Parquet`
* `num_samples`: With the Parquet format, this parameter doesn’t need to be specified.
* `check`: Raw and Parquet dataset formats don’t use this parameter, so use the `None` value, or disregard this parameter altogether.
* `slot_size_array`: An array of table vocabulary size.
* `float_label_dense`: **This is valid only for the `Raw` dataset format.** If its value is set to `true`, the label and dense features for each sample are interpreted as `float` values. Otherwise, they are read as `int` values while the dense features are preprocessed with `log(dense[i] + 1.f)`. The default value is `false`.

For example:
```json
{
	"name": "data",
	"type": "Data",
	"format": "Raw",
	"num_samples": 4195155968,
	"slot_size_array": [39884406, 39043, 17289, 7420, 20263, 3, 7120, 1543,  63, 38532951,  2953546, 403346, 10, 2208, 11938, 
  155, 4, 976, 14, 39979771, 25641295, 39664984, 585935, 12972, 108, 36],
	"source": "/etc/workspace/dataset/train_data.bin",
	"eval_num_samples": 89137319,
	"eval_source": "/etc/workspace/dataset/test_data.bin",
	"check": "None",
	"cache_eval_data": true,
	"label": {
      "top": "label",
      "label_dim": 1
	},
	"dense": {
      "top": "dense",
      "dense_dim": 13
	},
	"sparse": [
    {
		"top": "data1",
		"type": "LocalizedSlot",
		"max_feature_num_per_sample": 26,
    "max_nnz": 1,
		"slot_num": 26
    }
	]
}
```

#### Non-Trainable Parameters
Some of the layers, such as Batch Norm, will generate statistical results during training. As a result, these layers are referred to as non-trainable parameters because they are outputs of CTR training and used in inference. During training, these non-trainable parameters will be placed into a configuration file along with weights.

For example:
```json
{
  "layers": [
    {
      "type": "BatchNorm",
      "mean": [-0.192325, 0.003050, -0.323447, -0.034817, -0.091861],
      "var": [0.738942, 0.410794, 1.370279, 1.156337, 0.638146]
    },
    {
      "type": "BatchNorm",
      "mean": [-0.759954, 0.251507, -0.648882, -0.176316, 0.515163],
      "var": [1.434012, 1.422724, 1.001451, 1.756962, 1.126412]
    },
    {
      "type": "BatchNorm",
      "mean": [0.851878, -0.837513, -0.694674, 0.791046, -0.849544],
      "var": [1.694500, 5.405566, 4.211646, 1.936811, 5.659098]
    }
  ]
}
```

### Embedding Layers
* An embedding table can be segmented into multiple slots, or feature fields, which spans multiple GPUs and multiple nodes. There are two types of embeddings that we support:
* `LocalizedSlotSparseEmbeddingHash`: Each individual slot is located in each GPU, and not shared. This type of embedding has the best scalability. The `plan_file` parameter should be specified when using this embedding. To generate a plan file, please refer to the [**README**](../samples/dcn/README.md) in the DCN sample. We also support `LocalizedSlotSparseEmbeddingOneHot`, which is an optimized version of `LocalizedSlotSparseEmbeddingHash`, as well as single-node training with p2p connections between each pair of GPUs and the one-hot input.
* `DistributedSlotSparseEmbeddingHash`: Each GPU will have a portion of a slot. This type of embedding is useful when there's an existing load imbalance among slots and OOM issues. This embedding should be used for single GPU training.

The following parameters can be set for both embeddings:
* `max_vocabulary_size_per_gpu`: Maximum possible size of the embedding for one GPU.
* `embedding_vec_size`: Size of each embedding vector.
* `combiner`: 0 is sum and 1 is mean.
* `optimizer`: (optional) HugeCTR supports different optimizers in dense and sparse models. If not specified, HugeCTR will reuse the optimizer specified for the dense model.

For example:
```json
    {
      "name": "sparse_embedding1",
      "type": "LocalizedSlotSparseEmbeddingHash",
      "bottom": "data1",
      "top": "sparse_embedding1",
      "plan_file": "all2all_plan_bi_1.json",
      "sparse_embedding_hparam": {
        "max_vocabulary_size_per_gpu": 1737710,
        "embedding_vec_size": 16,
        "combiner": 0
      },
      "optimizer": {
        "type": "Adam",
        "learning_rate": true,
        "adam_hparam": {
          "learning_rate": 0.001,
          "beta1": 0.9,
          "beta2": 0.999,
          "epsilon": 0.0000001
        }
      }
    }
```

### Dense Layers

* **Reshape**: The first layer that appears after the embedding layer to reshape the tensor from 3D to 2D. Reshape is the only layer that accepts both 3D and 2D input and the output must be 2D. `leading_dim` in `Reshape` is the leading dimension of the output.
* **Concat**: You can `Concat` at most five tensors into one and list the name in the `bottom` array. Note that the second dimension, which is usually batch size, should be the same.
* **Slice**: Copies specific `ranges` of input tensor to named output tensors. In the example below, we duplicate input tensor with `Slice`. 0 is inclusive and 429 is exclusive.

For example:
```json
    {
      "name": "reshape1",
      "type": "Reshape",
      "bottom": "sparse_embedding1",
      "top": "reshape1",
      "leading_dim": 416
    }
    {
      "name": "concat1",
      "type": "Concat",
      "bottom": ["reshape1","dense"],
      "top": "concat1"
    }
    {
      "name": "slice1",
      "type": "Slice",
      "bottom": "concat1",
      "ranges": [[0,429], [0,429]],
      "top": ["slice11", "slice12"]
    }
```

* **ELU**: The type name is `ELU`, and a `elu_param` called `alpha` in it can be configured.
* **Fully Connected** (`InnerProduct`): Bias is supported in fully connected layers and `num_output` is the dimension of output.
* **Fused Fully Connected** (`FusedInnerProduct`): Fused bias and RELU activation are added into a single layer.
* **Loss** (optional): Different from the other layers, with `loss` you can specify which `regularization` you will use. By default, no regularization will be used.

For example:
```json
    {
      "name": "elu1",
      "type": "ELU",
      "bottom": "fc1",
      "top": "elu1",
      "elu_param": {
        "alpha": 1.0
      }
    }
    {
      "name": "fc8",
      "type": "InnerProduct",
      "bottom": "concat2",
      "top": "fc8",
      "fc_param": {
        "num_output": 1
      }
    }
    {
      "name": "fc2",
      "type": "FusedInnerProduct",
      "bottom": "fc1",
      "top": "fc2",
      "fc_param": {
        "num_output": 256
      }
    }
    {
      "name": "loss",
      "type": "BinaryCrossEntropyLoss",
      "bottom": ["fc8","label"],
      "regularizer": "L2",
      "top": "loss"
    }
```

Interaction layer example:
```json
{
  "name": "interaction1",
  "type": "Interaction",
  "bottom": ["fc3", "sparse_embedding1"],
  "top": "interaction1"
}
```

For additional information, see [**parser.cpp**](../HugeCTR/src/parser.cpp).

#### Model File
The Model file is formatted using binary. The purpose of the model file is to initialize model weights and identify where the trained weight is stored. With the Model file, it is assumed that the weights are stored in the same order as the layers in the configuration file that are currently in use.

We provide a tutorial regarding [how to dump a model to TensorFlow](../tutorial/dump_to_tf/readMe.md).
