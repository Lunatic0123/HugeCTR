{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b906abab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "# Each user is responsible for checking the content of datasets and the\n",
    "# applicable licenses and determining if suitable for the intended use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6958f0",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/notebooks/dlsw-notebooks/merlin_hugectr_hugectr-wdl-prediction/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# HugeCTR Wide and Deep Model with Criteo\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this notebook, we provide a tutorial that shows how to train a wide and deep model using the high-level Python API from HugeCTR on the original Criteo dataset as training data.\n",
    "We show how to produce prediction results based on different types of local database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235745a8",
   "metadata": {},
   "source": [
    "## Setup HugeCTR\n",
    "\n",
    "To setup the environment, refer to [HugeCTR Example Notebooks](../notebooks) and follow the instructions there before running the following."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf41447",
   "metadata": {},
   "source": [
    "## Dataset Preprocessing\n",
    "\n",
    "### Generate training and validation data folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae158c8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define some data folder to store the original and preprocessed data\n",
    "# Standard Libraries\n",
    "import os\n",
    "from time import time\n",
    "import re\n",
    "import shutil\n",
    "import glob\n",
    "import warnings\n",
    "BASE_DIR = \"/wdl_train\"\n",
    "train_path  = os.path.join(BASE_DIR, \"train\")\n",
    "val_path = os.path.join(BASE_DIR, \"val\")\n",
    "CUDA_VISIBLE_DEVICES = os.environ.get(\"CUDA_VISIBLE_DEVICES\", \"0\")\n",
    "n_workers = len(CUDA_VISIBLE_DEVICES.split(\",\"))\n",
    "frac_size = 0.15\n",
    "allow_multi_gpu = False\n",
    "use_rmm_pool = False\n",
    "max_day = None  # (Optional) -- Limit the dataset to day 0-max_day for debugging\n",
    "\n",
    "if os.path.isdir(train_path):\n",
    "    shutil.rmtree(train_path)\n",
    "os.makedirs(train_path)\n",
    "\n",
    "if os.path.isdir(val_path):\n",
    "    shutil.rmtree(val_path)\n",
    "os.makedirs(val_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e7db3c",
   "metadata": {},
   "source": [
    "### Download the original Criteo dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfa9fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da41ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -P $train_path https://storage.googleapis.com/criteo-cail-datasets/day_0.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4508791b",
   "metadata": {},
   "source": [
    "\n",
    "Split the dataset into training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f9620ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!gzip -d -c $train_path/day_0.gz > day_0\n",
    "!head -n 10000000 day_0 > $train_path/train.txt\n",
    "!tail -n 2000000 day_0 > $val_path/test.txt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9919817c",
   "metadata": {},
   "source": [
    "### Preprocessing with NVTabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "616c6f19",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /wdl_train/preprocess.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile '/wdl_train/preprocess.py'\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import glob\n",
    "import time\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "import dask_cudf\n",
    "from dask_cuda import LocalCUDACluster\n",
    "from dask.distributed import Client\n",
    "\n",
    "import nvtabular as nvt\n",
    "from merlin.core.compat import device_mem_size\n",
    "from nvtabular.ops import (\n",
    "    Categorify,\n",
    "    Clip,\n",
    "    FillMissing,\n",
    "    Normalize,\n",
    "    get_embedding_sizes,\n",
    ")\n",
    "\n",
    "\n",
    "# %load_ext memory_profiler\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format=\"%(asctime)s %(message)s\")\n",
    "logging.root.setLevel(logging.NOTSET)\n",
    "logging.getLogger(\"numba\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"asyncio\").setLevel(logging.WARNING)\n",
    "\n",
    "# define dataset schema\n",
    "CATEGORICAL_COLUMNS = [\"C\" + str(x) for x in range(1, 27)]\n",
    "CONTINUOUS_COLUMNS = [\"I\" + str(x) for x in range(1, 14)]\n",
    "LABEL_COLUMNS = [\"label\"]\n",
    "COLUMNS = LABEL_COLUMNS + CONTINUOUS_COLUMNS + CATEGORICAL_COLUMNS\n",
    "# /samples/criteo mode doesn't have dense features\n",
    "criteo_COLUMN = LABEL_COLUMNS + CATEGORICAL_COLUMNS\n",
    "# For new feature cross columns\n",
    "CROSS_COLUMNS = []\n",
    "\n",
    "\n",
    "NUM_INTEGER_COLUMNS = 13\n",
    "NUM_CATEGORICAL_COLUMNS = 26\n",
    "NUM_TOTAL_COLUMNS = 1 + NUM_INTEGER_COLUMNS + NUM_CATEGORICAL_COLUMNS\n",
    "\n",
    "# compute the partition size with GB\n",
    "def bytesto(bytes, to, bsize=1024):\n",
    "    a = {\"k\": 1, \"m\": 2, \"g\": 3, \"t\": 4, \"p\": 5, \"e\": 6}\n",
    "    r = float(bytes)\n",
    "    return bytes / (bsize ** a[to])\n",
    "\n",
    "\n",
    "# process the data with NVTabular\n",
    "def process_NVT(args):\n",
    "    if args.feature_cross_list:\n",
    "        feature_pairs = [pair.split(\"_\") for pair in args.feature_cross_list.split(\",\")]\n",
    "        for pair in feature_pairs:\n",
    "            CROSS_COLUMNS.append(pair[0] + \"_\" + pair[1])\n",
    "\n",
    "    logging.info(\"NVTabular processing\")\n",
    "    train_input = os.path.join(args.data_path, \"train/train.txt\")\n",
    "    val_input = os.path.join(args.data_path, \"val/test.txt\")\n",
    "    PREPROCESS_DIR_temp_train = os.path.join(args.out_path, \"train/temp-parquet-after-conversion\")\n",
    "    PREPROCESS_DIR_temp_val = os.path.join(args.out_path, \"val/temp-parquet-after-conversion\")\n",
    "    PREPROCESS_DIR_temp = [PREPROCESS_DIR_temp_train, PREPROCESS_DIR_temp_val]\n",
    "    train_output = os.path.join(args.out_path, \"train\")\n",
    "    val_output = os.path.join(args.out_path, \"val\")\n",
    "\n",
    "    # Make sure we have a clean parquet space for cudf conversion\n",
    "    for one_path in PREPROCESS_DIR_temp:\n",
    "        if os.path.exists(one_path):\n",
    "            shutil.rmtree(one_path)\n",
    "        os.mkdir(one_path)\n",
    "\n",
    "    ## Get Dask Client\n",
    "\n",
    "    # Deploy a Single-Machine Multi-GPU Cluster\n",
    "    device_size = device_mem_size(kind=\"total\")\n",
    "    device_pool_size = int(args.device_pool_frac * device_size)\n",
    "    cluster = None\n",
    "    if args.protocol == \"ucx\":\n",
    "        UCX_TLS = os.environ.get(\"UCX_TLS\", \"tcp,cuda_copy,cuda_ipc,sockcm\")\n",
    "        os.environ[\"UCX_TLS\"] = UCX_TLS\n",
    "        cluster = LocalCUDACluster(\n",
    "            protocol=args.protocol,\n",
    "            CUDA_VISIBLE_DEVICES=args.devices,\n",
    "            n_workers=len(args.devices.split(\",\")),\n",
    "            device_memory_limit=int(device_size * args.device_limit_frac),\n",
    "            dashboard_address=\":\" + args.dashboard_port,\n",
    "            rmm_pool_size=(device_pool_size // 256) * 256,\n",
    "        )\n",
    "    else:\n",
    "        cluster = LocalCUDACluster(\n",
    "            protocol=args.protocol,\n",
    "            n_workers=len(args.devices.split(\",\")),\n",
    "            CUDA_VISIBLE_DEVICES=args.devices,\n",
    "            device_memory_limit=int(device_size * args.device_limit_frac),\n",
    "            dashboard_address=\":\" + args.dashboard_port,\n",
    "            rmm_pool_size=(device_pool_size // 256) * 256,\n",
    "        )\n",
    "\n",
    "    # Create the distributed client\n",
    "    if cluster:\n",
    "        client = Client(cluster)\n",
    "    else:\n",
    "        client = Client(processes=False)\n",
    "\n",
    "    # calculate the total processing time\n",
    "    runtime = time.time()\n",
    "\n",
    "    # test dataset without the label feature\n",
    "    if args.dataset_type == \"test\":\n",
    "        global LABEL_COLUMNS\n",
    "        LABEL_COLUMNS = []\n",
    "\n",
    "    ##-----------------------------------##\n",
    "    # Dask rapids converts txt to parquet\n",
    "    # Dask cudf dataframe = ddf\n",
    "\n",
    "    ## train/valid txt to parquet\n",
    "    train_valid_paths = [\n",
    "        (train_input, PREPROCESS_DIR_temp_train),\n",
    "        (val_input, PREPROCESS_DIR_temp_val),\n",
    "    ]\n",
    "\n",
    "    for input, temp_output in train_valid_paths:\n",
    "        ddf = dask_cudf.read_csv(\n",
    "            input, sep=\"\\t\", names=LABEL_COLUMNS + CONTINUOUS_COLUMNS + CATEGORICAL_COLUMNS\n",
    "        )\n",
    "\n",
    "        if args.feature_cross_list:\n",
    "            feature_pairs = [pair.split(\"_\") for pair in args.feature_cross_list.split(\",\")]\n",
    "            for pair in args.feature_cross_list.split(\",\"):\n",
    "                feature_pair = pair.split(\"_\")\n",
    "                ddf[pair] = ddf[feature_pair[0]] + ddf[feature_pair[1]]\n",
    "\n",
    "        ## Convert label col to FP32\n",
    "        if args.parquet_format and args.dataset_type == \"train\":\n",
    "            ddf[\"label\"] = ddf[\"label\"].astype(\"float32\")\n",
    "\n",
    "        # Save it as parquet format for better memory usage\n",
    "        ddf.to_parquet(temp_output, header=True)\n",
    "        ##-----------------------------------##\n",
    "\n",
    "    COLUMNS = LABEL_COLUMNS + CONTINUOUS_COLUMNS + CROSS_COLUMNS + CATEGORICAL_COLUMNS\n",
    "    train_paths = glob.glob(os.path.join(PREPROCESS_DIR_temp_train, \"*.parquet\"))\n",
    "    valid_paths = glob.glob(os.path.join(PREPROCESS_DIR_temp_val, \"*.parquet\"))\n",
    "\n",
    "    categorify_op = Categorify(freq_threshold=args.freq_limit)\n",
    "    cat_features = CATEGORICAL_COLUMNS >> categorify_op\n",
    "    cont_features = CONTINUOUS_COLUMNS >> FillMissing() >> Clip(min_value=0) >> Normalize()\n",
    "    cross_cat_op = Categorify(freq_threshold=args.freq_limit)\n",
    "\n",
    "    features = LABEL_COLUMNS\n",
    "    if args.criteo_mode == 0:\n",
    "        features += cont_features\n",
    "        for pair in args.feature_cross_list.split(\",\"):\n",
    "            features += [pair] >> cross_cat_op\n",
    "\n",
    "    features += cat_features\n",
    "\n",
    "    workflow = nvt.Workflow(features, client=client)\n",
    "\n",
    "    logging.info(\"Preprocessing\")\n",
    "\n",
    "    output_format = \"hugectr\"\n",
    "    if args.parquet_format:\n",
    "        output_format = \"parquet\"\n",
    "\n",
    "    # just for /samples/criteo model\n",
    "    train_ds_iterator = nvt.Dataset(\n",
    "        train_paths, engine=\"parquet\", part_size=int(args.part_mem_frac * device_size)\n",
    "    )\n",
    "    valid_ds_iterator = nvt.Dataset(\n",
    "        valid_paths, engine=\"parquet\", part_size=int(args.part_mem_frac * device_size)\n",
    "    )\n",
    "\n",
    "    shuffle = None\n",
    "    if args.shuffle == \"PER_WORKER\":\n",
    "        shuffle = nvt.io.Shuffle.PER_WORKER\n",
    "    elif args.shuffle == \"PER_PARTITION\":\n",
    "        shuffle = nvt.io.Shuffle.PER_PARTITION\n",
    "\n",
    "    logging.info(\"Train Datasets Preprocessing.....\")\n",
    "\n",
    "    dict_dtypes = {}\n",
    "    for col in CATEGORICAL_COLUMNS:\n",
    "        dict_dtypes[col] = np.int64\n",
    "    if not args.criteo_mode:\n",
    "        for col in CONTINUOUS_COLUMNS:\n",
    "            dict_dtypes[col] = np.float32\n",
    "    for col in CROSS_COLUMNS:\n",
    "        dict_dtypes[col] = np.int64\n",
    "    for col in LABEL_COLUMNS:\n",
    "        dict_dtypes[col] = np.float32\n",
    "\n",
    "    conts = CONTINUOUS_COLUMNS if not args.criteo_mode else []\n",
    "\n",
    "    workflow.fit(train_ds_iterator)\n",
    "\n",
    "    if output_format == \"hugectr\":\n",
    "        workflow.transform(train_ds_iterator).to_hugectr(\n",
    "            cats=CROSS_COLUMNS + CATEGORICAL_COLUMNS,\n",
    "            conts=conts,\n",
    "            labels=LABEL_COLUMNS,\n",
    "            output_path=train_output,\n",
    "            shuffle=shuffle,\n",
    "            out_files_per_proc=args.out_files_per_proc,\n",
    "            num_threads=args.num_io_threads,\n",
    "        )\n",
    "    else:\n",
    "        workflow.transform(train_ds_iterator).to_parquet(\n",
    "            output_path=train_output,\n",
    "            dtypes=dict_dtypes,\n",
    "            cats=CROSS_COLUMNS + CATEGORICAL_COLUMNS,\n",
    "            conts=conts,\n",
    "            labels=LABEL_COLUMNS,\n",
    "            shuffle=shuffle,\n",
    "            out_files_per_proc=args.out_files_per_proc,\n",
    "            num_threads=args.num_io_threads,\n",
    "        )\n",
    "\n",
    "    logging.info(\"Valid Datasets Preprocessing.....\")\n",
    "\n",
    "    if output_format == \"hugectr\":\n",
    "        workflow.transform(valid_ds_iterator).to_hugectr(\n",
    "            cats=CATEGORICAL_COLUMNS + CROSS_COLUMNS,\n",
    "            conts=conts,\n",
    "            labels=LABEL_COLUMNS,\n",
    "            output_path=val_output,\n",
    "            shuffle=shuffle,\n",
    "            out_files_per_proc=args.out_files_per_proc,\n",
    "            num_threads=args.num_io_threads,\n",
    "        )\n",
    "    else:\n",
    "        workflow.transform(valid_ds_iterator).to_parquet(\n",
    "            output_path=val_output,\n",
    "            dtypes=dict_dtypes,\n",
    "            cats=CATEGORICAL_COLUMNS + CROSS_COLUMNS,\n",
    "            conts=conts,\n",
    "            labels=LABEL_COLUMNS,\n",
    "            shuffle=shuffle,\n",
    "            out_files_per_proc=args.out_files_per_proc,\n",
    "            num_threads=args.num_io_threads,\n",
    "        )\n",
    "\n",
    "    embeddings_dict_cat = categorify_op.get_embedding_sizes(CATEGORICAL_COLUMNS)\n",
    "    embeddings_dict_cross = cross_cat_op.get_embedding_sizes(CROSS_COLUMNS)\n",
    "    embeddings = [embeddings_dict_cross[c][0] for c in CROSS_COLUMNS] + [\n",
    "        embeddings_dict_cat[c][0] for c in CATEGORICAL_COLUMNS\n",
    "    ]\n",
    "\n",
    "    print(\"Slot size array is: \", embeddings)\n",
    "    ##--------------------##\n",
    "\n",
    "    logging.info(\"NVTabular processing done\")\n",
    "\n",
    "    runtime = time.time() - runtime\n",
    "\n",
    "    print(\"\\nDask-NVTabular Criteo Preprocessing\")\n",
    "    print(\"--------------------------------------\")\n",
    "    print(f\"data_path          | {args.data_path}\")\n",
    "    print(f\"output_path        | {args.out_path}\")\n",
    "    print(f\"partition size     | {'%.2f GB'%bytesto(int(args.part_mem_frac * device_size),'g')}\")\n",
    "    print(f\"protocol           | {args.protocol}\")\n",
    "    print(f\"device(s)          | {args.devices}\")\n",
    "    print(f\"rmm-pool-frac      | {(args.device_pool_frac)}\")\n",
    "    print(f\"out-files-per-proc | {args.out_files_per_proc}\")\n",
    "    print(f\"num_io_threads     | {args.num_io_threads}\")\n",
    "    print(f\"shuffle            | {args.shuffle}\")\n",
    "    print(\"======================================\")\n",
    "    print(f\"Runtime[s]         | {runtime}\")\n",
    "    print(\"======================================\\n\")\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=(\"Multi-GPU Criteo Preprocessing\"))\n",
    "\n",
    "    #\n",
    "    # System Options\n",
    "    #\n",
    "\n",
    "    parser.add_argument(\"--data_path\", type=str, help=\"Input dataset path (Required)\")\n",
    "    parser.add_argument(\"--out_path\", type=str, help=\"Directory path to write output (Required)\")\n",
    "    parser.add_argument(\n",
    "        \"-d\",\n",
    "        \"--devices\",\n",
    "        default=os.environ.get(\"CUDA_VISIBLE_DEVICES\", \"0\"),\n",
    "        type=str,\n",
    "        help='Comma-separated list of visible devices (e.g. \"0,1,2,3\"). ',\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-p\",\n",
    "        \"--protocol\",\n",
    "        choices=[\"tcp\", \"ucx\"],\n",
    "        default=\"tcp\",\n",
    "        type=str,\n",
    "        help=\"Communication protocol to use (Default 'tcp')\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--device_limit_frac\",\n",
    "        default=0.5,\n",
    "        type=float,\n",
    "        help=\"Worker device-memory limit as a fraction of GPU capacity (Default 0.8). \",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--device_pool_frac\",\n",
    "        default=0.9,\n",
    "        type=float,\n",
    "        help=\"RMM pool size for each worker  as a fraction of GPU capacity (Default 0.9). \"\n",
    "        \"The RMM pool frac is the same for all GPUs, make sure each one has enough memory size\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_io_threads\",\n",
    "        default=0,\n",
    "        type=int,\n",
    "        help=\"Number of threads to use when writing output data (Default 0). \"\n",
    "        \"If 0 is specified, multi-threading will not be used for IO.\",\n",
    "    )\n",
    "\n",
    "    #\n",
    "    # Data-Decomposition Parameters\n",
    "    #\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--part_mem_frac\",\n",
    "        default=0.125,\n",
    "        type=float,\n",
    "        help=\"Maximum size desired for dataset partitions as a fraction \"\n",
    "        \"of GPU capacity (Default 0.125)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--out_files_per_proc\",\n",
    "        default=8,\n",
    "        type=int,\n",
    "        help=\"Number of output files to write on each worker (Default 8)\",\n",
    "    )\n",
    "\n",
    "    #\n",
    "    # Preprocessing Options\n",
    "    #\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"-f\",\n",
    "        \"--freq_limit\",\n",
    "        default=0,\n",
    "        type=int,\n",
    "        help=\"Frequency limit for categorical encoding (Default 0)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-s\",\n",
    "        \"--shuffle\",\n",
    "        choices=[\"PER_WORKER\", \"PER_PARTITION\", \"NONE\"],\n",
    "        default=\"PER_PARTITION\",\n",
    "        help=\"Shuffle algorithm to use when writing output data to disk (Default PER_PARTITION)\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--feature_cross_list\",\n",
    "        default=None,\n",
    "        type=str,\n",
    "        help=\"List of feature crossing cols (e.g. C1_C2, C3_C4)\",\n",
    "    )\n",
    "\n",
    "    #\n",
    "    # Diagnostics Options\n",
    "    #\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--profile\",\n",
    "        metavar=\"PATH\",\n",
    "        default=None,\n",
    "        type=str,\n",
    "        help=\"Specify a file path to export a Dask profile report (E.g. dask-report.html).\"\n",
    "        \"If this option is excluded from the command, not profile will be exported\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dashboard_port\",\n",
    "        default=\"8787\",\n",
    "        type=str,\n",
    "        help=\"Specify the desired port of Dask's diagnostics-dashboard (Default `3787`). \"\n",
    "        \"The dashboard will be hosted at http://<IP>:<PORT>/status\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\"--criteo_mode\", type=int, default=0)\n",
    "    parser.add_argument(\"--parquet_format\", type=int, default=1)\n",
    "    parser.add_argument(\"--dataset_type\", type=str, default=\"train\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    args.n_workers = len(args.devices.split(\",\"))\n",
    "    return args\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = parse_args()\n",
    "\n",
    "    process_NVT(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f7e259b",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-26 04:30:43,128 NVTabular processing\n",
      "2023-05-26 04:30:45,000 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize\n",
      "2023-05-26 04:30:45,000 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize\n",
      "2023-05-26 04:30:53,847 Preprocessing\n",
      "2023-05-26 04:30:54,160 Train Datasets Preprocessing.....\n",
      "2023-05-26 04:31:14,725 Valid Datasets Preprocessing.....\n",
      "Slot size array is:  [62962, 127889, 56869, 12448, 11969, 6832, 18364, 4, 5960, 1170, 43, 57084, 29015, 33861, 11, 1956, 5598, 55, 4, 913, 15, 56488, 48591, 57463, 26037, 7790, 58, 34]\n",
      "2023-05-26 04:31:18,677 NVTabular processing done\n",
      "\n",
      "Dask-NVTabular Criteo Preprocessing\n",
      "--------------------------------------\n",
      "data_path          | /wdl_train/\n",
      "output_path        | /wdl_train/\n",
      "partition size     | 3.97 GB\n",
      "protocol           | tcp\n",
      "device(s)          | 0\n",
      "rmm-pool-frac      | 0.5\n",
      "out-files-per-proc | 8\n",
      "num_io_threads     | 2\n",
      "shuffle            | PER_PARTITION\n",
      "======================================\n",
      "Runtime[s]         | 32.06131315231323\n",
      "======================================\n",
      "\n",
      "2023-05-26 04:31:18,682 Attempted to close worker that is already Status.closing. Reason: worker-handle-scheduler-connection-broken\n",
      "2023-05-26 04:31:18,683 Attempted to close worker that is already Status.closed. Reason: worker-close\n"
     ]
    }
   ],
   "source": [
    "!python3 /wdl_train/preprocess.py --data_path /wdl_train/ \\\n",
    "--out_path /wdl_train/ --freq_limit 6 --feature_cross_list C1_C2,C3_C4 \\\n",
    "--device_pool_frac 0.5  --devices '0' --num_io_threads 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4125c1c9",
   "metadata": {},
   "source": [
    "#### Check the preprocessed training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8c15e77",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 3103496\n",
      "-rw-r--r-- 1 root root        258 May 26 04:31 _file_list.txt\n",
      "-rw-r--r-- 1 root root     271567 May 26 04:31 _metadata\n",
      "-rw-r--r-- 1 root root       1887 May 26 04:31 _metadata.json\n",
      "-rw-r--r-- 1 root root   79777109 May 26 04:31 part_0.parquet\n",
      "-rw-r--r-- 1 root root   79821862 May 26 04:31 part_1.parquet\n",
      "-rw-r--r-- 1 root root   79946970 May 26 04:31 part_2.parquet\n",
      "-rw-r--r-- 1 root root   79783392 May 26 04:31 part_3.parquet\n",
      "-rw-r--r-- 1 root root   79875076 May 26 04:31 part_4.parquet\n",
      "-rw-r--r-- 1 root root   79844899 May 26 04:31 part_5.parquet\n",
      "-rw-r--r-- 1 root root   79876452 May 26 04:31 part_6.parquet\n",
      "-rw-r--r-- 1 root root   79767942 May 26 04:31 part_7.parquet\n",
      "-rw-r--r-- 1 root root      31277 May 26 04:31 schema.pbtxt\n",
      "drwxr-xr-x 2 root root        226 May 26 04:30 temp-parquet-after-conversion\n",
      "-rw-r--r-- 1 root root 2538954147 May 26 04:30 train.txt\n"
     ]
    }
   ],
   "source": [
    "!ls -ll /wdl_train/train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f671766",
   "metadata": {},
   "source": [
    "### WDL Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aad42841",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile './model.py'\n",
    "import hugectr\n",
    "#from mpi4py import MPI\n",
    "solver = hugectr.CreateSolver(max_eval_batches = 4000,\n",
    "                              batchsize_eval = 2720,\n",
    "                              batchsize = 2720,\n",
    "                              lr = 0.001,\n",
    "                              vvgpu = [[1]],\n",
    "                              repeat_dataset = True,\n",
    "                              i64_input_key = True)\n",
    "\n",
    "reader = hugectr.DataReaderParams(data_reader_type = hugectr.DataReaderType_t.Parquet,\n",
    "                                  source = [\"/wdl_train/train/_file_list.txt\"],\n",
    "                                  eval_source = \"/wdl_train/val/_file_list.txt\",\n",
    "                                  check_type = hugectr.Check_t.Non,\n",
    "                                  slot_size_array = [278018, 415262, 249058, 19561, 14212, 6890, 18592, 4, 6356, 1254, 52, 226170, 80508, 72308, 11, 2169, 7597, 61, 4, 923, 15, 249619, 168974, 243480, 68212, 9169, 75, 34])\n",
    "optimizer = hugectr.CreateOptimizer(optimizer_type = hugectr.Optimizer_t.Adam,\n",
    "                                    update_type = hugectr.Update_t.Global,\n",
    "                                    beta1 = 0.9,\n",
    "                                    beta2 = 0.999,\n",
    "                                    epsilon = 0.0000001)\n",
    "model = hugectr.Model(solver, reader, optimizer)\n",
    "\n",
    "model.add(hugectr.Input(label_dim = 1, label_name = \"label\",\n",
    "                        dense_dim = 13, dense_name = \"dense\",\n",
    "                        data_reader_sparse_param_array = \n",
    "                        [hugectr.DataReaderSparseParam(\"wide_data\", 1, True, 2),\n",
    "                        hugectr.DataReaderSparseParam(\"deep_data\", 2, False, 26)]))\n",
    "\n",
    "model.add(hugectr.SparseEmbedding(embedding_type = hugectr.Embedding_t.DistributedSlotSparseEmbeddingHash, \n",
    "                            workspace_size_per_gpu_in_mb = 80,\n",
    "                            embedding_vec_size = 1,\n",
    "                            combiner = \"sum\",\n",
    "                            sparse_embedding_name = \"sparse_embedding2\",\n",
    "                            bottom_name = \"wide_data\",\n",
    "                            optimizer = optimizer))\n",
    "model.add(hugectr.SparseEmbedding(embedding_type = hugectr.Embedding_t.DistributedSlotSparseEmbeddingHash, \n",
    "                            workspace_size_per_gpu_in_mb = 1350,\n",
    "                            embedding_vec_size = 16,\n",
    "                            combiner = \"sum\",\n",
    "                            sparse_embedding_name = \"sparse_embedding1\",\n",
    "                            bottom_name = \"deep_data\",\n",
    "                            optimizer = optimizer))\n",
    "\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Reshape,\n",
    "                            bottom_names = [\"sparse_embedding1\"],\n",
    "                            top_names = [\"reshape1\"],\n",
    "                            leading_dim=416))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Reshape,\n",
    "                            bottom_names = [\"sparse_embedding2\"],\n",
    "                            top_names = [\"reshape2\"],\n",
    "                            leading_dim=2))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReduceSum,\n",
    "                            bottom_names = [\"reshape2\"],\n",
    "                            top_names = [\"wide_redn\"],\n",
    "                            axis = 1))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Concat,\n",
    "                            bottom_names = [\"reshape1\", \"dense\"],\n",
    "                            top_names = [\"concat1\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"concat1\"],\n",
    "                            top_names = [\"fc1\"],\n",
    "                            num_output=1024))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc1\"],\n",
    "                            top_names = [\"relu1\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Dropout,\n",
    "                            bottom_names = [\"relu1\"],\n",
    "                            top_names = [\"dropout1\"],\n",
    "                            dropout_rate=0.5))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"dropout1\"],\n",
    "                            top_names = [\"fc2\"],\n",
    "                            num_output=1024))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc2\"],\n",
    "                            top_names = [\"relu2\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Dropout,\n",
    "                            bottom_names = [\"relu2\"],\n",
    "                            top_names = [\"dropout2\"],\n",
    "                            dropout_rate=0.5))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"dropout2\"],\n",
    "                            top_names = [\"fc3\"],\n",
    "                            num_output=1))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Add,\n",
    "                            bottom_names = [\"fc3\", \"wide_redn\"],\n",
    "                            top_names = [\"add1\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.BinaryCrossEntropyLoss,\n",
    "                            bottom_names = [\"add1\", \"label\"],\n",
    "                            top_names = [\"loss\"]))\n",
    "model.compile()\n",
    "model.summary()\n",
    "model.fit(max_iter = 21000, display = 1000, eval_interval = 4000, snapshot = 20000, snapshot_prefix = \"/wdl_train/model/wdl/\")\n",
    "model.graph_to_json(graph_config_file = \"/wdl_train/model/wdl.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39d1f362",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MpiInitService: Initialized!\n",
      "HugeCTR Version: 23.4\n",
      "====================================================Model Init=====================================================\n",
      "[HCTR][05:22:57.077][WARNING][RK0][main]: The model name is not specified when creating the solver.\n",
      "[HCTR][05:22:57.077][INFO][RK0][main]: Global seed is 1262996030\n",
      "[HCTR][05:22:57.759][INFO][RK0][main]: Device to NUMA mapping:\n",
      "  GPU 1 ->  node 0\n",
      "[HCTR][05:23:01.750][WARNING][RK0][main]: Peer-to-peer access cannot be fully enabled.\n",
      "[HCTR][05:23:01.750][DEBUG][RK0][main]: [device 1] allocating 0.0000 GB, available 30.6238 \n",
      "[HCTR][05:23:01.750][INFO][RK0][main]: Start all2all warmup\n",
      "[HCTR][05:23:01.751][INFO][RK0][main]: End all2all warmup\n",
      "[HCTR][05:23:01.780][INFO][RK0][main]: Using All-reduce algorithm: NCCL\n",
      "[HCTR][05:23:01.786][INFO][RK0][main]: Device 1: Tesla V100-SXM2-32GB\n",
      "[HCTR][05:23:01.789][INFO][RK0][main]: eval source /wdl_train/val/_file_list.txt max_row_group_size 33421\n",
      "[HCTR][05:23:01.792][INFO][RK0][main]: train source /wdl_train/train/_file_list.txt max_row_group_size 133025\n",
      "[HCTR][05:23:01.793][INFO][RK0][main]: num of DataReader workers for train: 1\n",
      "[HCTR][05:23:01.793][INFO][RK0][main]: num of DataReader workers for eval: 1\n",
      "[HCTR][05:23:01.794][DEBUG][RK0][main]: [device 1] allocating 0.0018 GB, available 30.3679 \n",
      "[HCTR][05:23:01.795][DEBUG][RK0][main]: [device 1] allocating 0.0018 GB, available 30.3621 \n",
      "[HCTR][05:23:01.808][INFO][RK0][main]: Vocabulary size: 2138588\n",
      "[HCTR][05:23:01.808][INFO][RK0][main]: max_vocabulary_size_per_gpu_=6990506\n",
      "[HCTR][05:23:01.812][DEBUG][RK0][main]: [device 1] allocating 0.0788 GB, available 29.8347 \n",
      "[HCTR][05:23:01.812][INFO][RK0][main]: max_vocabulary_size_per_gpu_=7372800\n",
      "[HCTR][05:23:01.816][DEBUG][RK0][main]: [device 1] allocating 1.3516 GB, available 28.3562 \n",
      "[HCTR][05:23:01.817][INFO][RK0][main]: Graph analysis to resolve tensor dependency\n",
      "===================================================Model Compile===================================================\n",
      "[HCTR][05:23:01.821][DEBUG][RK0][main]: [device 1] allocating 0.2162 GB, available 28.1238 \n",
      "[HCTR][05:23:01.821][DEBUG][RK0][main]: [device 1] allocating 0.0056 GB, available 28.1179 \n",
      "[HCTR][05:23:10.289][INFO][RK0][main]: gpu0 start to init embedding\n",
      "[HCTR][05:23:10.289][INFO][RK0][main]: gpu0 init embedding done\n",
      "[HCTR][05:23:10.289][INFO][RK0][main]: gpu0 start to init embedding\n",
      "[HCTR][05:23:10.292][INFO][RK0][main]: gpu0 init embedding done\n",
      "[HCTR][05:23:10.292][DEBUG][RK0][main]: [device 1] allocating 0.0001 GB, available 28.1179 \n",
      "[HCTR][05:23:10.294][INFO][RK0][main]: Starting AUC NCCL warm-up\n",
      "[HCTR][05:23:10.299][INFO][RK0][main]: Warm-up done\n",
      "[HCTR][05:23:10.299][DEBUG][RK0][main]: Nothing to preallocate\n",
      "===================================================Model Summary===================================================\n",
      "[HCTR][05:23:10.299][INFO][RK0][main]: Model structure on each GPU\n",
      "Label                                   Dense                         Sparse                        \n",
      "label                                   dense                          wide_data,deep_data           \n",
      "(2720,1)                                (2720,13)                               \n",
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "Layer Type                              Input Name                    Output Name                   Output Shape                  \n",
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "DistributedSlotSparseEmbeddingHash      wide_data                     sparse_embedding2             (2720,2,1)                    \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "DistributedSlotSparseEmbeddingHash      deep_data                     sparse_embedding1             (2720,26,16)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Reshape                                 sparse_embedding1             reshape1                      (2720,416)                    \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Reshape                                 sparse_embedding2             reshape2                      (2720,2)                      \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "ReduceSum                               reshape2                      wide_redn                     (2720,1)                      \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Concat                                  reshape1                      concat1                       (2720,429)                    \n",
      "                                        dense                                                                                     \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            concat1                       fc1                           (2720,1024)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "ReLU                                    fc1                           relu1                         (2720,1024)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Dropout                                 relu1                         dropout1                      (2720,1024)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            dropout1                      fc2                           (2720,1024)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "ReLU                                    fc2                           relu2                         (2720,1024)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Dropout                                 relu2                         dropout2                      (2720,1024)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            dropout2                      fc3                           (2720,1)                      \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Add                                     fc3                           add1                          (2720,1)                      \n",
      "                                        wide_redn                                                                                 \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "BinaryCrossEntropyLoss                  add1                          loss                                                        \n",
      "                                        label                                                                                     \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "=====================================================Model Fit=====================================================\n",
      "[HCTR][05:23:10.299][INFO][RK0][main]: Use non-epoch mode with number of iterations: 21000\n",
      "[HCTR][05:23:10.299][INFO][RK0][main]: Training batchsize: 2720, evaluation batchsize: 2720\n",
      "[HCTR][05:23:10.299][INFO][RK0][main]: Evaluation interval: 4000, snapshot interval: 20000\n",
      "[HCTR][05:23:10.299][INFO][RK0][main]: Dense network trainable: True\n",
      "[HCTR][05:23:10.299][INFO][RK0][main]: Sparse embedding sparse_embedding1 trainable: True\n",
      "[HCTR][05:23:10.299][INFO][RK0][main]: Sparse embedding sparse_embedding2 trainable: True\n",
      "[HCTR][05:23:10.299][INFO][RK0][main]: Use mixed precision: False, scaler: 1.000000, use cuda graph: True\n",
      "[HCTR][05:23:10.299][INFO][RK0][main]: lr: 0.001000, warmup_steps: 1, end_lr: 0.000000\n",
      "[HCTR][05:23:10.299][INFO][RK0][main]: decay_start: 0, decay_steps: 1, decay_power: 2.000000\n",
      "[HCTR][05:23:10.299][INFO][RK0][main]: Training source file: /wdl_train/train/_file_list.txt\n",
      "[HCTR][05:23:10.299][INFO][RK0][main]: Evaluation source file: /wdl_train/val/_file_list.txt\n",
      "[HCTR][05:23:18.987][INFO][RK0][main]: Iter: 1000 Time(1000 iters): 8.68321s Loss: 0.125823 lr:0.001\n",
      "[HCTR][05:23:27.587][INFO][RK0][main]: Iter: 2000 Time(1000 iters): 8.5956s Loss: 0.11697 lr:0.001\n",
      "[HCTR][05:23:36.175][INFO][RK0][main]: Iter: 3000 Time(1000 iters): 8.58384s Loss: 0.115881 lr:0.001\n",
      "[HCTR][05:23:44.749][INFO][RK0][main]: Iter: 4000 Time(1000 iters): 8.56965s Loss: 0.114301 lr:0.001\n",
      "[HCTR][05:23:49.496][INFO][RK0][main]: Evaluation, AUC: 0.747877\n",
      "[HCTR][05:23:49.496][INFO][RK0][main]: Eval Time for 4000 iters: 4.74613s\n",
      "[HCTR][05:23:58.100][INFO][RK0][main]: Iter: 5000 Time(1000 iters): 13.3459s Loss: 0.126996 lr:0.001\n",
      "[HCTR][05:24:06.702][INFO][RK0][main]: Iter: 6000 Time(1000 iters): 8.59771s Loss: 0.108037 lr:0.001\n",
      "[HCTR][05:24:15.301][INFO][RK0][main]: Iter: 7000 Time(1000 iters): 8.59455s Loss: 0.126929 lr:0.001\n",
      "[HCTR][05:24:23.899][INFO][RK0][main]: Iter: 8000 Time(1000 iters): 8.59391s Loss: 0.105164 lr:0.001\n",
      "[HCTR][05:24:28.506][INFO][RK0][main]: Evaluation, AUC: 0.719409\n",
      "[HCTR][05:24:28.506][INFO][RK0][main]: Eval Time for 4000 iters: 4.60613s\n",
      "[HCTR][05:24:37.102][INFO][RK0][main]: Iter: 9000 Time(1000 iters): 13.1988s Loss: 0.108601 lr:0.001\n",
      "[HCTR][05:24:45.695][INFO][RK0][main]: Iter: 10000 Time(1000 iters): 8.58771s Loss: 0.114181 lr:0.001\n",
      "[HCTR][05:24:54.294][INFO][RK0][main]: Iter: 11000 Time(1000 iters): 8.59515s Loss: 0.102487 lr:0.001\n",
      "[HCTR][05:25:02.895][INFO][RK0][main]: Iter: 12000 Time(1000 iters): 8.59662s Loss: 0.101383 lr:0.001\n",
      "[HCTR][05:25:07.517][INFO][RK0][main]: Evaluation, AUC: 0.696508\n",
      "[HCTR][05:25:07.517][INFO][RK0][main]: Eval Time for 4000 iters: 4.62149s\n",
      "[HCTR][05:25:16.101][INFO][RK0][main]: Iter: 13000 Time(1000 iters): 13.2012s Loss: 0.106047 lr:0.001\n",
      "[HCTR][05:25:24.690][INFO][RK0][main]: Iter: 14000 Time(1000 iters): 8.58473s Loss: 0.114361 lr:0.001\n",
      "[HCTR][05:25:33.283][INFO][RK0][main]: Iter: 15000 Time(1000 iters): 8.58798s Loss: 0.0902672 lr:0.001\n",
      "[HCTR][05:25:41.863][INFO][RK0][main]: Iter: 16000 Time(1000 iters): 8.57613s Loss: 0.0979891 lr:0.001\n",
      "[HCTR][05:25:46.496][INFO][RK0][main]: Evaluation, AUC: 0.686972\n",
      "[HCTR][05:25:46.496][INFO][RK0][main]: Eval Time for 4000 iters: 4.63229s\n",
      "[HCTR][05:25:55.091][INFO][RK0][main]: Iter: 17000 Time(1000 iters): 13.2233s Loss: 0.115308 lr:0.001\n",
      "[HCTR][05:26:03.673][INFO][RK0][main]: Iter: 18000 Time(1000 iters): 8.57796s Loss: 0.0990158 lr:0.001\n",
      "[HCTR][05:26:12.256][INFO][RK0][main]: Iter: 19000 Time(1000 iters): 8.578s Loss: 0.0970965 lr:0.001\n",
      "[HCTR][05:26:20.853][INFO][RK0][main]: Iter: 20000 Time(1000 iters): 8.59287s Loss: 0.0835783 lr:0.001\n",
      "[HCTR][05:26:25.485][INFO][RK0][main]: Evaluation, AUC: 0.662475\n",
      "[HCTR][05:26:25.485][INFO][RK0][main]: Eval Time for 4000 iters: 4.63122s\n",
      "[HCTR][05:26:25.497][INFO][RK0][main]: Rank0: Write hash table to file\n",
      "[HCTR][05:26:25.519][INFO][RK0][main]: Rank0: Write hash table to file\n",
      "[HCTR][05:26:25.537][INFO][RK0][main]: Dumping sparse weights to files, successful\n",
      "[HCTR][05:26:25.550][INFO][RK0][main]: Rank0: Write optimzer state to file\n",
      "[HCTR][05:26:25.563][INFO][RK0][main]: Done\n",
      "[HCTR][05:26:25.578][INFO][RK0][main]: Rank0: Write optimzer state to file\n",
      "[HCTR][05:26:25.591][INFO][RK0][main]: Done\n",
      "[HCTR][05:26:25.856][INFO][RK0][main]: Rank0: Write optimzer state to file\n",
      "[HCTR][05:26:26.071][INFO][RK0][main]: Done\n",
      "[HCTR][05:26:26.348][INFO][RK0][main]: Rank0: Write optimzer state to file\n",
      "[HCTR][05:26:26.561][INFO][RK0][main]: Done\n",
      "[HCTR][05:26:26.576][INFO][RK0][main]: Dumping sparse optimzer states to files, successful\n",
      "[HCTR][05:26:26.579][INFO][RK0][main]: Dumping dense weights to file, successful\n",
      "[HCTR][05:26:26.586][INFO][RK0][main]: Dumping dense optimizer states to file, successful\n",
      "[HCTR][05:26:35.196][INFO][RK0][main]: Iter: 21000 Time(1000 iters): 14.3386s Loss: 0.0844829 lr:0.001\n",
      "[HCTR][05:26:35.196][INFO][RK0][main]: Finish 21000 iterations with batchsize: 2720 in 204.90s.\n",
      "[HCTR][05:26:35.197][INFO][RK0][main]: Save the model graph to /wdl_train/model/wdl.json successfully\n"
     ]
    }
   ],
   "source": [
    "!python ./model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b649a6a9",
   "metadata": {},
   "source": [
    "## Prepare Inference Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "130dd6ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 634936\n",
      "-rw-r--r-- 1 root root       242 May 26 04:31 _file_list.txt\n",
      "-rw-r--r-- 1 root root    217718 May 26 04:31 _metadata\n",
      "-rw-r--r-- 1 root root      1879 May 26 04:31 _metadata.json\n",
      "-rw-r--r-- 1 root root  17489097 May 26 04:31 part_0.parquet\n",
      "-rw-r--r-- 1 root root  17521515 May 26 04:31 part_1.parquet\n",
      "-rw-r--r-- 1 root root  17459606 May 26 04:31 part_2.parquet\n",
      "-rw-r--r-- 1 root root  17556341 May 26 04:31 part_3.parquet\n",
      "-rw-r--r-- 1 root root  17527364 May 26 04:31 part_4.parquet\n",
      "-rw-r--r-- 1 root root  17492305 May 26 04:31 part_5.parquet\n",
      "-rw-r--r-- 1 root root  17508965 May 26 04:31 part_6.parquet\n",
      "-rw-r--r-- 1 root root  17575602 May 26 04:31 part_7.parquet\n",
      "-rw-r--r-- 1 root root     31277 May 26 04:31 schema.pbtxt\n",
      "drwxr-xr-x 2 root root        50 May 26 04:30 temp-parquet-after-conversion\n",
      "-rw-r--r-- 1 root root 509766965 May 26 04:30 test.txt\n"
     ]
    }
   ],
   "source": [
    "!ls -l /wdl_train/val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "449cb610",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>I1</th>\n",
       "      <th>I2</th>\n",
       "      <th>I3</th>\n",
       "      <th>I4</th>\n",
       "      <th>I5</th>\n",
       "      <th>I6</th>\n",
       "      <th>I7</th>\n",
       "      <th>I8</th>\n",
       "      <th>I9</th>\n",
       "      <th>I10</th>\n",
       "      <th>...</th>\n",
       "      <th>C17</th>\n",
       "      <th>C18</th>\n",
       "      <th>C19</th>\n",
       "      <th>C20</th>\n",
       "      <th>C21</th>\n",
       "      <th>C22</th>\n",
       "      <th>C23</th>\n",
       "      <th>C24</th>\n",
       "      <th>C25</th>\n",
       "      <th>C26</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.051831</td>\n",
       "      <td>-0.490904</td>\n",
       "      <td>-0.512615</td>\n",
       "      <td>-0.135830</td>\n",
       "      <td>-0.222800</td>\n",
       "      <td>-0.164725</td>\n",
       "      <td>-0.053983</td>\n",
       "      <td>-0.298238</td>\n",
       "      <td>-0.435927</td>\n",
       "      <td>-0.409435</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>24804</td>\n",
       "      <td>0</td>\n",
       "      <td>14164</td>\n",
       "      <td>2028</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.067326</td>\n",
       "      <td>-0.015481</td>\n",
       "      <td>-0.512615</td>\n",
       "      <td>-0.135830</td>\n",
       "      <td>-0.222800</td>\n",
       "      <td>-0.164725</td>\n",
       "      <td>-0.053983</td>\n",
       "      <td>-0.298238</td>\n",
       "      <td>2.166419</td>\n",
       "      <td>-0.409435</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>813</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.065389</td>\n",
       "      <td>-0.486852</td>\n",
       "      <td>-0.626555</td>\n",
       "      <td>-0.100009</td>\n",
       "      <td>-0.172440</td>\n",
       "      <td>-0.164725</td>\n",
       "      <td>-0.053983</td>\n",
       "      <td>-0.295652</td>\n",
       "      <td>-0.761220</td>\n",
       "      <td>-0.409435</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>890</td>\n",
       "      <td>0</td>\n",
       "      <td>1483</td>\n",
       "      <td>167</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.198029</td>\n",
       "      <td>-0.494956</td>\n",
       "      <td>-0.398675</td>\n",
       "      <td>-0.045562</td>\n",
       "      <td>-0.206014</td>\n",
       "      <td>-0.164725</td>\n",
       "      <td>-0.053983</td>\n",
       "      <td>-0.202561</td>\n",
       "      <td>-0.679897</td>\n",
       "      <td>-0.409435</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>672</td>\n",
       "      <td>466</td>\n",
       "      <td>722</td>\n",
       "      <td>794</td>\n",
       "      <td>2261</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.007282</td>\n",
       "      <td>-0.484151</td>\n",
       "      <td>0.740724</td>\n",
       "      <td>-0.125800</td>\n",
       "      <td>-0.222800</td>\n",
       "      <td>0.398216</td>\n",
       "      <td>-0.053983</td>\n",
       "      <td>-0.298238</td>\n",
       "      <td>0.214659</td>\n",
       "      <td>1.698777</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>338</td>\n",
       "      <td>482</td>\n",
       "      <td>364</td>\n",
       "      <td>102</td>\n",
       "      <td>5759</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         I1        I2        I3        I4        I5        I6        I7  \\\n",
       "0 -0.051831 -0.490904 -0.512615 -0.135830 -0.222800 -0.164725 -0.053983   \n",
       "1 -0.067326 -0.015481 -0.512615 -0.135830 -0.222800 -0.164725 -0.053983   \n",
       "2 -0.065389 -0.486852 -0.626555 -0.100009 -0.172440 -0.164725 -0.053983   \n",
       "3  0.198029 -0.494956 -0.398675 -0.045562 -0.206014 -0.164725 -0.053983   \n",
       "4 -0.007282 -0.484151  0.740724 -0.125800 -0.222800  0.398216 -0.053983   \n",
       "\n",
       "         I8        I9       I10  ...  C17  C18  C19  C20    C21  C22    C23  \\\n",
       "0 -0.298238 -0.435927 -0.409435  ...    2    2    4    0  24804    0  14164   \n",
       "1 -0.298238  2.166419 -0.409435  ...    0   27    7    2      2    2      0   \n",
       "2 -0.295652 -0.761220 -0.409435  ...    2    3    1    0    890    0   1483   \n",
       "3 -0.202561 -0.679897 -0.409435  ...    3    1    1  672    466  722    794   \n",
       "4 -0.298238  0.214659  1.698777  ...    1    1    1  338    482  364    102   \n",
       "\n",
       "    C24  C25  C26  \n",
       "0  2028    1    5  \n",
       "1   813    2    1  \n",
       "2   167    5    9  \n",
       "3  2261    5    1  \n",
       "4  5759    1    1  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet(\"/wdl_train/val/part_0.parquet\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "85069108",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head(10).to_csv('/wdl_train/infer_test.csv', sep=',', index=False,header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f219ba",
   "metadata": {},
   "source": [
    "## Create prediction scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e98d68e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /wdl_train/wdl_predict.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile '/wdl_train/wdl_predict.py'\n",
    "from hugectr.inference import InferenceParams, CreateInferenceSession\n",
    "import hugectr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from mpi4py import MPI\n",
    "def wdl_inference(model_name, network_file, dense_file, embedding_file_list, data_file, enable_cache, use_rocksdb=False, rocksdb_path=None):\n",
    "    CATEGORICAL_COLUMNS=[\"C\" + str(x) for x in range(1, 27)]+[\"C1_C2\",\"C3_C4\"]\n",
    "    CONTINUOUS_COLUMNS=[\"I\" + str(x) for x in range(1, 14)]\n",
    "    LABEL_COLUMNS = ['label']\n",
    "    emb_size = [249058, 19561, 14212, 6890, 18592, 4, 6356, 1254, 52, 226170, 80508, 72308, 11, 2169, 7597, 61, 4, 923, 15, 249619, 168974, 243480, 68212, 9169, 75, 34, 278018, 415262]\n",
    "    shift = np.insert(np.cumsum(emb_size), 0, 0)[:-1]\n",
    "    test_df=pd.read_csv(data_file,sep=',')\n",
    "    config_file = network_file\n",
    "    row_ptrs = list(range(0,21))+list(range(0,261))\n",
    "    dense_features =  list(test_df[CONTINUOUS_COLUMNS].values.flatten())\n",
    "    test_df[CATEGORICAL_COLUMNS].astype(np.int64)\n",
    "    embedding_columns = list((test_df[CATEGORICAL_COLUMNS]+shift).values.flatten())\n",
    "    \n",
    "    \n",
    "    persistent_db_params = hugectr.inference.PersistentDatabaseParams()\n",
    "    if use_rocksdb:\n",
    "        persistent_db_params = hugectr.inference.PersistentDatabaseParams(\n",
    "                                  backend = hugectr.DatabaseType_t.rocks_db,\n",
    "                                  path = rocksdb_path\n",
    "                                )\n",
    "    \n",
    "\n",
    "    # create parameter server, embedding cache and inference session\n",
    "    inference_params = InferenceParams(model_name = model_name,\n",
    "                                max_batchsize = 64,\n",
    "                                hit_rate_threshold = 0.5,\n",
    "                                dense_model_file = dense_file,\n",
    "                                sparse_model_files = embedding_file_list,\n",
    "                                device_id = 0,\n",
    "                                use_gpu_embedding_cache = enable_cache,\n",
    "                                cache_size_percentage = 0.9,\n",
    "                                persistent_db = persistent_db_params,\n",
    "                                i64_input_key = True,\n",
    "                                use_mixed_precision = False)\n",
    "    inference_session = CreateInferenceSession(config_file, inference_params)\n",
    "    output = inference_session.predict(dense_features, embedding_columns, row_ptrs)\n",
    "    print(\"WDL multi-embedding table inference result is {}\".format(output))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_name = sys.argv[1]\n",
    "    print(\"{} multi-embedding table prediction\".format(model_name))\n",
    "    network_file = sys.argv[2]\n",
    "    print(\"{} multi-embedding table prediction network is {}\".format(model_name,network_file))\n",
    "    dense_file = sys.argv[3]\n",
    "    print(\"{} multi-embedding table prediction dense file is {}\".format(model_name,dense_file))\n",
    "    embedding_file_list = str(sys.argv[4]).split(',')\n",
    "    print(\"{} multi-embedding table prediction sparse files are {}\".format(model_name,embedding_file_list))\n",
    "    data_file = sys.argv[5]\n",
    "    print(\"{} multi-embedding table prediction input data path is {}\".format(model_name,data_file))\n",
    "    input_dbtype = sys.argv[6]\n",
    "    print(\"{} multi-embedding table prediction input dbtype path is {}\".format(model_name,input_dbtype))\n",
    "    if input_dbtype==\"disabled\":\n",
    "        wdl_inference(model_name, network_file, dense_file, embedding_file_list, data_file, True)\n",
    "    if input_dbtype==\"rocksdb\":\n",
    "        rocksdb_path = sys.argv[7]\n",
    "        print(\"{} multi-embedding table prediction rocksdb_path path is {}\".format(model_name,rocksdb_path))\n",
    "        wdl_inference(model_name, network_file, dense_file, embedding_file_list, data_file, True, True, rocksdb_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58bdd0f",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "Use different types of databases as a local parameter server to get the wide and deep model prediction results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478b1f54",
   "metadata": {},
   "source": [
    "### Load model embedding tables into local memory as parameter server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "604cb6f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wdl multi-embedding table prediction\n",
      "wdl multi-embedding table prediction network is /wdl_train/model/wdl.json\n",
      "wdl multi-embedding table prediction dense file is /wdl_train/model/wdl/_dense_20000.model\n",
      "wdl multi-embedding table prediction sparse files are ['/wdl_train/model/wdl/0_sparse_20000.model/', '/wdl_train/model/wdl/1_sparse_20000.model']\n",
      "wdl multi-embedding table prediction input data path is /wdl_train/infer_test.csv\n",
      "wdl multi-embedding table prediction input dbtype path is disabled\n",
      "MpiInitService: MPI was already initialized by another (non-HugeCTR) mechanism.\n",
      "[HCTR][05:28:49.476][WARNING][RK0][main]: default_value_for_each_table.size() is not equal to the number of embedding tables\n",
      "[HCTR][05:28:49.476][INFO][RK0][main]: default_emb_vec_value is not specified using default: 0\n",
      "[HCTR][05:28:49.476][INFO][RK0][main]: default_emb_vec_value is not specified using default: 0\n",
      "====================================================HPS Create====================================================\n",
      "[HCTR][05:28:49.476][INFO][RK0][main]: Creating HashMap CPU database backend...\n",
      "[HCTR][05:28:49.476][DEBUG][RK0][main]: Created blank database backend in local memory!\n",
      "[HCTR][05:28:49.476][INFO][RK0][main]: Volatile DB: initial cache rate = 1\n",
      "[HCTR][05:28:49.476][INFO][RK0][main]: Volatile DB: cache missed embeddings = 0\n",
      "[HCTR][05:28:49.476][DEBUG][RK0][main]: Created raw model loader in local memory!\n",
      "[HCTR][05:28:49.675][INFO][RK0][main]: Table: hps_et.wdl.sparse_embedding2; cached 190851 / 190851 embeddings in volatile database (HashMapBackend); load: 190851 / 18446744073709551615 (0.00%).\n",
      "[HCTR][05:28:49.881][INFO][RK0][main]: Table: hps_et.wdl.sparse_embedding1; cached 438628 / 438628 embeddings in volatile database (HashMapBackend); load: 438628 / 18446744073709551615 (0.00%).\n",
      "[HCTR][05:28:49.881][DEBUG][RK0][main]: Real-time subscribers created!\n",
      "[HCTR][05:28:49.881][INFO][RK0][main]: Creating embedding cache in device 0.\n",
      "[HCTR][05:28:49.886][INFO][RK0][main]: Model name: wdl\n",
      "[HCTR][05:28:49.886][INFO][RK0][main]: Max batch size: 64\n",
      "[HCTR][05:28:49.886][INFO][RK0][main]: Fuse embedding tables: False\n",
      "[HCTR][05:28:49.886][INFO][RK0][main]: Number of embedding tables: 2\n",
      "[HCTR][05:28:49.886][INFO][RK0][main]: Use GPU embedding cache: True, cache size percentage: 0.900000\n",
      "[HCTR][05:28:49.886][INFO][RK0][main]: Embedding cache type: dynamic\n",
      "[HCTR][05:28:49.886][INFO][RK0][main]: Use I64 input key: True\n",
      "[HCTR][05:28:49.886][INFO][RK0][main]: Configured cache hit rate threshold: 0.500000\n",
      "[HCTR][05:28:49.886][INFO][RK0][main]: The size of thread pool: 80\n",
      "[HCTR][05:28:49.886][INFO][RK0][main]: The size of worker memory pool: 2\n",
      "[HCTR][05:28:49.886][INFO][RK0][main]: The size of refresh memory pool: 1\n",
      "[HCTR][05:28:49.886][INFO][RK0][main]: The refresh percentage : 0.000000\n",
      "[HCTR][05:28:51.447][INFO][RK0][main]: Global seed is 3595974557\n",
      "[HCTR][05:28:51.531][INFO][RK0][main]: Device to NUMA mapping:\n",
      "  GPU 0 ->  node 0\n",
      "[HCTR][05:28:53.402][WARNING][RK0][main]: Peer-to-peer access cannot be fully enabled.\n",
      "[HCTR][05:28:53.402][DEBUG][RK0][main]: [device 0] allocating 0.0000 GB, available 30.5847 \n",
      "[HCTR][05:28:53.402][INFO][RK0][main]: Start all2all warmup\n",
      "[HCTR][05:28:53.402][INFO][RK0][main]: End all2all warmup\n",
      "[HCTR][05:28:53.403][INFO][RK0][main]: Model name: wdl\n",
      "[HCTR][05:28:53.403][INFO][RK0][main]: Use mixed precision: False\n",
      "[HCTR][05:28:53.403][INFO][RK0][main]: Use cuda graph: True\n",
      "[HCTR][05:28:53.403][INFO][RK0][main]: Max batchsize: 64\n",
      "[HCTR][05:28:53.403][INFO][RK0][main]: Use I64 input key: True\n",
      "[HCTR][05:28:53.403][INFO][RK0][main]: start create embedding for inference\n",
      "[HCTR][05:28:53.403][INFO][RK0][main]: sparse_input name wide_data\n",
      "[HCTR][05:28:53.403][INFO][RK0][main]: sparse_input name deep_data\n",
      "[HCTR][05:28:53.403][INFO][RK0][main]: create embedding for inference success\n",
      "[HCTR][05:28:53.403][DEBUG][RK0][main]: [device 0] allocating 0.0001 GB, available 30.3347 \n",
      "[HCTR][05:28:53.404][INFO][RK0][main]: Inference stage skip BinaryCrossEntropyLoss layer, replaced by Sigmoid layer\n",
      "[HCTR][05:28:53.404][DEBUG][RK0][main]: [device 0] allocating 0.0128 GB, available 30.3132 \n",
      "[HCTR][05:28:53.985][WARNING][RK0][main]: InferenceSession is not suitable for multi-GPU offline inference. Please use InferenceModel: https://nvidia-merlin.github.io/HugeCTR/main/api/python_interface.html#inferencemodel\n",
      "WDL multi-embedding table inference result is [0.09066542983055115, 0.26852551102638245, 0.28295737504959106, 0.07364904880523682, 0.2965098023414612, 0.10407719761133194, 0.4754742681980133, 0.5024058818817139, 0.05602413788437843, 0.07264009118080139]\n"
     ]
    }
   ],
   "source": [
    "!python /wdl_train/wdl_predict.py \"wdl\" \"/wdl_train/model/wdl.json\" \"/wdl_train/model/wdl/_dense_20000.model\" \"/wdl_train/model/wdl/0_sparse_20000.model/,/wdl_train/model/wdl/1_sparse_20000.model\" \"/wdl_train/infer_test.csv\" \"disabled\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7284260e",
   "metadata": {},
   "source": [
    "### Load model embedding tables into local RocksDB as a parameter Server\n",
    "\n",
    "Create a RocksDB directory with read and write permissions for storing model embedded tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b9f750d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p -m 700 /wdl_train/rocksdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c59e4b05",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wdl multi-embedding table prediction\n",
      "wdl multi-embedding table prediction network is /wdl_train/model/wdl.json\n",
      "wdl multi-embedding table prediction dense file is /wdl_train/model/wdl/_dense_20000.model\n",
      "wdl multi-embedding table prediction sparse files are ['/wdl_train/model/wdl/0_sparse_20000.model/', '/wdl_train/model/wdl/1_sparse_20000.model']\n",
      "wdl multi-embedding table prediction input data path is /wdl_train/infer_test.csv\n",
      "wdl multi-embedding table prediction input dbtype path is rocksdb\n",
      "wdl multi-embedding table prediction rocksdb_path path is /wdl_train/rocksdb\n",
      "MpiInitService: MPI was already initialized by another (non-HugeCTR) mechanism.\n",
      "[HCTR][05:29:24.931][WARNING][RK0][main]: default_value_for_each_table.size() is not equal to the number of embedding tables\n",
      "[HCTR][05:29:24.932][INFO][RK0][main]: default_emb_vec_value is not specified using default: 0\n",
      "[HCTR][05:29:24.932][INFO][RK0][main]: default_emb_vec_value is not specified using default: 0\n",
      "====================================================HPS Create====================================================\n",
      "[HCTR][05:29:24.932][INFO][RK0][main]: Creating HashMap CPU database backend...\n",
      "[HCTR][05:29:24.932][DEBUG][RK0][main]: Created blank database backend in local memory!\n",
      "[HCTR][05:29:24.932][INFO][RK0][main]: Volatile DB: initial cache rate = 1\n",
      "[HCTR][05:29:24.932][INFO][RK0][main]: Volatile DB: cache missed embeddings = 0\n",
      "[HCTR][05:29:24.932][INFO][RK0][main]: Creating RocksDB backend...\n",
      "[HCTR][05:29:24.932][INFO][RK0][main]: Connecting to RocksDB database...\n",
      "[HCTR][05:29:24.934][INFO][RK0][main]: RocksDB /wdl_train/rocksdb, found column family `default`.\n",
      "[HCTR][05:29:24.962][INFO][RK0][main]: Connected to RocksDB database!\n",
      "[HCTR][05:29:24.962][DEBUG][RK0][main]: Created raw model loader in local memory!\n",
      "[HCTR][05:29:25.190][INFO][RK0][main]: Table: hps_et.wdl.sparse_embedding2; cached 190851 / 190851 embeddings in volatile database (HashMapBackend); load: 190851 / 18446744073709551615 (0.00%).\n",
      "[HCTR][05:29:25.734][INFO][RK0][main]: Table: hps_et.wdl.sparse_embedding1; cached 438628 / 438628 embeddings in volatile database (HashMapBackend); load: 438628 / 18446744073709551615 (0.00%).\n",
      "[HCTR][05:29:26.579][DEBUG][RK0][main]: Real-time subscribers created!\n",
      "[HCTR][05:29:26.579][INFO][RK0][main]: Creating embedding cache in device 0.\n",
      "[HCTR][05:29:26.584][INFO][RK0][main]: Model name: wdl\n",
      "[HCTR][05:29:26.584][INFO][RK0][main]: Max batch size: 64\n",
      "[HCTR][05:29:26.584][INFO][RK0][main]: Fuse embedding tables: False\n",
      "[HCTR][05:29:26.584][INFO][RK0][main]: Number of embedding tables: 2\n",
      "[HCTR][05:29:26.584][INFO][RK0][main]: Use GPU embedding cache: True, cache size percentage: 0.900000\n",
      "[HCTR][05:29:26.584][INFO][RK0][main]: Embedding cache type: dynamic\n",
      "[HCTR][05:29:26.584][INFO][RK0][main]: Use I64 input key: True\n",
      "[HCTR][05:29:26.584][INFO][RK0][main]: Configured cache hit rate threshold: 0.500000\n",
      "[HCTR][05:29:26.584][INFO][RK0][main]: The size of thread pool: 80\n",
      "[HCTR][05:29:26.584][INFO][RK0][main]: The size of worker memory pool: 2\n",
      "[HCTR][05:29:26.584][INFO][RK0][main]: The size of refresh memory pool: 1\n",
      "[HCTR][05:29:26.584][INFO][RK0][main]: The refresh percentage : 0.000000\n",
      "[HCTR][05:29:28.096][INFO][RK0][main]: Global seed is 1275207064\n",
      "[HCTR][05:29:28.175][INFO][RK0][main]: Device to NUMA mapping:\n",
      "  GPU 0 ->  node 0\n",
      "[HCTR][05:29:30.024][WARNING][RK0][main]: Peer-to-peer access cannot be fully enabled.\n",
      "[HCTR][05:29:30.024][DEBUG][RK0][main]: [device 0] allocating 0.0000 GB, available 30.5847 \n",
      "[HCTR][05:29:30.024][INFO][RK0][main]: Start all2all warmup\n",
      "[HCTR][05:29:30.024][INFO][RK0][main]: End all2all warmup\n",
      "[HCTR][05:29:30.025][INFO][RK0][main]: Model name: wdl\n",
      "[HCTR][05:29:30.025][INFO][RK0][main]: Use mixed precision: False\n",
      "[HCTR][05:29:30.025][INFO][RK0][main]: Use cuda graph: True\n",
      "[HCTR][05:29:30.025][INFO][RK0][main]: Max batchsize: 64\n",
      "[HCTR][05:29:30.025][INFO][RK0][main]: Use I64 input key: True\n",
      "[HCTR][05:29:30.025][INFO][RK0][main]: start create embedding for inference\n",
      "[HCTR][05:29:30.025][INFO][RK0][main]: sparse_input name wide_data\n",
      "[HCTR][05:29:30.025][INFO][RK0][main]: sparse_input name deep_data\n",
      "[HCTR][05:29:30.025][INFO][RK0][main]: create embedding for inference success\n",
      "[HCTR][05:29:30.025][DEBUG][RK0][main]: [device 0] allocating 0.0001 GB, available 30.3347 \n",
      "[HCTR][05:29:30.026][INFO][RK0][main]: Inference stage skip BinaryCrossEntropyLoss layer, replaced by Sigmoid layer\n",
      "[HCTR][05:29:30.026][DEBUG][RK0][main]: [device 0] allocating 0.0128 GB, available 30.3132 \n",
      "[HCTR][05:29:30.573][WARNING][RK0][main]: InferenceSession is not suitable for multi-GPU offline inference. Please use InferenceModel: https://nvidia-merlin.github.io/HugeCTR/main/api/python_interface.html#inferencemodel\n",
      "WDL multi-embedding table inference result is [0.09066542983055115, 0.26852551102638245, 0.28295737504959106, 0.07364904880523682, 0.2965098023414612, 0.10407719761133194, 0.4754742681980133, 0.5024058818817139, 0.05602413788437843, 0.07264009118080139]\n",
      "[HCTR][05:29:30.576][INFO][RK0][main]: Disconnecting from RocksDB database...\n",
      "[HCTR][05:29:30.578][INFO][RK0][main]: Disconnected from RocksDB database!\n"
     ]
    }
   ],
   "source": [
    "!python /wdl_train/wdl_predict.py \"wdl\" \"/wdl_train/model/wdl.json\" \\\n",
    "\"/wdl_train/model/wdl/_dense_20000.model\" \\\n",
    "\"/wdl_train/model/wdl/0_sparse_20000.model/,/wdl_train/model/wdl/1_sparse_20000.model\" \\\n",
    "\"/wdl_train/infer_test.csv\" \\\n",
    "\"rocksdb\"  \"/wdl_train/rocksdb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b4d2db-3098-463f-a895-01479cf2e93e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}