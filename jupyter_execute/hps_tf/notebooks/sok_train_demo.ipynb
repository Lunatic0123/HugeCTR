{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdfec37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "# Each user is responsible for checking the content of datasets and the\n",
    "# applicable licenses and determining if suitable for the intended use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4337359",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/notebooks/dlsw-notebooks/merlin_hugectr_hps-sok-to-dlrm-demo/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# SOK Train DLRM Demo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "80ac179f",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook demonstrates how to train a DLRM model with SparseOperationKit (SOK) and then make inference with HierarchicalParameterServer(HPS). It is recommended to run [sparse_operation_kit_demo.ipynb](https://github.com/NVIDIA-Merlin/HugeCTR/blob/master/sparse_operation_kit/notebooks/sparse_operation_kit_demo.ipynb) and [hierarchical_parameter_server_demo.ipynb](hierarchical_parameter_server_demo.ipynb) before diving into this notebook.\n",
    "\n",
    "For more details about SOK, please refer to [SOK Documentation](https://nvidia-merlin.github.io/HugeCTR/sparse_operation_kit/master/index.html). For more details about HPS APIs, please refer to [HPS APIs](https://nvidia-merlin.github.io/HugeCTR/main/hierarchical_parameter_server/api/index.html). For more details about HPS, please refer to [HugeCTR Hierarchical Parameter Server (HPS)](https://nvidia-merlin.github.io/HugeCTR/main/hierarchical_parameter_server/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202109ad",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "### Get SOK from NGC\n",
    "\n",
    "Both SOK and HPS Python modules are preinstalled in the 24.06 and later [Merlin HugeCTR Container](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/merlin/containers/merlin-hugectr): `nvcr.io/nvidia/merlin/merlin-hugectr:24.06`.\n",
    "\n",
    "You can check the existence of the required libraries by running the following Python code after launching this container.\n",
    "\n",
    "```bash\n",
    "$ python3 -c \"import sparse_operation_kit as sok\"\n",
    "$ python3 -c \"import hierarchical_parameter_server as hps\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c936f7",
   "metadata": {},
   "source": [
    "## Configurations\n",
    "\n",
    "First of all we specify the required configurations, e.g., the arguments needed for generating the dataset, the model parameters and the paths to save the model. We will use DLRM model which has one embedding table, bottom MLP layers, interaction layer and top MLP layers. Please note that the input to the embedding layer will be a sparse key tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039e7d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import horovod.tensorflow as hvd\n",
    "import sparse_operation_kit as sok\n",
    "import struct\n",
    "\n",
    "args = dict()\n",
    "\n",
    "args[\"gpu_num\"] = 2                               # the number of available GPUs\n",
    "args[\"iter_num\"] = 10                             # the number of training iteration\n",
    "args[\"slot_num\"] = 26                             # the number of feature fields in this embedding layer\n",
    "args[\"embed_vec_sizes\"] = [16]*args[\"slot_num\"]                       # the dimension of embedding vectors\n",
    "args[\"dense_dim\"] = 13                            # the dimension of dense features\n",
    "args[\"global_batch_size\"] = 1024                  # the globally batchsize for all GPUs\n",
    "args[\"local_batch_size\"] = int(args[\"global_batch_size\"]/args[\"gpu_num\"])                  # the locally batchsize for all GPUs\n",
    "args[\"table_names\"] = [\"table\"+str(i) for i in range(args[\"slot_num\"])]                            # embedding table names\n",
    "args[\"max_vocabulary_sizes\"] = np.random.randint(1000, 1200, size=args[\"slot_num\"]).tolist()\n",
    "args[\"max_nnz\"] = np.random.randint(1, 100, size=args[\"slot_num\"])\n",
    "args[\"combiner\"] = [\"mean\"]*args[\"slot_num\"]\n",
    "args[\"sok_backend_type\"] = \"hybrid\"               # selcet sok backend type , hybrid means use HKV, hbm means use DET \n",
    "\n",
    "args[\"ps_config_file\"] = \"dlrm.json\"\n",
    "args[\"dense_model_path\"] = \"dlrm_dense.model\"\n",
    "args[\"sparse_model_path\"] = \"dlrm_sparse.model\"\n",
    "args[\"sok_embedding_table_path\"] = \"sok_dlrm_sparse.model\"\n",
    "args[\"saved_path\"] = \"dlrm_tf_saved_model\"\n",
    "args[\"np_key_type\"] = np.int64\n",
    "args[\"np_vector_type\"] = np.float32\n",
    "args[\"tf_key_type\"] = tf.int64\n",
    "args[\"tf_vector_type\"] = tf.float32\n",
    "\n",
    "hvd.init()\n",
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "if gpus:\n",
    "    tf.config.experimental.set_visible_devices(gpus[hvd.local_rank()], \"GPU\")\n",
    "sok.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c74eff6-6a27-4b23-a3eb-9559d0685ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_samples(batch_size,iters, vocabulary_range_per_slot, max_nnz, dense_dim):\n",
    "    num_samples = batch_size*iters\n",
    "\n",
    "    def generate_ragged_tensor_samples(embedding_table_sizes,batch_size, lookup_num, hotness, iters):\n",
    "\n",
    "        if len(hotness) != lookup_num:\n",
    "            raise ValueError(\"Length of hotness list must be equal to lookup_num\")\n",
    "        total_indices = []\n",
    "        for i in range(lookup_num):\n",
    "            offsets = np.random.randint(1, hotness[i] + 1, iters * batch_size)\n",
    "            offsets = tf.convert_to_tensor(offsets, dtype=tf.int64)\n",
    "            values = np.random.randint(0, embedding_table_sizes[i], tf.reduce_sum(offsets))\n",
    "            values = tf.convert_to_tensor(values, dtype=tf.int64)\n",
    "            total_indices.append(tf.RaggedTensor.from_row_lengths(values, offsets))\n",
    "        return total_indices\n",
    "\n",
    "    sparse_keys = generate_ragged_tensor_samples(vocabulary_range_per_slot,batch_size,len(vocabulary_range_per_slot),max_nnz,iters)\n",
    "    dense_features = np.random.random((num_samples, dense_dim)).astype(np.float32)\n",
    "    labels = np.random.randint(low=0, high=2, size=(num_samples, 1))\n",
    "    return sparse_keys, dense_features, labels\n",
    "\n",
    "def tf_dataset(sparse_keys, dense_features, labels, batchsize):\n",
    "    total_data = []\n",
    "    total_data.extend(sparse_keys)\n",
    "    total_data.append(dense_features)\n",
    "    total_data.append(labels)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(tuple(total_data))\n",
    "    dataset = dataset.batch(batchsize, drop_remainder=True)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe74fa65",
   "metadata": {},
   "source": [
    "## Build model with SOK embedding layers\n",
    "\n",
    "We define the model graph for training with SOK embedding variables, i.e., `sok.DynamicVariable` and lookup sparse values use `sok.lookup_sparse`ï¼Œe can then train the model and save the trained weights of the embedding table into file system. As for the dense layers, they are saved as a separate model graph, which can be loaded directly during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1f685fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(tf.keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "                arch,\n",
    "                activation='relu',\n",
    "                out_activation=None,\n",
    "                **kwargs):\n",
    "        super(MLP, self).__init__(**kwargs)\n",
    "        self.layers = []\n",
    "        index = 0\n",
    "        for units in arch[:-1]:\n",
    "            self.layers.append(tf.keras.layers.Dense(units, activation=activation, name=\"{}_{}\".format(kwargs['name'], index)))\n",
    "            index+=1\n",
    "        self.layers.append(tf.keras.layers.Dense(arch[-1], activation=out_activation, name=\"{}_{}\".format(kwargs['name'], index)))\n",
    "\n",
    "\n",
    "    def call(self, inputs, training=True):\n",
    "        x = self.layers[0](inputs)\n",
    "        for layer in self.layers[1:]:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "class SecondOrderFeatureInteraction(tf.keras.layers.Layer):\n",
    "    def __init__(self, self_interaction=False):\n",
    "        super(SecondOrderFeatureInteraction, self).__init__()\n",
    "        self.self_interaction = self_interaction\n",
    "\n",
    "    def call(self, inputs):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        num_feas =  tf.shape(inputs)[1] \n",
    "\n",
    "        dot_products = tf.matmul(inputs, inputs, transpose_b=True)\n",
    "\n",
    "        ones = tf.ones_like(dot_products)\n",
    "        mask = tf.linalg.band_part(ones, 0, -1)\n",
    "        out_dim = num_feas * (num_feas + 1) // 2\n",
    "\n",
    "        if not self.self_interaction:\n",
    "            mask = mask - tf.linalg.band_part(ones, 0, 0)\n",
    "            out_dim = num_feas * (num_feas - 1) // 2\n",
    "        flat_interactions = tf.reshape(tf.boolean_mask(dot_products, mask), (batch_size, out_dim))\n",
    "        return flat_interactions\n",
    "\n",
    "class SokEmbLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self,embedding_dims,embedding_table_sizes,var_type,combiners,table_names,name):\n",
    "        super(SokEmbLayer, self).__init__(name=name)\n",
    "        self.table_num = len(embedding_dims)\n",
    "        self.combiners = combiners\n",
    "        self.initializers = [\"uniform\"]*self.table_num\n",
    "\n",
    "        self.sok_vars = [sok.DynamicVariable(\n",
    "            dimension=embedding_dims[i],\n",
    "            var_type=var_type,\n",
    "            initializer=self.initializers[i],\n",
    "            init_capacity=embedding_table_sizes[i],\n",
    "            max_capacity=embedding_table_sizes[i],\n",
    "            name = table_names[i]\n",
    "        )\n",
    "        for i in range(self.table_num)\n",
    "        ]\n",
    "        self.reshape_layer_list = []\n",
    "        for i in range(self.table_num):\n",
    "            self.reshape_layer_list.append(tf.keras.layers.Reshape((1, args[\"embed_vec_sizes\"][i]), name = \"sok_reshape\"+str(i)))\n",
    "        self.concat1 = tf.keras.layers.Concatenate(axis=1, name = \"sok_concat1\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        embeddings = sok.lookup_sparse(self.sok_vars, inputs, combiners=self.combiners)\n",
    "        ret_embeddings = []\n",
    "        for i in range(args[\"slot_num\"]):\n",
    "            ret_embeddings.append(self.reshape_layer_list[i](embeddings[i]))\n",
    "        ret_embeddings = self.concat1(ret_embeddings)\n",
    "        return ret_embeddings\n",
    "\n",
    "class DLRM(tf.keras.models.Model):\n",
    "    def __init__(self,\n",
    "                 combiners,\n",
    "                 embedding_table_sizes,\n",
    "                 embed_vec_dims,\n",
    "                 sok_backend_type,\n",
    "                 slot_num,\n",
    "                 dense_dim,\n",
    "                 arch_bot,\n",
    "                 arch_top,\n",
    "                 self_interaction,\n",
    "                 table_names,\n",
    "                 **kwargs):\n",
    "        super(DLRM, self).__init__(**kwargs)\n",
    "\n",
    "        self.combiners = combiners\n",
    "        self.embed_vec_dims = embed_vec_dims\n",
    "        self.sok_backend_type = sok_backend_type\n",
    "        self.embedding_table_sizes = embedding_table_sizes\n",
    "        self.slot_num = len(combiners)\n",
    "        self.dense_dim = dense_dim\n",
    "\n",
    "        self.embedding_model = SokEmbLayer(embedding_dims=self.embed_vec_dims,\n",
    "                                         embedding_table_sizes = self.embedding_table_sizes,\n",
    "                                         var_type = self.sok_backend_type,combiners=combiners,table_names = table_names,name=\"sok_embedding\")\n",
    "\n",
    "        self.bot_nn = MLP(arch_bot, name = \"bottom\", out_activation='relu')\n",
    "        self.top_nn = MLP(arch_top, name = \"top\", out_activation='sigmoid')\n",
    "        self.interaction_op = SecondOrderFeatureInteraction(self_interaction)\n",
    "        if self_interaction:\n",
    "            self.interaction_out_dim = (self.slot_num+1) * (self.slot_num+2) // 2\n",
    "        else:\n",
    "           self.interaction_out_dim = self.slot_num * (self.slot_num+1) // 2\n",
    "\n",
    "        self.reshape_layer1 = tf.keras.layers.Reshape((1, arch_bot[-1]), name = \"dense_reshape1\")\n",
    "        self.concat1 = tf.keras.layers.Concatenate(axis=1, name = \"dense_concat1\")\n",
    "        self.concat2 = tf.keras.layers.Concatenate(axis=1, name = \"dense_concat2\")\n",
    "\n",
    "    def call(self, inputs, training=True):\n",
    "        input_sparse = inputs[0]\n",
    "        input_dense = inputs[1]\n",
    "\n",
    "        embedding_vectors = self.embedding_model(input_sparse)\n",
    "        dense_x = self.bot_nn(input_dense)\n",
    "        concat_features = self.concat1([embedding_vectors, self.reshape_layer1(dense_x)])\n",
    "        Z = self.interaction_op(embedding_vectors)\n",
    "        z = self.concat2([dense_x, Z])\n",
    "        logit = self.top_nn(z)\n",
    "\n",
    "        return logit, embedding_vectors\n",
    "\n",
    "    def summary(self):\n",
    "        sparse_inputs = []\n",
    "        for i in range(self.slot_num):\n",
    "            sparse_inputs.append(tf.keras.Input(shape=(args[\"max_nnz\"][i], ), sparse=True, dtype=args[\"tf_key_type\"])) \n",
    "        dense_input = tf.keras.Input(shape=(self.dense_dim, ), dtype=tf.float32)\n",
    "        inputs = [sparse_inputs,dense_input]\n",
    "        model = tf.keras.models.Model(inputs=inputs, outputs=self.call(inputs))\n",
    "        return model.summary()\n",
    "\n",
    "    def get_embedding_model(self):\n",
    "        return self.embedding_model\n",
    "\n",
    "    def get_embedding_variables(self):\n",
    "        return self.embedding_model.trainable_variables\n",
    "\n",
    "    def get_dense_variables(self):\n",
    "        tmp_var = self.trainable_variables\n",
    "        sparse_vars , dense_vars = sok.filter_variables(tmp_var)\n",
    "        return dense_vars\n",
    "\n",
    "\n",
    "    def embedding_load(self,path,opt):\n",
    "        embedding_vars = self.get_embedding_variables()\n",
    "        sok.load(path, embedding_vars, opt)\n",
    "\n",
    "    def embedding_dump(self,path,opt):\n",
    "        embedding_vars = self.get_embedding_variables()\n",
    "        sok.dump(path, embedding_vars, opt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79673a06-0db7-4a41-bfe9-20e7160a3f6b",
   "metadata": {},
   "source": [
    "## Train with SOK models\n",
    "\n",
    "Define a Trainer class to wrap the training of SOK. When training SOK, the following points need to be noted:\r",
    "1.Two gradient tapes need to be defined because the dense variables may need to be wrapped with Horovod's hvd.DistributedGradientTape.\n",
    "2.\n",
    "SOK variables need to be updated using SOK's optimizer, while the dense variables need to be updated using TensorFlow's optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4b516ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "   def __init__(self,args):\n",
    "       self.args = args\n",
    "       self.dlrm = DLRM(combiners = args[\"combiner\"],\n",
    "                   embedding_table_sizes = args[\"max_vocabulary_sizes\"],\n",
    "                   embed_vec_dims = args[\"embed_vec_sizes\"],\n",
    "                   sok_backend_type = args[\"sok_backend_type\"],\n",
    "                   slot_num = args[\"slot_num\"],\n",
    "                   dense_dim = args[\"dense_dim\"],\n",
    "                   arch_bot = [256, 128, args[\"embed_vec_sizes\"][0]],\n",
    "                   arch_top = [256, 128, 1],\n",
    "                   self_interaction = False,\n",
    "                   table_names = args[\"table_names\"])\n",
    "\n",
    "       # initialize optimizer\n",
    "       optimizer = tf.optimizers.Adam(learning_rate=1.0)\n",
    "       self.embedding_opt = sok.OptimizerWrapper(optimizer)\n",
    "       self.dense_opt = tf.optimizers.Adam(learning_rate=1.0)\n",
    "\n",
    "       self.loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "   \n",
    "   def train(self):\n",
    "       embedding_vars = self.dlrm.get_embedding_variables()\n",
    "       dense_vars = self.dlrm.get_dense_variables()\n",
    "\n",
    "       @tf.function\n",
    "       def _train_step(inputs, labels):\n",
    "           with tf.GradientTape() as tape, tf.GradientTape() as emb_tape:\n",
    "               logit, embedding_vector = self.dlrm(inputs, training=True)\n",
    "               loss = self.loss_fn(labels, logit)\n",
    "\n",
    "           tape = hvd.DistributedGradientTape(tape)\n",
    "           dense_grads = tape.gradient(loss, dense_vars)\n",
    "           embedding_grads = emb_tape.gradient(loss, embedding_vars)\n",
    "\n",
    "           self.embedding_opt.apply_gradients(zip(embedding_grads, embedding_vars))\n",
    "           self.dense_opt.apply_gradients(zip(dense_grads, dense_vars))\n",
    "\n",
    "           return logit, embedding_vector, loss\n",
    "\n",
    "       sparse_keys, dense_features, labels = generate_random_samples(self.args[\"local_batch_size\"], self.args[\"iter_num\"], self.args[\"max_vocabulary_sizes\"], self.args[\"max_nnz\"], self.args[\"dense_dim\"])\n",
    "       dataset = tf_dataset(sparse_keys, dense_features, labels, self.args[\"local_batch_size\"])\n",
    "       for i, input_tuple in enumerate(dataset):\n",
    "           sparse_keys = input_tuple[:-2]\n",
    "           dense_features = input_tuple[-2]\n",
    "           labels = input_tuple[-1]\n",
    "           inputs = [sparse_keys, dense_features]\n",
    "           logit, embedding_vector, loss = _train_step(inputs, labels)\n",
    "           print(\"-\"*20, \"Step {}, loss: {}\".format(i, loss),  \"-\"*20)\n",
    "       self.dlrm.summary()\n",
    "\n",
    "   def dump_model(self):\n",
    "       self.dlrm.embedding_dump(args[\"sok_embedding_table_path\"],self.embedding_opt)\n",
    "    \n",
    "\n",
    "       \n",
    "       dense_model = tf.keras.Model([self.dlrm.get_layer(\"sok_embedding\").output,\n",
    "                             self.dlrm.get_layer(\"bottom\").input],\n",
    "                             self.dlrm.get_layer(\"top\").output)\n",
    "       dense_model.summary()\n",
    "       dense_model.save(args[\"dense_model_path\"])\n",
    "\n",
    "trainer = Trainer(args)\n",
    "trainer.train()\n",
    "trainer.dump_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "285201f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-29 07:17:01.079021: I sparse_operation_kit/kit_cc/kit_cc_infra/src/parameters/raw_manager.cc:192] Saving EmbeddingVariable to dlrm_sparse.model..\n",
      "2022-07-29 07:17:01.079021: I sparse_operation_kit/kit_cc_impl/embedding/common/src/dumping_functions.cc:60] Worker: 0, GPU: 0 key-index count = 260000\n",
      "2022-07-29 07:17:01.079021: I sparse_operation_kit/kit_cc_impl/embedding/common/src/dumping_functions.cc:147] Worker: 0, GPU: 0: dumping parameters from hashtable..\n",
      "2022-07-29 07:17:01.079021: I sparse_operation_kit/kit_cc/kit_cc_infra/src/parameters/raw_manager.cc:200] Saved EmbeddingVariable to dlrm_sparse.model.\n",
      "total 18360\n",
      "-rw-r--r-- 1 nobody nogroup 16640000 Jul 29 07:17 emb_vector\n",
      "-rw-r--r-- 1 nobody nogroup  2080000 Jul 29 07:17 key\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p dlrm_sparse.model\n",
    "embedding_saver.dump_to_file(trained_model.embedding_layer.embedding_variable, args[\"embedding_table_path\"])\n",
    "!mv dlrm_sparse.model/EmbeddingVariable_keys.file dlrm_sparse.model/key\n",
    "!mv dlrm_sparse.model/EmbeddingVariable_values.file dlrm_sparse.model/emb_vector\n",
    "!ls -l dlrm_sparse.model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}